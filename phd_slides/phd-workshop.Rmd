---
title: "Whats inside the news?"
author: Franziska Löw
date: December 16, 2017
output: 
  revealjs::revealjs_presentation:
    theme: simple
    highlight: haddock
    center: true
    transition: slide
    css: style.css
    #self_contained: false
    incremental: true
---

```{r include=FALSE}
suppressPackageStartupMessages({
  library(dplyr)       # Data manipulation
  library(stringr)     # String manipulation
  library(lubridate)   # Date and time manipulation
  library(magrittr)    # Advanced piping
  library(pushoverr)   # Pushover notifications
  library(readr)       # Importing data
  library(data.table)
  library(pscl)
  library(boot)
  library(stm)
  library(tm)
  library(readxl)
  library(tidytext)
  library(MCMCpack)
  library(Compositional)
  
  library(ggplot2)     # Static data visualization
  library(ggpmisc)
  library(RColorBrewer) 
  library(ggrepel)
  library(plotly)
  library(scales)      # Scales
  library(viridis)     # Viridis color scales

  library(stringdist)  # String distances
  library(proxy)       # Distance measures
})

# Theming
quartzFonts(
  Roboto =
    c("Roboto-Light",
      "Roboto-Bold",
      "Roboto-Regular",
      "Roboto-Thin")
)

theme_set(
  theme_bw(base_family = "Roboto", base_size = 10) +
    theme(
      plot.title = element_text(size = 12,
                                margin = margin(0, 0, 4, 0, "pt")),
      plot.subtitle = element_text(size = 12),
      plot.caption = element_text(size = 6, hjust = 0),
      axis.title = element_text(size = 8),
      panel.border = element_blank()
    )
)
col <- brewer.pal(8,"Dark2")
```

## Agenda

1. **Introduction**

2. **Methodology**

3. **Literature Review**

4. **First Results**

5. **Conclusion**

# Introduction
## Online News

<div style="position:relative; width:640px; height:480px; margin:0 auto;">
  <img class="fragment" src="img/screenshot-spon.png" style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/screenshot-focus.png" style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/screenshot-bild.png" style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/screenshot-tagesschau.png" style="position:absolute;top:0;left:0;" />
</div>

## Business Model of online News

<img class="fragment" src="img/onlinenews-2.png" width="900"/>

---------------------------

### Research Question: 
Does the business model have an effect on the editorial content?

### Methodology: 
  1. Estimate a Structural Topic Model
  2. Use posterior distribution to estimate the effect of document metadata. 

## Data
Online news articles about domestic politics from 01.06.2017 - 22.11.2017

<img class="fragment" src="img/timeline.png" width="850"/>

## Concepts

- A single observation in a textual database is called a *document*.

- The set of documents that make up the dataset is called a *corpus*.

- Covariates associated with each document are called *metadata*.

## Data Structure
```{r message=FALSE, warning=FALSE, include=FALSE}
load("data/presentation-data.Rda")
table <- inspect(dtm[1:5,1:10])
```

- Documents (articles) are stored on a "Document-Term-Matrix"
-
```{r echo=FALSE}
htmlTable::htmlTable(table)
```

- Documents are seen as "bag of words"
- Each article has metadata: publisher (news platform) and the day it was published. 
  

## How to find out latent topics in an article?

# Methodology

## Topic Model

<img src="img/lda-2.png">
<small>Credits: Christine Doig</small>

## The intuition behind LDA

<img src="img/lda-blei.png" width="750"/>

<small>Credits: Blei (2012)</small>

* Model the generation of documents with latent topic structure
    * a topic ~ a distribution over words
    * a document ~ a mixture over topics
    * a word ~ a sample drawn from one topic

    
<aside class="notes">
Mixed memberhip model: Population of topics stays the same, but distrubution over topics changes for each document.
Each document is comming from a mixture model, where the mixture proportions change from document to document but mixture components are fixed a cross all documents.
</aside>

## LDA as a graphical model


* Nodes are random variables; arrows indicate dependence


* Plates indicate replicated variables:
    * $N =$ collection of words within a document.
    * $D =$ collection of documents within a corpus.


* Shaded nodes are observed; unshaded nodes are hidden
    * observed: word in a document $w_{d,n}$
    * fixed: mixture components (number of topics $K$ & vocabulary)
    * hidden: mixture proportions (per-document topic proportions $\theta_d$ & word-topic distribution $\beta_k$)

----------------------------
<img src="img/lda-plate-2.png" width="600">
----------------------------

<aside class="notes">
Diagram:
(1)defines a factorization of the joint probability distribution of hidden and observed random variables.
(2)encodes indepence assumptions about the variables (which variables are dependent and conditionally independent) 
(3)Connects to algorithm for computing with data: Inference problem: finding the hidden variables given the observations.
</aside>

## Generative Process I
* Generative Process for the $n^{th}$ word-position in document $d$: 
    1. $K$: choose the number of topics;
    2. $\theta_d$: for each document $d$, choose a distribution over topics;
    3. $z_{d,n}$: according to $\theta_d$, assign a topic for the $n^{th}$ word;
    4. $w_{d,n}$: choose a term from that topic according to $\beta_k$
    5. $N$: repead this process for all $n$ word-positions in the document.
    6. $D$: conduct this process for all $d$ documents in the corpus


## R Markdown

test 1

***

test 2



## Generative Process II

$p(\beta_{1:K},\theta_{1:D},z_{1:D}, w_{1:D})=\displaystyle \prod_{i=1}^{K}p(\beta_i)\displaystyle \prod_{d=1}^{D}p(\theta_d)(\prod_{n=1}^Np(z_{d,n,}|\theta_d)p(w_{d,n}|\beta_{1:K},z_{d,n}))$

```{r}
draws <- rdirichlet(200, c(1,1,1) )
bivt.contour(draws)
```

## Bayesian inference

Posterior probability: $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$

$p(\theta_{1:D},z_{1:D},\beta_{1:K}|w_{1:D}) = \frac{p(\theta_{1:D},\beta_{1:K},z_{1:D})(w_{1:D})}{p(w_{1:D})}$
  
  * **numerator**: joint distribution of all random variables (can be easily computed for any setting of hidden variables.)
  * **denominator**: marginal probability of the observations (probability of seeing the observed corpus under any topic model.)

- Then use posterior expectations to perform the task at hand: information retrieval, document similarity, exploration, and others.

<aside class="notes">
the joint distribution defines a posterior. 
In theory, it can be computed by summing the joint distribution over every possible instantiation of the hidden topic structure.
$p(\theta,z,\beta|w)$ the probability of the hidden structures given the observed words in a document.
That number of possible topic structures, however, is exponentially large; this sum is intractable to compute. As for many modern probabilistic models of interest—and for much of modern Bayesian statistics—we cannot compute the posterior because of the denominator, which is known as the evidence. A central research goal of modern probabilistic modeling is to develop efficient methods for approximating it. Topic modeling algorithms—like the algorithms used to create Figures 1 and 3—are often adaptations of general-purpose methods for approximating the posterior distribution.
First uncover the structure (calculate th posterior) - use the discovered distributions to perform task
</aside>

## Research Process

<img class="fragment" src="img/research_strategy.png" width="1000"/>

<aside class="notes">
  1. We have knowledge about the world and want to answer a specific question: We know that newspapers are based on different business models. does that have an effect on the topics discussed in the newspaper?
  2. We make assumptions and bring them together with our data to uncover patterns.
  3. We use these patterns (posterior distributions) to predict a variable
</aside>

## Strucutral Topic Model
- We want to use estimates of $\theta_d$ as the dependent variable in an regression on covariates to test whether different types of documents have different content.

- This is contradictory because documents are assumed to be generated by the same statistical process.

- The structural topic model (STM) of Roberts et. al. (2016) explicitly introduces covariates into a topic model, and allows one to estimate the impact of document-level covariates on topic content and prevalence as part of the topic model itself.

## Topic Prevalence vs. Content
- The process for generating individual words is the same as for plain LDA conditional on the $\beta_k$ and $\pi_d$ terms.

- However both objects can depend on potentially different sets of document- level covariates. Each document has:
    1. **Topic Prevalence**: Attributes that affect the likelihood of discussing topic $k$
    2. **Topic Content**: Attributes that affect the likelihood of including term $v$ overall, and of including it within topic $k$
    
- The generation of the $\beta_k$ and $\pi_k$ terms is via multinomial logistic regression, which breaks local conjugacy.

## Model Selection
- There are three parameters we need to make assumptions about: number of topics $K$ and priors $\alpha, \eta$:
  - Priors don’t receive too much attention in literature: 
    - Griffiths & Steyvers (2002) recommend $\eta = 200/V$ and $\alpha = 50/K$. 
    - Smaller values will tend to generate more concentrated distributions. (See also Wallach et.al. (2009)).
  - $K$ is clearer. Two potential goals:
    1. Predict text well. Statistical criteria to select $K$.
    2. Interpretability. General versus specific.

## Approximate Posterior
- Mean field variational methods (Blei et al., 2001, 2003)
- Expectation propagation (Minka and Lafferty, 2002)
- Collapsed Gibbs sampling (Griffiths and Steyvers, 2002)
- Distributed samplung (Newsman et al., 2008; Ahmed et al., 2012)
- Stochastic inference (Hoffman et al., 2010, 2013; Mimno et al., 2012)
- Factorization inference (Arora et al., 2012; Anandkumar et al., 2012)
- Amortized inference (Srivastava and Sutton, 2019)
- ...

<aside class="notes">
  - We cannot obtain exact solutions for our posterior
  - Instead of obtaining a closed-form solution for the posterior distribution, we must approximate it.
</aside>

## Predicitve Distributions

$\beta_{k,v} = \frac{m_{k,v}+\eta}{\sum_{v=1}^V(m_{k,v}+\eta)}$

$\theta_{k,v} = \frac{n_{d,k}+\alpha}{\sum_{k=1}^K(n_{d,k}+\alpha)}$

## Formalizing Interpretability
- Chang et. al. (2009) propose an objective way of determining whether topics are indeed interpretable.
- Two tests:
  1. *Word intrusion*:
      - Form set of words consisting of top five words from topic k + word with low probability in topic k. 
      - Ask subjects to identify inserted word.
  2. *Topic intrusion*: 
      - Show subjects a snippet of a document + top three topics associated to it + randomly drawn other topic. 
      - Ask to identify inserted topic.
      
# Model Results

```{r Extract wtp and dtp, include=FALSE}
# Word-topic probabilities
stmOut %>% tidy("beta") %>% filter(!is.na(topic)) -> posts.wtp
# Document-topic probabilities
stmOut %>% tidy("gamma") -> posts.dtp

# Label Topics
label <- labelTopics(stmOut, n=3)

prob <- as.data.frame(label$prob, stringsAsFactors = F)
frex <- as.data.frame(label$frex, stringsAsFactors = F)
lift <- as.data.frame(label$lift, stringsAsFactors = F)
score <- as.data.frame(label$score, stringsAsFactors = F)

topicLabel <- prob %>% 
  transmute(topic = rownames(.),
            topic_name = paste(prob$V1,prob$V2,prob$V3,score$V1,score$V2,score$V3, sep=","),
            prob = paste(prob$V1,prob$V2,prob$V3, sep=","),
            frex = paste(frex$V1,frex$V2,frex$V3, sep=","),
            lift = paste(lift$V1,lift$V2,lift$V3, sep=","),
            score = paste(score$V1,score$V2,score$V3, sep=",")) 

topicLabel$topic_name <- vapply(lapply(strsplit(topicLabel$topic_name, ","), unique), paste, character(1L), collapse = " ")
rm(prob, frex, lift, score)
```

## Topic Proportions

<img src="img/topic_proportion.png" width="600">

## Sample Articles
