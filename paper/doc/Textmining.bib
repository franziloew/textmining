
@misc{center_for_history_and_new_media_schnelleinstieg_nodate,
	title = {Schnelleinstieg},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@article{berry_estimating_1994,
	title = {Estimating {Discrete}-{Choice} {Models} of {Product} {Differentiation}},
	volume = {25},
	issn = {07416261},
	url = {http://www.jstor.org/stable/2555829},
	abstract = {This article considers the problem of "supply-and-demand" analysis on a cross section of oligopoly markets with differentiated products. The primary methodology is to assume that demand can be described by a discrete-choice model and that prices are endogenously determined by price-setting firms. In contrast to some previous empirical work, the techniques explicitly allow for the possibility that prices are correlated with unobserved demand factors in the cross section of markets. The article proposes estimation by "inverting" the market-share equation to find the implied mean levels of utility for each good. This method allows for estimation by traditional instrumental variables techniques.},
	number = {2},
	journal = {The RAND Journal of Economics},
	author = {Berry, Steven T.},
	year = {1994},
	pages = {242--262}
}

@incollection{mcfadden_conditional_1974,
	address = {New York},
	title = {Conditional {Logit} {Analysis} of {Qualitative} {Choice} {Behavior}},
	booktitle = {Frontiers in {Econometrics}},
	publisher = {Academic Press},
	author = {McFadden, Daniel},
	editor = {Zarembka, P.},
	year = {1974},
	pages = {105--142}
}

@article{hendler_lancasters_1975,
	title = {Lancaster's {New} {Approach} to {Consumer} {Demand} and {Its} {Limitations}},
	volume = {65},
	issn = {00028282},
	url = {http://www.jstor.org/stable/1806408},
	number = {1},
	journal = {The American Economic Review},
	author = {Hendler, Reuven},
	year = {1975},
	pages = {194--199}
}

@article{hendler_lancasters_1975-1,
	title = {Lancaster's {New} {Approach} to {Consumer} {Demand} and {Its} {Limitations}},
	volume = {65},
	issn = {00028282},
	url = {http://www.jstor.org/stable/1806408},
	number = {1},
	journal = {The American Economic Review},
	author = {Hendler, Reuven},
	year = {1975},
	pages = {194--199}
}

@article{steven_berry_differentiated_2004,
	title = {Differentiated {Products} {Demand} {Systems} from a {Combination} of {Micro} and {Macro} {Data}: {The} {New} {Car} {Market}},
	volume = {112},
	url = {http://dx.doi.org/10.1086/379939},
	doi = {10.1086/379939},
	abstract = {In this paper, we consider how rich sources of information on consumer choice can help to identify demand parameters in a widely used class of differentiated products demand models. Most important, we show how to use “second‐choice” data on automotive purchases to obtain good estimates of substitution patterns in the automobile industry. We use our estimates to make out‐of‐sample predictions about important recent changes in industry structure.},
	number = {1},
	journal = {Journal of Political Economy},
	author = {{Steven Berry} and {James Levinsohn} and {Ariel Pakes}},
	year = {2004},
	pages = {68--105}
}

@article{russo_defining_2016,
	title = {Defining the relevant market in the sharing economy.},
	volume = {5},
	url = {http://policyreview.info/articles/analysis/defining-relevant-market-sharing-economy},
	number = {2},
	journal = {Internet Policy Review},
	author = {Russo, Francesco and Stasi, Maria Luisa},
	month = jun,
	year = {2016}
}

@misc{noauthor_autocorrelation_nodate,
	title = {{AUTOCORRELATION} - notes\_10.pdf},
	url = {http://www.ltrr.arizona.edu/~dmeko/notes_10.pdf},
	urldate = {2016-08-10},
	file = {AUTOCORRELATION - notes_10.pdf:/Users/Franzi/Zotero/storage/NPA8Q8TM/notes_10.html:text/html}
}

@article{armstrong_competition_2006,
	title = {Competition in two-sided markets},
	volume = {37},
	issn = {1756-2171},
	url = {http://dx.doi.org/10.1111/j.1756-2171.2006.tb00037.x},
	doi = {10.1111/j.1756-2171.2006.tb00037.x},
	number = {3},
	journal = {The RAND Journal of Economics},
	author = {Armstrong, Mark},
	year = {2006},
	pages = {668--691}
}

@book{anderson_discrete_1992,
	address = {Cambridge MA},
	title = {Discrete {Choice} {Theory} of {Product} {Differentiation}},
	isbn = {978-0-262-01128-0},
	publisher = {MIT Press},
	author = {Anderson, Simon P. and de Palma , Andre and Thisse, Jacques\_francois},
	year = {1992}
}

@article{roger_two-sided_2016,
	title = {Two-sided competition with vertical differentiation},
	issn = {1617-7134},
	url = {http://dx.doi.org/10.1007/s00712-016-0507-3},
	doi = {10.1007/s00712-016-0507-3},
	abstract = {This paper studies duopoly in which two-sided platforms compete in differentiated products in a two-sided market. Direct competition on both sides leads to results that depart from much of the current literature. Under some conditions the unique equilibrium in pure strategies can be computed. It features discounts on one side and muted differentiation as the cross-market externality intensifies competition. Less standard, that equilibrium fails to exist when the externality is too powerful (that side becomes too lucrative). A mixed-strategy equilibrium always exists and is characterized. These results are robust to variations in the extensive form. The model may find applications in the media, internet trading platforms, search engine competition, social media or even health insurance (HMO/PPO).},
	journal = {Journal of Economics},
	author = {Roger, Guillaume},
	year = {2016},
	pages = {1--25}
}

@book{wooldridge_econometric_2002,
	title = {Econometric {Analysis} of {Cross} {Section} and {Panel} {Data}},
	isbn = {978-0-262-23219-7},
	abstract = {This graduate text provides an intuitive but rigorous treatment of contemporary methods used in microeconometric research. The book makes clear that applied microeconometrics is about the estimation of marginal and treatment effects, and that parametric estimation is simply a means to this end. It also clarifies the distinction between causality and statistical association.The book focuses specifically on cross section and panel data methods. Population assumptions are stated separately from sampling assumptions, leading to simple statements as well as to important insights. The unified approach to linear and nonlinear models and to cross section and panel data enables straightforward coverage of more advanced methods. The numerous end-of-chapter problems are an important component of the book. Some problems contain important points not fully described in the text, and others cover new ideas that can be analyzed using tools presented in the current and previous chapters. Several problems require the use of the data sets located at the author's website.},
	language = {en},
	publisher = {MIT Press},
	author = {Wooldridge, Jeffrey M.},
	month = jan,
	year = {2002},
	note = {Google-Books-ID: cdBPOJUP4VsC},
	keywords = {Business \& Economics / Economics / General, Business \& Economics / Accounting / General, Business \& Economics / Econometrics, Business \& Economics / Economics / Microeconomics}
}

@article{paha_empirical_2011,
	title = {Empirical methods in the analysis of collusion},
	volume = {38},
	issn = {1573-6911},
	url = {http://dx.doi.org/10.1007/s10663-010-9160-1},
	doi = {10.1007/s10663-010-9160-1},
	abstract = {Regression methods are commonly used in competition lawsuits for, e.g., determining overcharges in price-fixing cases. Technical evaluations of these methods' pros and cons are not necessarily intuitive. Appraisals that are based on case studies are descriptive but need not be universally valid. This paper opens up the black box called econometrics for competition cases. This is done by complementing theoretical arguments with estimation results. These results are obtained for data that is generated by a simulation-model of a collusive industry. Using such data leaves little room for debate about the quality of these methods because estimates of, e.g., overcharges can be compared to their true underlying values. This analysis provides arguments for demonstrating that thoroughly conducted econometric analyses yield better results than simple techniques such as before-and-after comparisons.},
	number = {3},
	journal = {Empirica},
	author = {Paha, Johannes},
	year = {2011},
	pages = {389--415}
}

@article{rochet_platform_2003,
	title = {Platform {Competition} in {Two}-{Sided} {Markets}},
	volume = {1},
	issn = {1542-4774},
	url = {http://onlinelibrary.wiley.com/doi/10.1162/154247603322493212/abstract},
	doi = {10.1162/154247603322493212},
	abstract = {Many if not most markets with network externalities are two-sided. To succeed, platforms in industries such as software, portals and media, payment systems and the Internet, must “get both sides of the market on board.” Accordingly, platforms devote much attention to their business model, that is, to how they court each side while making money overall. This paper builds a model of platform competition with two-sided markets. It unveils the determinants of price allocation and end-user surplus for different governance structures (profit-maximizing platforms and not-for-profit joint undertakings), and compares the outcomes with those under an integrated monopolist and a Ramsey planner. (JEL: L5, L82, L86, L96)},
	language = {en},
	number = {4},
	urldate = {2016-08-04},
	journal = {Journal of the European Economic Association},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	month = jun,
	year = {2003},
	pages = {990--1029},
	file = {Snapshot:/Users/Franzi/Zotero/storage/WJI5PS42/abstract.html:text/html}
}

@misc{noauthor_innovation_nodate,
	title = {Innovation and {Price} {Competition} in a {Two}-{Sided} {Market} - viewcontent.cgi},
	url = {http://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=2716&context=sis_research},
	urldate = {2016-08-06},
	file = {Innovation and Price Competition in a Two-Sided Market - viewcontent.cgi:/Users/Franzi/Zotero/storage/PUDE8GM8/viewcontent.html:text/html}
}

@article{white_insulated_2012,
	title = {Insulated {Platform} {Competition}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=1694317},
	doi = {10.2139/ssrn.1694317},
	language = {en},
	urldate = {2016-10-10},
	journal = {SSRN Electronic Journal},
	author = {White, Alexander and Weyl, E. Glen},
	year = {2012}
}

@techreport{dannunzio_vertical_2015,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Vertical {Integration} in {Two}-{Sided} {Markets}: {Exclusive} {Provision} and {Program} {Quality}},
	shorttitle = {Vertical {Integration} in {Two}-{Sided} {Markets}},
	url = {http://papers.ssrn.com/abstract=2359615},
	abstract = {We study how vertical integration in a two-sided media market affects investments in premium content. We show that a content provider provides the premium content exclusively to a platform, regardless of what the vertical structure of the industry is. However, a vertically integrated content provider has lower incentives to invest in quality than an independent one. With asymmetric platforms, the platform with a competitive advantage in the advertising market obtains the exclusive content, and the content provider invests even less when it is integrated with it. We determine that the content provider prefers to acquire the platform with a competitive advantage in the advertising market. Vertical integration reduces both consumer and total surplus. Our results suggest that authorities should carefully assess the effects of a merger on the incentives to invest in content quality, incorporating non-price measures in merger analysis. An intervention at the distribution stage that enforces non-exclusive provision reduces quality and may have adverse effects on consumer and total surplus. Advertising caps might also have the effect of reducing quality.},
	number = {ID 2359615},
	urldate = {2016-08-03},
	institution = {Social Science Research Network},
	author = {D'Annunzio, Anna},
	month = mar,
	year = {2015},
	keywords = {advertising cap, exclusive contracts, media, merger, premium content, quality investment, two- sided markets, vertical integration},
	file = {Snapshot:/Users/Franzi/Zotero/storage/EQW6SR2Q/papers.html:text/html}
}

@article{dou_dynamic_2016,
	title = {Dynamic {Platform} {Competition}: {Optimal} {Pricing} and {Piggybacking} {Under} {Network} {Effects}},
	shorttitle = {Dynamic {Platform} {Competition}},
	url = {https://papers.ssrn.com/abstract=2842901},
	abstract = {A repeated challenge of the two-sided market literature is the “chicken-and-egg” problem. In a single-period setting, subsidizing one side of the market to jump},
	urldate = {2016-10-10},
	author = {Dou, Yifan and Wu, D. J.},
	month = sep,
	year = {2016},
	file = {Snapshot:/Users/Franzi/Zotero/storage/BW2WF274/papers.html:text/html}
}

@incollection{benghozi_innovation_2009,
	series = {Contributions to {Economics}},
	title = {Innovation and {Regulation} in the {Digital} {Age}: {A} {Call} for {New} {Perspectives}},
	copyright = {©2009 Physica-Verlag Heidelberg},
	isbn = {978-3-7908-2081-2 978-3-7908-2082-9},
	shorttitle = {Innovation and {Regulation} in the {Digital} {Age}},
	url = {http://link.springer.com/chapter/10.1007/978-3-7908-2082-9_28},
	abstract = {Relations between innovation and regulation are all but fluid and simple. When innovation, beyond just developing new techniques, means redefining the very framework for implementing and operating technologies, it often means breaking the rules, challenging them. In the digital economy, the way it overturns the regulation and rule setting system is particularly radical. Innovation is key to create competitive advantage in a highly dynamic sector such as information and communications technologies. Firms invest heavily in productive resources and take steps to protect their competitive advantage. Productive resources are either network and connection infrastructure or consumer control which is rarely seen as such. It could be consumer’s attention or visits which requires massive investments in content, for example. It does include intermediation platform like search engine, programs, knowledge, entertainment and other immaterial products of the digital age. There is a need to rethink regulation on the basis of innovation and mobilization of these productive resources. The digital economy calls for a more holistic consideration of the link between innovation and mobilization of value on the one hand, and regulation on the other.},
	language = {en},
	urldate = {2016-08-06},
	booktitle = {Telecommunication {Markets}},
	publisher = {Physica-Verlag HD},
	author = {Benghozi, Pierre-Jean and Gille, Laurent and Vallée, Alain},
	editor = {Curwen, Peter and Haucap, Justus and Preissl, Brigitte},
	year = {2009},
	note = {DOI: 10.1007/978-3-7908-2082-9\_28},
	keywords = {Industrial Organization, e-Commerce/e-business, Media Management, Communications Engineering, Networks, R \& D/Technology Policy},
	pages = {503--525},
	file = {Snapshot:/Users/Franzi/Zotero/storage/7KQ4EFGQ/978-3-7908-2082-9_28.html:text/html}
}

@article{lin_innovation_2011,
	title = {Innovation and {Price} {Competition} in a {Two}-{Sided} {Market}},
	volume = {28},
	issn = {0742-1222},
	url = {http://dx.doi.org/10.2753/MIS0742-1222280207},
	doi = {10.2753/MIS0742-1222280207},
	abstract = {We examine a platform's optimal two-sided pricing strategy while considering seller-side innovation decisions and price competition. We model the innovation race among sellers in both finite and infinite horizons. In the finite case, we analytically show that the platform's optimal seller-side access fee fully extracts the sellers' surplus, and that the optimal buyer-side access fee mitigates price competition among sellers. The platform's optimal strategy may be to charge or subsidize buyers depending on the degree of variation in the buyers' willingness to pay for quality; this optimal strategy induces full participation on both sides. Furthermore, a wider quality gap among sellers' products lowers the optimal buyer-side fee but leads to a higher optimal seller-side fee. In the infinite innovation race, we perform computations to find the stationary Markov equilibrium of sellers' innovation rate. Our results show that when all sellers innovate, there exists a parameterization under which a higher seller-side access fee stimulates innovation.},
	number = {2},
	urldate = {2016-08-06},
	journal = {J. Manage. Inf. Syst.},
	author = {Lin, Mei and Li, Shaojin and Whinston, Andrew},
	month = oct,
	year = {2011},
	keywords = {Innovation, Price Competition, Two-Sided Markets},
	pages = {171--202}
}

@techreport{rochet_must-take_2007,
	type = {{DNB} {Working} {Paper}},
	title = {Must-{Take} {Cards} and the {Tourist} {Test}},
	url = {https://ideas.repec.org/p/dnb/dnbwpp/127.html},
	abstract = {Antitrust authorities often argue that merchants cannot reasonably turn down payment cards and therefore are forced to accept unacceptably high merchant discounts. The paper attempts to shed light on this must-take cards view from two angles. First, the paper gives some operational content to the notion of must-take card through the tourist test (would the merchant want to refuse a card payment when a non-repeat customer with enough cash in her pocket is about to pay at the cash register?) and analyzes its relevance as an indicator of excessive interchange fees. Second, it identifies four key sources of potential social biases in the payment card associations' determination of interchange fees: internalization by merchants of a fraction of cardholder surplus, issuers' per-transaction markup, merchant heterogeneity, and extent of cardholder multi-homing. It compares the industry and social optima both in the short term (fixed number of issuers) and the long term (in which issuer offerings and entry respond to profitability).},
	number = {127},
	urldate = {2016-10-10},
	institution = {Netherlands Central Bank, Research Department},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	year = {2007},
	keywords = {Card payment systems, interchange fee, internalization, multi-homing, tourist test.},
	file = {RePEc PDF:/Users/Franzi/Zotero/storage/K4R8EQ4T/Rochet und Tirole - 2007 - Must-Take Cards and the Tourist Test.pdf:application/pdf;RePEc Snapshot:/Users/Franzi/Zotero/storage/DC4R3HKJ/127.html:text/html}
}

@article{noel_analyzing_2005,
	title = {Analyzing {Market} {Definition} and {Power} in {Multi}-sided {Platform} {Markets}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=835504},
	doi = {10.2139/ssrn.835504},
	language = {en},
	urldate = {2016-10-10},
	journal = {SSRN Electronic Journal},
	author = {Noel, Michael D. and Evans, David S.},
	year = {2005}
}

@article{rochet_two-sided_2006,
	title = {Two-sided markets: a progress report},
	volume = {37},
	issn = {1756-2171},
	shorttitle = {Two-sided markets},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1756-2171.2006.tb00036.x/abstract},
	doi = {10.1111/j.1756-2171.2006.tb00036.x},
	abstract = {We provide a roadmap to the burgeoning literature on two-sided markets and present new results. We identify two-sided markets with markets in which the structure, and not only the level of prices charged by platforms, matters. The failure of the Coase theorem is necessary but not sufficient for two-sidedness. We build a model integrating usage and membership externalities that unifies two hitherto disparate strands of the literature emphasizing either form of externality, and obtain new results on the mix of membership and usage charges when price setting or bargaining determine payments between end-users.},
	language = {en},
	number = {3},
	urldate = {2016-10-18},
	journal = {The RAND Journal of Economics},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	month = sep,
	year = {2006},
	pages = {645--667},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/IH993GGD/Rochet und Tirole - 2006 - Two-sided markets a progress report.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/WJ69HTDE/abstract.html:text/html}
}

@article{salop_monopolistic_1979,
	title = {Monopolistic {Competition} with {Outside} {Goods}},
	volume = {10},
	issn = {0361-915X},
	url = {http://econpapers.repec.org/article/rjebellje/v_3a10_3ay_3a1979_3ai_3aspring_3ap_3a141-156.htm},
	abstract = {The Chamberlinian monopolistically competitive equilibrium has been explored and extended in a number of recent papers. These analyses have paid only cursory attention to the existence of an industry outside the Chamberlinian group. In this article I analyze a model of spatial competition in which a second commodity is explicitly treated. In this two-industry economy, a zero-profit equilibrium with symmetrically located firms may exhibit rather strange properties. First, demand curves are kinked although firms make "Nash" conjectures. If equilibrium lies at the kink, the effects of parameter changes are perverse. In the short run, prices are rigid in the face of small cost changes. In the long run, increases in costs lower equilibrium prices. Increases in market size raise prices. The welfare properties are also perverse at a kinked equilibrium.},
	number = {1},
	urldate = {2016-10-18},
	journal = {Bell Journal of Economics},
	author = {Salop, Steven C.},
	year = {1979},
	pages = {141--156},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/TZZTM5HK/v_3a10_3ay_3a1979_3ai_3aspring_3ap_3a141-156.html:text/html}
}

@article{malam_mergers_2011,
	title = {Mergers of ad-sponsored media platforms},
	url = {http://ww.w.webmeets.com/files/papers/EARIE/2011/634/media_mergers_Sept.pdf},
	abstract = {An oligopoly model of competition among two-sided media platforms is presented to examine some competitive effects of mergers.   The model characterises ad-sponsored media platforms that
charge a zero price to viewers (offer free content) when competing simultaneously for advertisers. The  model  indicates  that  mergers  among ad-sponsored  platforms  have  a  competition-intensifying effect,  which offsets the incentive to increase prices on the advertiser side. When the assumption
of full market coverage in the advertiser market is relaxed,  the decrease in prices improves social
welfare},
	urldate = {2016-10-18},
	author = {Malam, Craig R.},
	month = aug,
	year = {2011},
	file = {media_mergers_Sept.pdf:/Users/Franzi/Zotero/storage/CIBINTVE/media_mergers_Sept.html:text/html}
}

@techreport{filistrucchi_merger_2010,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Merger {Simulation} in a {Two}-{Sided} {Market}: {The} {Case} of the {Dutch} {Daily} {Newspapers}},
	shorttitle = {Merger {Simulation} in a {Two}-{Sided} {Market}},
	url = {https://papers.ssrn.com/abstract=1694313},
	abstract = {We develop a structural econometric framework that allows us to simulate the effects of mergers among two-sided platforms selling differentiated products. We apply the proposed methodology to the Dutch newspaper industry. Our structural model encompasses demands for differentiated products on both sides of the market and profit maximization by competing oligopolistic publishers who choose subscription and advertising prices, while taking the interactions between the two-sides of the market into account. We measure the sign and size of the indirect network effects between the two sides of the market and simulate the effects of a hypothetical merger on prices and welfare.},
	number = {ID 1694313},
	urldate = {2016-10-18},
	institution = {Social Science Research Network},
	author = {Filistrucchi, Lapo and Klein, Tobias J. and Michielsen, Thomas},
	month = sep,
	year = {2010},
	keywords = {Two-Sided Markets, SSNIP test, network effects, advertising, newspapers, Merger Simulation},
	file = {Snapshot:/Users/Franzi/Zotero/storage/HU7F9G6I/papers.html:text/html}
}

@article{caillaud_chicken_2003,
	title = {Chicken \& {Egg}: {Competition} among {Intermediation} {Service} {Providers}},
	volume = {34},
	issn = {0741-6261},
	shorttitle = {Chicken \& {Egg}},
	url = {http://econpapers.repec.org/article/rjerandje/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.htm},
	abstract = {We analyze a model of imperfect price competition between intermediation service providers. We insist on features that are relevant for informational intermediation via the Internet: the presence of indirect network externalities, the possibility of using the nonexclusive services of several intermediaries, and the widespread practice of price discrimination based on users' identity and on usage. Efficient market structures emerge in equilibrium, as well as some specific form of inefficient structures. Intermediaries have incentives to propose non-exclusive services, as this moderates competition and allows them to exert market power. We analyze in detail the pricing and business strategies followed by intermediation services providers. Copyright 2003 by the RAND Corporation.},
	number = {2},
	urldate = {2016-10-18},
	journal = {RAND Journal of Economics},
	author = {Caillaud, Bernard and Jullien, Bruno},
	year = {2003},
	pages = {309--28},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/GD5R58G2/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.html:text/html}
}

@techreport{filistrucchi_ssnip_2008,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {SSNIP} {Test} for {Two}-{Sided} {Markets}: {The} {Case} of {Media}},
	shorttitle = {A {SSNIP} {Test} for {Two}-{Sided} {Markets}},
	url = {https://papers.ssrn.com/abstract=1287442},
	abstract = {I discuss the design and implementation of a SSNIP test in order to identify the relevant market in a media market. I argue that in such a two-sided market the traditional SSNIP test cannot be applied as it is usually conceived but rather should be modified in order to take into account indirect network externalities. I discuss the issues of which price the hypothetical monopolist should be thought of as raising, of whether we should look at profits changes on only one side or on both sides of the market and of which feedback among the two sides of the market we should take into account. I then derive the relevant formulas for Critical Loss Analysis. These look much uglier than in a single-sided market but in fact they are easy to calculate as they are still expressed in terms of elasticities and of current observed markups, prices and quantities. Data requirements are however higher as one needs to estimate the matrixes of the own and cross price elasticities of demand on the two-sides of the market and the matrixes of the network effects. The paper fills a gap in the economic literature, so much more as market definition in media markets is at the centre of many recent competition policy and regulation cases around the world.},
	number = {ID 1287442},
	urldate = {2016-10-18},
	institution = {Social Science Research Network},
	author = {Filistrucchi, Lapo},
	month = oct,
	year = {2008},
	keywords = {Two-Sided Markets, SSNIP test, Hypothetical Monopolist test, critical loss analysis},
	file = {Snapshot:/Users/Franzi/Zotero/storage/5EF8R939/papers.html:text/html}
}

@techreport{filistrucchi_identifying_2012,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Identifying {Two}-{Sided} {Markets}},
	url = {https://papers.ssrn.com/abstract=2008661},
	abstract = {We review the burgeoning literature on two-sided markets focusing on the different definitions that have been proposed. In particular, we show that the well-known definition given by Evans is a particular case of the more general definition proposed by Rochet and Tirole. We then identify the crucial elements that make a market two-sided and, drawing from both theory and practice, derive suggestions for the identification of the two-sided nature of a market. Our suggestions are relevant not only for the analysis of traditional two-sided markets, such as newspapers and payment cards, but also for the analysis of many new markets, such as those for online social networks, online search engines and Internet news aggregators.},
	number = {ID 2008661},
	urldate = {2016-10-18},
	institution = {Social Science Research Network},
	author = {Filistrucchi, Lapo and Geradin, Damien and Damme, Van and Eric},
	month = feb,
	year = {2012},
	keywords = {Two-Sided Markets, network effects, platforms},
	file = {Snapshot:/Users/Franzi/Zotero/storage/J9VDPHRI/papers.html:text/html}
}

@techreport{ackerberg_quantifying_2006,
	type = {Working {Paper}},
	title = {Quantifying {Equilibrium} {Network} {Externalities} in the {ACH} {Banking} {Industry}},
	url = {http://www.nber.org/papers/w12488},
	abstract = {We seek to determine the causes and magnitudes of network externalities for the automated clearinghouse (ACH) electronic payments system. We construct an equilibrium model of customer and bank adoption of ACH. We structurally estimate the parameters of the model using an indirect inference procedure and panel data. The parameters are identified from exogenous variation in the adoption decisions of banks based outside the network and other factors. We find that most of the impediment to ACH adoption is from large customer fixed costs of adoption. Policies to provide moderate subsidies to customers and larger subsidies to banks for ACH adoption could increase welfare significantly.},
	number = {12488},
	urldate = {2016-10-19},
	institution = {National Bureau of Economic Research},
	author = {Ackerberg, Daniel A. and Gowrisankaran, Gautam},
	month = aug,
	year = {2006},
	file = {NBER Full Text PDF:/Users/Franzi/Zotero/storage/26SGPGXH/Ackerberg und Gowrisankaran - 2006 - Quantifying Equilibrium Network Externalities in t.pdf:application/pdf}
}

@techreport{filistrucchi_price_2013,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Price {Competition} in {Two}-{Sided} {Markets} with {Heterogeneous} {Consumers} and {Network} {Effects}},
	url = {https://papers.ssrn.com/abstract=2336411},
	abstract = {We model a two-sided market with heterogeneous customers and two heterogeneous network effects. In our model, customers on each market side care differently about both the number and the type of customers on the other side. Examples of two-sided markets are online platforms or daily newspapers. In the latter case, for instance, readership demand depends on the amount and the type of advertisements. Also, advertising demand depends on the number of readers and the distribution of readers across demographic groups. There are feedback loops because advertising demand depends on the numbers of readers, which again depends on the amount of advertising, and so on. Due to the difficulty in dealing with such feedback loops when publishers set prices on both sides of the market, most of the literature has avoided models with Bertrand competition on both sides or has resorted to simplifying assumptions such as linear demands or the presence of only one network effect. We address this issue by first presenting intuitive sufficient conditions for demand on each side to be unique given prices on both sides. We then derive sufficient conditions for the existence and uniqueness of an equilibrium in prices. For merger analysis, or any other policy simulation in the context of competition policy, it is important that equilibria exist and are unique. Otherwise, one cannot predict prices or welfare effects after a merger or a policy change. The conditions are related to the own- and cross-price effects, as well as the strength of the own and cross network effects. We show that most functional forms used in empirical work, such as logit type demand functions, tend to satisfy these conditions for realistic values of the respective parameters. Finally, using data on the Dutch daily newspaper industry, we estimate a flexible model of demand which satisfies the above conditions and evaluate the effects of a hypothetical merger and study the effects of a shrinking market for offline newspapers.},
	number = {ID 2336411},
	urldate = {2016-10-18},
	institution = {Social Science Research Network},
	author = {Filistrucchi, Lapo and Klein, Tobias J.},
	month = oct,
	year = {2013},
	keywords = {Two-Sided Markets, competition policy, newspapers, Merger Simulation, indirect network effects, equilibrium},
	file = {Snapshot:/Users/Franzi/Zotero/storage/VZX9FWPR/papers.html:text/html}
}

@book{jenkins_spectral_1968,
	address = {San Francisco},
	title = {Spectral analysis and its applications},
	isbn = {978-0-8162-4464-5},
	language = {English},
	publisher = {Holden-Day},
	author = {Jenkins, Gwilym M and Watts, Donald G},
	year = {1968},
	note = {OCLC: 381124}
}

@book{box_time_2008,
	address = {Hoboken, NJ},
	edition = {4},
	series = {Wiley series in probability and statistics},
	title = {Time series analysis: forecasting and control},
	isbn = {978-0-470-27284-8},
	shorttitle = {Time series analysis},
	publisher = {Wiley},
	author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C.},
	year = {2008},
	keywords = {*Time-series analysis / Prediction theory / Transfer functions / Feedback control systems -- Mathematical models, *Zeitreihenanalyse / Kontrolltheorie / Rückkopplung / Mathematisches Modell}
}

@book{chatfield_analysis_2004,
	address = {Boca Raton, Fla. [u.a.]},
	edition = {6},
	series = {Texts in statistical science},
	title = {The analysis of time series: an introduction},
	isbn = {978-1-58488-317-3},
	shorttitle = {The analysis of time series},
	publisher = {Chapman \& Hall/CRC},
	author = {Chatfield, Christopher},
	year = {2004},
	keywords = {*Time-series analysis, *Zeitreihenanalyse, *Zeitreihenanalyse / Lehrbuch, Lehrbuch, Statistische Methodenlehre, Zeitreihenanalyse}
}

@book{brockwell_introduction_2002,
	address = {New York, NY},
	edition = {Second Edition},
	series = {Springer {Texts} in {Statistics}},
	title = {Introduction to time series and forecasting},
	isbn = {978-0-387-21657-7},
	publisher = {Springer-Verlag New York, Inc},
	author = {Brockwell, Peter J.},
	editor = {Davis, Richard A.},
	year = {2002},
	keywords = {*Statistics / Mathematical statistics / Economics -- Statistics / Econometrics, *Zeitreihenanalyse / Lehrbuch}
}

@techreport{behringer_price_2009,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Price {Wars} in {Two}-{Sided} {Markets}: {The} {Case} of the {UK} {Quality} {Newspapers}},
	shorttitle = {Price {Wars} in {Two}-{Sided} {Markets}},
	url = {https://papers.ssrn.com/abstract=1499866},
	abstract = {This paper investigates the price war in the UK quality newspaper industry in the 1990s. We build a model of the newspaper market which encompasses demand for differentiated products on both, the readers and advertisers side of the market, and profit maximization by four competing oligopolistic editors who recognize the existence of an indirect network effect of circulation on advertising demand. Editors choose first the political position, then simultaneously cover prices and advertising tariffs. We contribute to the literature on two-sided markets by endogenizing the political differentiation of newspapers in a model with more than two firms. We simulate changes to market structure in order to explore which of the candidate explanations is most likely to lie behind the observed price war.},
	number = {ID 1499866},
	urldate = {2016-10-18},
	institution = {Social Science Research Network},
	author = {Behringer, Stefan and Filistrucchi, Lapo},
	month = nov,
	year = {2009},
	keywords = {SSRN, Price Wars in Two-Sided Markets: The Case of the UK Quality Newspapers, Stefan  Behringer, Lapo  Filistrucchi},
	file = {Snapshot:/Users/Franzi/Zotero/storage/A2INHV6Q/papers.html:text/html}
}

@article{rysman_empirical_2007,
	title = {An {Empirical} {Analysis} of {Payment} {Card} {Usage}},
	volume = {55},
	issn = {0022-1821},
	url = {http://www.jstor.org/stable/4622372},
	abstract = {I exploit a unique data set on the payment card industry to study issues associated with network effects and two-sided markets. I show that consumers concentrate their spending on a single payment network (single-homing), although many maintain unused cards that allow the ability to use multiple networks (multi-homing). Further, I establish a regional correlation between consumer usage and merchant acceptance within the four major networks (Visa, Mastercard, American Express and Discover). This correlation is suggestive of the existence of a positive feedback loop between consumer usage and merchant acceptance.},
	number = {1},
	urldate = {2016-10-19},
	journal = {The Journal of Industrial Economics},
	author = {Rysman, Marc},
	year = {2007},
	pages = {1--36}
}

@techreport{cayseele_prices_2009,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Prices and {Network} {Effects} in {Two}-{Sided} {Markets}: {The} {Belgian} {Newspaper} {Industry}},
	shorttitle = {Prices and {Network} {Effects} in {Two}-{Sided} {Markets}},
	url = {https://papers.ssrn.com/abstract=1404392},
	abstract = {This paper investigates the two-sided nature of the newspaper industry. We explicitly take into account cross network effects that exist between advertisers and newspaper readers. On one side, advertisers' demand for publicity space depends on the number of newspaper readers and their characteristics. On the other side, readers' demand can be, positively or negatively, influenced by the number of advertisements. In addition, editors may own several newspapers and hence a variety of cross-market effects that result from changes in market prices exist. To estimate demand parameters for both sides of the market, a specific structural model is needed that takes into account those effects. We estimate network effects and price elasticities for Belgian newspaper publishers to assess market power and the degree of competition in the market, which experienced a large consolidation wave over the last decades. This allows us to evaluate a recent merger in the Belgian newspaper industry.},
	number = {ID 1404392},
	urldate = {2016-10-18},
	institution = {Social Science Research Network},
	author = {Cayseele, Van and G, Patrick J. and Vanormelingen, Stijn},
	month = feb,
	year = {2009},
	keywords = {Two-Sided Markets, newspapers, demand estimation},
	file = {Snapshot:/Users/Franzi/Zotero/storage/ZGU2HCSD/papers.html:text/html}
}

@techreport{evans_antitrust_2013,
	type = {Working {Paper}},
	title = {The {Antitrust} {Analysis} of {Multi}-{Sided} {Platform} {Businesses}},
	url = {http://www.nber.org/papers/w18783},
	abstract = {This Chapter provides a survey of the economics literature on multi-sided platforms with particular focus on competition policy issues, including market definition, mergers, monopolization, and coordinated behavior. It provides a survey of the general industrial organization theory of multi-sided platforms and then considers various issues concerning the application of antitrust analysis to multi-sided platform businesses. It shows that it is not possible to know whether standard economic models, often relied on for antitrust analysis, apply to multi-sided platforms without explicitly considering the existence of multiple customer groups with interdependent demand. It summarizes many theoretical and empirical papers that demonstrate that a number of results for single-sided firms, which are the focus of much of the applied antitrust economics literature, do not apply directly to multi-sided platforms.},
	number = {18783},
	urldate = {2016-10-20},
	institution = {National Bureau of Economic Research},
	author = {Evans, David S. and Schmalensee, Richard},
	month = feb,
	year = {2013},
	file = {NBER Full Text PDF:/Users/Franzi/Zotero/storage/4Q6UQMSS/Evans und Schmalensee - 2013 - The Antitrust Analysis of Multi-Sided Platform Bus.pdf:application/pdf}
}

@techreport{jeziorski_merger_2010,
	type = {Economics {Working} {Paper} {Archive}},
	title = {Merger enforcement in two-sided markets},
	url = {https://ideas.repec.org/p/jhu/papers/570.html},
	abstract = {This paper studies mergers in two-sided markets by estimating a structural supply and demand model and performing counterfactual experiments. The analysis is performed on data for a merger wave in U.S. radio that occurred between 1996 and 2006. The paper makes two main contributions. First, I identify the conflicting incentives of merged firms to exercise market power on both sides of the market (listeners and advertisers in the case of radio). Second, I disaggregate the effects of mergers on consumers into changes in product variety and changes in supplied ad quantity. I find that firms have moderate market power over listeners in all markets, extensive market power over advertisers in small markets and no market power over advertisers in large markets. Counterfactuals reveal that extra product variety created by post-merger repositioning increased listeners' welfare by 1.3\% and decreased advertisers' welfare by about \$160m per-year. However, subsequent changes in supplied ad quantity decreased listener welfare by 0.4\% (for a total impact of +0.9\%) and advertiser welfare by an additional \$140m (for a total impact of -\$300m).},
	number = {570},
	urldate = {2016-10-18},
	institution = {The Johns Hopkins University,Department of Economics},
	author = {Jeziorski, Przemyslaw},
	year = {2010},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/I9SXR48N/570.html:text/html}
}

@article{rysman_competition_2004,
	title = {Competition between {Networks}: {A} {Study} of the {Market} for {Yellow} {Pages}},
	volume = {71},
	issn = {0034-6527},
	shorttitle = {Competition between {Networks}},
	url = {http://www.jstor.org/stable/3700635},
	abstract = {This paper estimates the importance of network effects in the market for Yellow Pages. I estimate three simultaneous equations: consumer demand for usage of a directory, advertiser demand for advertising and a publisher's first-order condition (derived from profit-maximizing behaviour). Estimation shows that advertisers value consumer usage and that consumers value advertising, implying a network effect. I find that internalizing network effects would significantly increase surplus. As an application, I consider whether the market benefits from monopoly (which takes advantage of network effects) or oligopoly (which reduces market power). I find that a more competitive market is preferable.},
	number = {2},
	urldate = {2016-10-19},
	journal = {The Review of Economic Studies},
	author = {Rysman, Marc},
	year = {2004},
	pages = {483--512}
}

@article{fan_ownership_2013,
	title = {Ownership {Consolidation} and {Product} {Characteristics}: {A} {Study} of the {US} {Daily} {Newspaper} {Market}},
	volume = {103},
	issn = {0002-8282},
	shorttitle = {Ownership {Consolidation} and {Product} {Characteristics}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.103.5.1598},
	doi = {10.1257/aer.103.5.1598},
	abstract = {This paper develops a structural model of newspaper markets
to analyze the effects of ownership consolidation, taking into
account not only firms' price adjustments but also the adjustments
in newspaper characteristics. A new dataset on newspaper prices
and characteristics is used to estimate the model. The paper then
simulates the effect of a merger in the Minneapolis newspaper
market and studies how welfare effects of mergers vary with market
characteristics. It finds that ignoring adjustments of product
characteristics causes substantial differences in estimated effects of
mergers.},
	number = {5},
	urldate = {2016-10-18},
	journal = {American Economic Review},
	author = {Fan, Ying},
	month = aug,
	year = {2013},
	pages = {1598--1628},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/KQV4BVJ9/Fan - 2013 - Ownership Consolidation and Product Characteristic.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/FW2N3JCG/articles.html:text/html}
}

@article{kaiser_when_2007,
	title = {When {Pricing} {Below} {Marginal} {Cost} {Pays} {Off}: {Optimal} {Price} {Choice} in a {Media} {Market} with {Upfront} {Pricing}},
	shorttitle = {When {Pricing} {Below} {Marginal} {Cost} {Pays} {Off}},
	url = {https://www.researchgate.net/publication/5161030_When_Pricing_Below_Marginal_Cost_Pays_Off_Optimal_Price_Choice_in_a_Media_Market_with_Upfront_Pricing},
	abstract = {I derive a model of profit maximization for a print media firm with upfront advertising pricing. The model is estimated using detailed quarterly data on German women's magazines observed between...},
	urldate = {2016-10-19},
	journal = {ResearchGate},
	author = {Kaiser, Ulrich},
	month = may,
	year = {2007},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/HAPMUNP6/Kaiser - 2007 - When Pricing Below Marginal Cost Pays Off Optimal.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/MEJNPCR4/5161030_When_Pricing_Below_Marginal_Cost_Pays_Off_Optimal_Price_Choice_in_a_Media_Market_with_U.html:text/html}
}

@article{song_estimating_2015,
	title = {Estimating {Platform} {Market} {Power} in {Two}-{Sided} {Markets} with an {Application} to {Magazine} {Advertising}},
	volume = {forthcoming},
	url = {https://papers.ssrn.com/abstract=1908621},
	abstract = {In this paper I develop a structural model of platform demand in two-sided markets and use it to estimate platform market power and predict price changes in post-merger markets. My model and estimation procedure are applicable to settings where (1) agents on each side care about the presence of agents on the other side and (2) platforms charge access fees on both sides. Using data on TV magazines in Germany I show that the magazines typically set copy prices below marginal costs and make profits from selling advertising space. I also show that platform mergers are much less harmful to consumers (readers and advertisers) than what the one-sided market model predicts.},
	urldate = {2016-10-19},
	journal = {American Economic Journal: Microeconomics},
	author = {Song, Minjae},
	month = jan,
	year = {2015},
	note = {forthcoming},
	keywords = {Merger Simulation, platform competition, two-sided single-homing, competitive bottleneck},
	file = {Snapshot:/Users/Franzi/Zotero/storage/S7MKN5VI/papers.html:text/html}
}

@article{rochet_cooperation_2002,
	title = {Cooperation among {Competitors}: {Some} {Economics} of {Payment} {Card} {Associations}},
	volume = {33},
	issn = {0741-6261},
	shorttitle = {Cooperation among {Competitors}},
	url = {http://www.jstor.org/stable/3087474},
	doi = {10.2307/3087474},
	abstract = {We analyze platforms in two-sided markets with network externalities, using the specific context of a payment card association. We study the cooperative determination of the interchange fee by member banks. The interchange fee is the "access charge" paid by the merchants' banks (the acquirers) to cardholders' banks (the issuers). We develop a framework in which banks and merchants may have market power and consumers and merchants decide rationally on whether to buy or accept a payment card. After drawing the welfare implications of a cooperative determination of the interchange fee, we describe in detail the factors affecting merchant resistance, compare cooperative and for-profit business models, and make a first cut in the analysis of system competition.},
	number = {4},
	urldate = {2016-10-21},
	journal = {The RAND Journal of Economics},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	year = {2002},
	pages = {549--570}
}

@incollection{evans_two-sided_2012,
	address = {Rochester, NY},
	title = {Two-{Sided} {Market} {Definition}},
	isbn = {978-1-61438-366-6},
	abstract = {This paper addresses the analysis of market definition when the parties involved in an antitrust or merger analysis include one or more two-sided platforms.  We discuss how standard market definition measures such as SSNIP tests, diversion ratios, and conditional logit demand analyses have to be modified to account for the unique characteristics of two-sided platforms. We also review how market definition of two- sided platforms was treated in recent US and EC case law.},
	booktitle = {Market {Definition} in {Antitrust}: {Theory} and {Case} {Studies}},
	publisher = {ABA Book Publishing},
	author = {Evans, David S.},
	year = {2012},
	keywords = {Two-Sided Markets, market definition, two-sided platforms, multi-sided platforms, antitrust analysis, merger analysis},
	file = {Snapshot:/Users/Franzi/Zotero/storage/HSU8IIVQ/papers.html:text/html}
}

@article{weyl_price_2010,
	title = {A {Price} {Theory} of {Multi}-sided {Platforms}},
	volume = {100},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.100.4.1642},
	doi = {10.1257/aer.100.4.1642},
	abstract = {I develop a general theory of monopoly pricing of networks. Platforms use insulating tariffs to avoid coordination failure, implementing any desired allocation. Profit maximization distorts in the spirit of A. Michael Spence (1975) by internalizing only network externalities to marginal users. Thus the empirical and prescriptive content of the popular Jean-Charles Rochet and Jean Tirole
(2006) model of two-sided markets turns on the nature of user heterogeneity. I propose a more plausible, yet equally tractable, model of heterogeneity in which users differ in their income or scale. My approach provides a general measure of market power and helps predict the effect of price regulation and mergers. (JEL D42, D85, L14)},
	number = {4},
	urldate = {2016-10-31},
	journal = {American Economic Review},
	author = {Weyl, E. Glen},
	month = sep,
	year = {2010},
	pages = {1642--1672},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/3GTB7WTI/Weyl - 2010 - A Price Theory of Multi-sided Platforms.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/MGXHE26T/articles.html:text/html}
}

@techreport{evans_industrial_2005,
	type = {Working {Paper}},
	title = {The {Industrial} {Organization} of {Markets} with {Two}-{Sided} {Platforms}},
	url = {http://www.nber.org/papers/w11603},
	abstract = {Two-sided platforms (2SPs) cater to two or more distinct groups of customers, facilitating value-creating interactions between them. The village market and the village matchmaker were 2SPs; eBay and Match.com are more recent examples. Other examples include payment card systems, magazines, shopping malls, and personal computer operating systems. Building on the seminal work of Rochet and Tirole (2003), a rapidly growing literature has illuminated the economic principles that apply to 2SPs generally. One key result is that 2SPs may find it profit-maximizing to charge prices for one customer group that are below marginal cost or even negative, and such skewed pricing pattern is prevalent, although not universal, in industries that appear to be based on 2SPs. Over the years, courts have also recognized that certain industries, notably payment card systems and newspapers, now understood to be based on 2SPs, are governed by unusual economic relationships. This chapter provides an introduction to the economics of 2SPs and its application to several competition policy issues.},
	number = {11603},
	urldate = {2016-10-20},
	institution = {National Bureau of Economic Research},
	author = {Evans, David S. and Schmalensee, Richard},
	month = sep,
	year = {2005},
	file = {NBER Full Text PDF:/Users/Franzi/Zotero/storage/NCANQR55/Evans und Schmalensee - 2005 - The Industrial Organization of Markets with Two-Si.pdf:application/pdf}
}

@article{wright_one-sided_2004,
	title = {One-sided {Logic} in {Two}-sided {Markets}},
	volume = {3},
	url = {http://econpapers.repec.org/article/bpjrneart/v_3a3_3ay_3a2004_3ai_3a1_3an_3a3.htm},
	abstract = {In this paper, I consider eight basic fallacies that can arise from using conventional wisdom from one-sided markets in two-sided market settings. These fallacies are illustrated using statements made in the context of regulatory investigations into credit card schemes in Australia and the United Kingdom. I discuss how these fallacies may be reconciled by proper use of a two-sided market analysis, making reference to the relevant economics literature where applicable. The analysis is supported by observations on other two-sided markets.},
	number = {1},
	urldate = {2016-10-21},
	journal = {Review of Network Economics},
	author = {Wright, Julian},
	year = {2004},
	pages = {1--21},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/S8T3FQRE/v_3a3_3ay_3a2004_3ai_3a1_3an_3a3.html:text/html}
}

@article{wilbur_two-sided_2008,
	title = {A {Two}-{Sided}, {Empirical} {Model} of {Television} {Advertising} and {Viewing} {Markets}},
	volume = {27},
	issn = {0732-2399},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mksc.1070.0303},
	doi = {10.1287/mksc.1070.0303},
	abstract = {For marketers, television remains the most important advertising medium. This paper proposes a two-sided model of the television industry. We estimate viewer demand for programs on one side and advertiser demand for audiences on the other. The primary objective is to understand how each group's program usage influences the other group. Four main conclusions emerge. First, viewers tend to be averse to advertising. When a highly rated network decreases its advertising time by 10\%, our model predicts a median audience gain of about 25\% (assuming no competitive reactions). Second, we find the price elasticity of advertising demand is −2.9, substantially more price elastic than 30 years ago. Third, we compare our estimates of advertiser and viewer preferences for program characteristics to networks' observed program choices. Our results suggest that advertiser preferences influence network choices more strongly than viewer preferences. Viewers' two most preferred program genres, Action and News, account for just 16\% of network program hours. Advertisers' two most preferred genres, Reality and Comedy, account for 47\% of network program hours. Fourth, we perform a counterfactual experiment in which some viewers gain access to a hypothetical advertisement avoidance technology. The results suggest that ad avoidance tends to increase equilibrium advertising quantities and decrease network revenues.},
	number = {3},
	urldate = {2016-11-01},
	journal = {Marketing Science},
	author = {Wilbur, Kenneth C.},
	month = may,
	year = {2008},
	pages = {356--378},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/H4QQ8BIM/Wilbur - 2008 - A Two-Sided, Empirical Model of Television Adverti.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/FRQGAND7/mksc.1070.html:text/html}
}

@article{box_distribution_1970,
	title = {Distribution of {Residual} {Autocorrelations} in {Autoregressive}-{Integrated} {Moving} {Average} {Time} {Series} {Models}},
	volume = {65},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2284333},
	doi = {10.2307/2284333},
	abstract = {Many statistical models, and in particular autoregressive-moving average time series models, can be regarded as means of transforming the data to white noise, that is, to an uncorrelated sequence of errors. If the parameters are known exactly, this random sequence can be computed directly from the observations; when this calculation is made with estimates substituted for the true parameter values, the resulting sequence is referred to as the "residuals," which can be regarded as estimates of the errors. If the appropriate model has been chosen, there will be zero autocorrelation in the errors. In checking adequacy of fit it is therefore logical to study the sample autocorrelation function of the residuals. For large samples the residuals from a correctly fitted model resemble very closely the true errors of the process; however, care is needed in interpreting the serial correlations of the residuals. It is shown here that the residual autocorrelations are to a close approximation representable as a singular linear transformation of the autocorrelations of the errors so that they possess a singular normal distribution. Failing to allow for this results in a tendency to overlook evidence of lack of fit. Tests of fit and diagnostic checks are devised which take these facts into account.},
	number = {332},
	urldate = {2016-08-15},
	journal = {Journal of the American Statistical Association},
	author = {Box, G. E. P. and Pierce, David A.},
	year = {1970},
	pages = {1509--1526}
}

@techreport{argentesi_market_2005,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Market {Definition} in the {Printed} {Media} {Industry}: {Theory} and {Practice}},
	shorttitle = {Market {Definition} in the {Printed} {Media} {Industry}},
	url = {http://papers.ssrn.com/abstract=779107},
	abstract = {This paper first discusses how the market is delineated in some recent antitrust cases in the printed media industry. It evaluates the extent to which the main features of the industry are incorporated into the analysis and affect market definition. In addition, we argue that an econometric analysis that does not incorporate these features can lead to biased estimates of elasticities. As demand substitution is a crucial element for defining market, bad estimates of elasticities could blur the boundaries of relevant markets. Second, we propose a simple model that encompasses these features and in particular the two-sidedness of the market. Thirdly, we review some empirical papers that analyze the issue of demand estimation in printed media. Finally, we perform a statistical estimation on a dataset of magazines in order to provide a measure of the possible bias that could arise in the estimation of elasticities when one does not use a proper model.},
	number = {ID 779107},
	urldate = {2016-08-11},
	institution = {Social Science Research Network},
	author = {Argentesi, Elena and Ivaldi, Marc},
	month = jun,
	year = {2005},
	keywords = {Two-Sided Markets, market definition, printed media},
	file = {Snapshot:/Users/Franzi/Zotero/storage/8E7KQG5E/papers.html:text/html}
}

@article{argentesi_estimating_2007,
	title = {Estimating market power in a two-sided market: {The} case of newspapers},
	volume = {22},
	shorttitle = {Estimating market power in a two-sided market},
	url = {https://ideas.repec.org/a/jae/japmet/v22y2007i7p1247-1266.html},
	abstract = {The newspaper industry is a two-sided market: the readers market and the advertising market are closely linked by inter-market network externalities. We estimate market power in the Italian newspaper industry by building a structural model which encompasses a demand estimation for differentiated products on both sides of the market and where profit maximization by the publishing firms takes into account the interaction between them. The question that we address is whether the observed price pattern is consistent with profit-maximizing behaviour by competing firms or is instead driven by some form of (tacit or explicit) coordinated practice. Copyright © 2007 John Wiley \& Sons, Ltd.},
	number = {7},
	urldate = {2016-08-11},
	journal = {Journal of Applied Econometrics},
	author = {Argentesi, Elena and Filistrucchi, Lapo},
	year = {2007},
	pages = {1247--1266},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/GTW7AKKD/v22y2007i7p1247-1266.html:text/html}
}

@article{affeldt_upward_2013,
	title = {Upward {Pricing} {Pressure} in {Two}‐sided {Markets}},
	volume = {123},
	url = {https://ideas.repec.org/a/ecj/econjl/v123y2013i11pf505-f523.html},
	abstract = {Pricing pressure indices have recently been proposed as alternative screening devices for horizontal mergers involving differentiated products. We extend the concept of Upward Pricing Pressure (UPP) proposed by Farrell and Shapiro (2010) to two-sided markets. Examples of such markets are the newspaper market, where the demand for advertising is related to the number of readers, and the market for online search, where advertising demand depends on the number of users. The formulas we derive are useful for screening mergers among two-sided platforms. Due to the two-sidedness they depend on four sets of diversion ratios that can either be estimated using market-level demand data or elicited in surveys. In an application, we evaluate a hypothetical merger in the Dutch daily newspaper market. Our results indicate that it is important to take the two-sidedness of the market into account when evaluating UPP.{\textless}P{\textgreater}(This abstract was borrowed from another version of this item.)},
	number = {11},
	urldate = {2016-08-11},
	journal = {Economic Journal},
	author = {Affeldt, Pauline and Filistrucchi, Lapo and Klein, Tobias J.},
	year = {2013},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/TVWECFJK/v123y2013i11pf505-f523.html:text/html}
}

@techreport{filistrucchi_assessing_2012,
	type = {Chapters},
	title = {Assessing {Unilateral} {Merger} {Effects} in the {Dutch} {Daily} {Newspaper} {Market}},
	url = {https://ideas.repec.org/h/elg/eechap/14933_10.html},
	abstract = {Bringing scholars and policymakers to the frontiers of research and addressing the critical issues of the day, the book presents original important new theoretical and empirical results. The distinguished contributors include: P. Agrel, K. Alexander, J. CrÃ©mer, X. Dassiou, G. Deltas, F. Etro, L. Filistrucchi, P. Fotis, M. Gilli, J. Harrington Jr, T. Huertas, M. Ivaldi, B. Jullien, V. Marques, M. Peitz, Y. Spiegel, E. Tarrantino and G. Wood.},
	urldate = {2016-08-11},
	institution = {Edward Elgar Publishing},
	author = {Filistrucchi, Lapo and Klein, Tobias J. and Michielsen, Thomas},
	year = {2012},
	keywords = {Economics and Finance},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/ZW6N2FWR/14933_10.html:text/html}
}

@techreport{filistrucchi_market_2013,
	type = {Working {Papers} - {Economics}},
	title = {Market {Definition} in {Two}-{Sided} {Markets}: {Theory} and {Practice}},
	shorttitle = {Market {Definition} in {Two}-{Sided} {Markets}},
	url = {https://ideas.repec.org/p/frz/wpaper/wp2013_05.rdf.html},
	abstract = {Drawing from the economics of two-sided markets, we provide suggestions for the definition of the relevant market in cases involving two-sided platforms, such as media outlets, online intermediaries, payment cards companies and auction houses. We also discuss when a one-sided approach may be harmless and when instead it can potentially lead to a wrong decision. We then show that the current practice of market definition in two-sided markets is only in part consistent with the above suggestions. Divergence between our suggestions and practice is due to the failure to fully incorporate the lessons from the economic theory of two-sided markets, to the desire to be consistent with previous practice and to the higher data requirements and the higher complexity of empirical analysis in cases involving two-sided platforms. In particular, competition authorities have failed to recognize the crucial difference between two-sided transaction and non-transaction markets and have been misled by the traditional argument that where there is no price, there is no market.},
	number = {wp2013\_05.rdf},
	urldate = {2016-08-11},
	institution = {Universita' degli Studi di Firenze, Dipartimento di Scienze per l'Economia e l'Impresa},
	author = {Filistrucchi, Lapo and Geradin, Damien and Damme, Eric van and Affeldt, Pauline},
	year = {2013},
	keywords = {Two-Sided Markets, market definition, two-sided platforms, SSNIP test, Hypothetical Monopolist test},
	file = {RePEc PDF:/Users/Franzi/Zotero/storage/DJSRMECS/Filistrucchi et al. - 2013 - Market Definition in Two-Sided Markets Theory and.pdf:application/pdf;RePEc Snapshot:/Users/Franzi/Zotero/storage/MV5CNC4H/wp2013_05.rdf.html:text/html}
}

@article{evans_economics_2013,
	title = {Economics {Of} {Vertical} {Restraints} {For} {Multi}-{Sided} {Platforms}},
	volume = {9},
	url = {https://ideas.repec.org/a/cpi/cpijrn/9.1.2013i=10842.html},
	abstract = {David S. Evans discusses the pro and anticompetitive uses of vertical restraints in multi-sided platforms. His paper notes that vertical restraints can assist in creating value, for example, by aiding in demand consolidation in a single platform or by ensuring a greater supply of liquidity to platform participants. Evans goes on to describe how certain types of restraints can lead to procompetitive benefits or instead be used in an anticompetitive fashion, for example, by preventing a platform from attaining enough demand (critical mass) on one side.},
	urldate = {2016-08-11},
	journal = {CPI Journal},
	author = {Evans, David S.},
	year = {2013},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/6UQUPB7I/9.1.2013i=10842.html:text/html}
}

@article{evans_consensus_2013,
	title = {The {Consensus} {Among} {Economists} on {Multisided} {Platforms} and the {Implications} for {Excluding} {Evidence} {That} {Ignores} {It}},
	volume = {6},
	url = {https://ideas.repec.org/a/cpi/atchrn/6.1.2013i=11096.html},
	abstract = {Economic evidence that fails to account for interdependent demand between customer groups of multisided platforms is not reliable and should not be accorded any weight in decisions by courts or competition authorities. David S. Evans (Global Economics Group)},
	urldate = {2016-08-11},
	journal = {Antitrust Chronicle},
	author = {Evans, David S.},
	year = {2013},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/SPNGAKHV/6.1.2013i=11096.html:text/html}
}

@article{evans_industrial_2007,
	title = {The {Industrial} {Organization} of {Markets} with {Two}-{Sided} {Platforms}},
	volume = {3},
	url = {https://ideas.repec.org/a/cpi/cpijrn/3.1.2007i=4907.html},
	abstract = {This paper provides a brief introduction to the economics of two-sided platforms and the implications for antitrust analysis.},
	urldate = {2016-08-11},
	journal = {CPI Journal},
	author = {Evans, David S. and Schmalensee, Richard},
	year = {2007},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/B876CR9E/3.1.2007i=4907.html:text/html}
}

@article{evans_economics_2008,
	title = {The {Economics} of the {Online} {Advertising} {Industry}},
	volume = {7},
	url = {https://ideas.repec.org/a/bpj/rneart/v7y2008i3n2.html},
	abstract = {Internet-based technologies are revolutionizing the stodgy \$625 billion global advertising industry. There are a number of public policy issues to consider. Will a single ad platform emerge or will several remain viable? What are the consequences of alternative market structures for a web economy that is increasingly based on selling eyeballs to advertisers? This article describes the online advertising industry. The industry is populated by a number of multi-sided platforms that facilitate connecting advertisers to viewers. Search-based advertising platforms, the most developed of these, have interesting economic features that result from the combination of keyword bidding by advertisers and single-homing.},
	number = {3},
	urldate = {2016-08-11},
	journal = {Review of Network Economics},
	author = {Evans, David S.},
	year = {2008},
	pages = {1--33},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/Z6B7DBRS/v7y2008i3n2.html:text/html}
}

@article{ljung_measure_1978,
	title = {On a measure of lack of fit in time series models},
	volume = {65},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/65/2/297},
	doi = {10.1093/biomet/65.2.297},
	abstract = {The overall test for lack of fit in autoregressive-moving average models proposed by Box \& Pierce (1970) is considered. It is shown that a substantially improved approximation results from a simple modification of this test. Some consideration is given to the power of such tests and their robustness when the innovations are nonnormal. Similar modifications in the overall tests used for transfer function-noise models are proposed},
	language = {en},
	number = {2},
	urldate = {2016-08-15},
	journal = {Biometrika},
	author = {Ljung, G. M. and Box, G. E. P.},
	month = jan,
	year = {1978},
	keywords = {Autoregressive–moving average model, Residual autocorrelation, Test for lack of fit, Transfer function-noise model},
	pages = {297--303},
	file = {Snapshot:/Users/Franzi/Zotero/storage/VFVN64QS/297.html:text/html}
}

@techreport{evans_defining_2007,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Defining {Markets} that {Involve} {Multi}-{Sided} {Platform} {Businesses}: {An} {Empirical} {Framework} with an {Application} to {Google}'s {Purchase} of {DoubleClick}},
	shorttitle = {Defining {Markets} that {Involve} {Multi}-{Sided} {Platform} {Businesses}},
	url = {http://papers.ssrn.com/abstract=1089073},
	abstract = {A multi-sided platform (MSP) serves as an intermediary for two or more groups of customers who are linked by indirect network effects. Recent research has found that MSPs are significant in many industries and that some standard economic results - such as the Lerner Index - do not apply to them, in material ways, without some significant modification to take linkages between the multiple sides into account. This article extends several key tools used for the analysis of mergers to situations in which one or more of the suppliers are MSPs. It shows that the application of traditional tools to mergers involving MSPs results in biases the direction of which depends on the particular tool being used and other conditions. It also extends these tools to the analysis of the merger of MSPs. The techniques are illustrated with an application to an acquisition by Google in the online advertising industry.},
	number = {ID 1089073},
	urldate = {2016-08-11},
	institution = {Social Science Research Network},
	author = {Evans, David S. and Noel, Michael D.},
	month = nov,
	year = {2007},
	keywords = {SSRN, Michael D.  Noel, David S. Evans, Defining Markets that Involve Multi-Sided Platform Businesses: An Empirical Framework with an Application to Google's Purchase of DoubleClick},
	file = {Snapshot:/Users/Franzi/Zotero/storage/7Z5RZP4R/papers.html:text/html}
}

@book{kaltenhauser_abstimmung_2005,
	address = {Wiesbaden},
	edition = {2005},
	title = {Abstimmung am {Kiosk}: {Der} {Einfluss} der {Titelseitengestaltung} politischer {Publikumszeitschriften} auf die {Einzelverkaufsauflage}},
	isbn = {978-3-8244-4617-9},
	shorttitle = {Abstimmung am {Kiosk}},
	language = {Deutsch},
	publisher = {Deutscher Universitats-Verlag},
	author = {Kaltenhäuser, Bettina},
	month = jan,
	year = {2005}
}

@misc{noauthor_eur-lex_nodate,
	title = {{EUR}-{Lex} - 31997Y1209(01) - {EN} - {EUR}-{Lex}},
	url = {http://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:31997Y1209(01)},
	urldate = {2016-08-11},
	file = {EUR-Lex - 31997Y1209(01) - EN - EUR-Lex:/Users/Franzi/Zotero/storage/FJ39F3TE/ALL.html:text/html}
}

@article{european_commission_commission_1997,
	title = {Commission notice on the definition of the relevant market},
	volume = {C371},
	url = {http://www.euchinacomp.org/attachments/article/18/05_Commission_Notice_on_the_definition_of_the_relevant_market_EN.pdf},
	number = {09 December},
	urldate = {2016-08-11},
	journal = {Official Journal},
	author = {{European Commission}},
	year = {1997},
	file = {5. Commission notice on the definition of the relevant market - 05_Commission_Notice_on_the_definition_of_the_relevant_market_EN.pdf:/Users/Franzi/Zotero/storage/RID596Z7/05_Commission_Notice_on_the_definition_of_the_relevant_market_EN.html:text/html}
}

@article{kaiser_price_2006,
	title = {Price structure in two-sided markets: {Evidence} from the magazine industry},
	volume = {24},
	issn = {0167-7187},
	shorttitle = {Price structure in two-sided markets},
	url = {http://econpapers.repec.org/article/eeeindorg/v_3a24_3ay_3a2006_3ai_3a1_3ap_3a1-28.htm},
	number = {1},
	urldate = {2016-08-11},
	journal = {International Journal of Industrial Organization},
	author = {Kaiser, Ulrich and Wright, Julian},
	year = {2006},
	pages = {1--28},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/NSC8QSNB/v_3a24_3ay_3a2006_3ai_3a1_3ap_3a1-28.html:text/html}
}

@book{motta_competition_2004,
	title = {Competition {Policy}: {Theory} and {Practice}},
	isbn = {978-0-521-01691-9},
	shorttitle = {Competition {Policy}},
	abstract = {This is the first book to provide a systematic treatment of the economics of antitrust (or competition policy) in a global context. It draws on the literature of industrial organisation and on original analyses to deal with such important issues as cartels, joint-ventures, mergers, vertical contracts, predatory pricing, exclusionary practices, and price discrimination, and to formulate policy implications on these issues. The interaction between theory and practice is one of the main features of the book, which contains frequent references to competition policy cases and a few fully developed case studies. The treatment is written to appeal to practitioners and students, to lawyers and economists. It is not only a textbook in economics for first year graduate or advanced undergraduate courses, but also a book for all those who wish to understand competition issues in a clear and rigorous way. Exercises and some solved problems are provided.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Motta, Massimo},
	month = jan,
	year = {2004},
	note = {Google-Books-ID: \_gLD41DfaJwC},
	keywords = {Business \& Economics / General, Business \& Economics / International / General, Business \& Economics / Economics / General, Business \& Economics / Industries / General, Business \& Economics / Industrial Management, Political Science / Public Policy / Economic Policy}
}

@article{evans_analysis_2008,
	title = {The {Analysis} of {Mergers} {That} {Involve} {Multisided} {Platform} {Businesses}},
	volume = {4},
	issn = {1744-6414, 1744-6422},
	url = {http://jcle.oxfordjournals.org/content/4/3/663},
	doi = {10.1093/joclec/nhn017},
	abstract = {A multisided platform (MSP) serves as an intermediary for two or more groups of customers who are linked by indirect network effects. Recent research has found that MSPs are significant in many industries and that some standard economic results—such as the Lerner Index—do not apply to them, in material ways, without some significant modification to take linkages between the multiple sides into account. This article extends several key tools used for the analysis of mergers to situations in which one or more of the suppliers are MSPs. It shows that the application of traditional tools to mergers involving MSPs results in biases, the direction of which depends on the particular tool being used and other conditions. It also extends these tools to the analysis of the merger of MSPs. The techniques are illustrated with an application to an acquisition involving the multisided online advertising industry.},
	language = {en},
	number = {3},
	urldate = {2016-08-11},
	journal = {Journal of Competition Law and Economics},
	author = {Evans, David S. and Noel, Michael D.},
	month = jan,
	year = {2008},
	pages = {663--695},
	file = {Snapshot:/Users/Franzi/Zotero/storage/NG9A9ZSJ/663.html:text/html}
}

@article{granger_spurious_1974,
	title = {Spurious regressions in econometrics},
	volume = {2},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407674900347},
	doi = {10.1016/0304-4076(74)90034-7},
	number = {2},
	urldate = {2016-08-15},
	journal = {Journal of Econometrics},
	author = {Granger, C. W. J. and Newbold, P.},
	month = jul,
	year = {1974},
	pages = {111--120},
	file = {ScienceDirect Snapshot:/Users/Franzi/Zotero/storage/ADJ5IIRV/0304407674900347.html:text/html}
}

@article{chandra_mergers_2009,
	title = {Mergers in {Two}-{Sided} {Markets}: {An} {Application} to the {Canadian} {Newspaper} {Industry}},
	volume = {18},
	issn = {1530-9134},
	shorttitle = {Mergers in {Two}-{Sided} {Markets}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1530-9134.2009.00237.x/abstract},
	doi = {10.1111/j.1530-9134.2009.00237.x},
	abstract = {In this paper, we study mergers in two-sided industries and, in particular, the effects of mergers in the newspaper industry. We present a model which shows that mergers in two-sided markets may not necessarily lead to higher prices for either side of the market. We test our conclusions by examining a spate of mergers in the Canadian newspaper industry in the late 1990s. Specifically, we analyze prices for both circulation and advertising to try to understand the impact that these mergers had on consumer welfare. We find that greater concentration did not lead to higher prices for either newspaper subscribers or advertisers.},
	language = {en},
	number = {4},
	urldate = {2016-08-11},
	journal = {Journal of Economics \& Management Strategy},
	author = {Chandra, Ambarish and Collard-Wexler, Allan},
	month = dec,
	year = {2009},
	pages = {1045--1070},
	file = {Snapshot:/Users/Franzi/Zotero/storage/Q2S4BAT6/abstract.html:text/html}
}

@book{shubik_market_1980,
	title = {Market {Structure} and {Behavior}},
	isbn = {978-0-674-55026-1},
	language = {en},
	publisher = {Harvard University Press},
	author = {Shubik, Martin and Levitan, Richard},
	year = {1980},
	note = {Google-Books-ID: j626AAAAIAAJ},
	keywords = {Business \& Economics / Economics / General, Business \& Economics / Industries / General, Business \& Economics / Commerce, Business \& Economics / Organizational Behavior, Mathematics / Game Theory}
}

@article{dixit_model_1979,
	title = {A {Model} of {Duopoly} {Suggesting} a {Theory} of {Entry} {Barriers}},
	volume = {10},
	issn = {0361-915X},
	url = {http://www.jstor.org/stable/3003317},
	doi = {10.2307/3003317},
	abstract = {This paper analyzes a model of duopoly with fixed costs. Leadership by one "established" firm may yield an outcome in which the second is inactive, but entry prevention is not a prior constraint. We find that two aspects of product differentiation have distinct effects: an absolute advantage in demand for the established firm makes entry harder, but a lower cross-price effect facilitates it. In the basic model we maintain the same quantity after entry. An extension of the model deals with the case where the threat of a predatory output increase after entry is made credible by carrying excess capacity prior to entry.},
	number = {1},
	urldate = {2016-08-11},
	journal = {The Bell Journal of Economics},
	author = {Dixit, Avinash},
	year = {1979},
	pages = {20--32}
}

@techreport{kind_competition_2006,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Competition for {Viewers} and {Advertisers} in a {TV} {Oligopoly}},
	url = {http://papers.ssrn.com/abstract=949725},
	abstract = {We consider a model of a TV oligopoly where TV channels transmit advertising and viewers dislike such commercials. We show that advertisers make a lower profit the larger the number of TV channels. If TV channels are sufficiently close substitutes, there will be underprovision of advertising relative to social optimum. We also find that the more viewers dislike ads, the more likely it is that welfare is increasing in the number of advertising financed TV channels. A publicly owned TV channel can partly correct market distortions, in some cases by having a larger amount of advertising than private TV channels. It may even have advertising in cases where advertising is wasteful per se.},
	number = {ID 949725},
	urldate = {2016-08-11},
	institution = {Social Science Research Network},
	author = {Kind, Hans Jarle and Nilssen, Tore and Sorgard, Lars},
	month = nov,
	year = {2006},
	keywords = {television industry, advertising},
	file = {Snapshot:/Users/Franzi/Zotero/storage/BHJN4CI2/papers.html:text/html}
}

@book{vogel_populare_1998,
	title = {Die populäre {Presse} in {Deutschland}: ihre {Grundlagen}, {Strukturen} und {Strategien}},
	isbn = {978-3-88927-222-5},
	shorttitle = {Die populäre {Presse} in {Deutschland}},
	language = {de},
	publisher = {Fischer},
	author = {Vogel, Andreas},
	year = {1998},
	note = {Google-Books-ID: FPhWAAAACAAJ}
}

@techreport{kind_business_2009,
	type = {{CESifo} {Working} {Paper} {Series}},
	title = {Business {Models} for {Media} {Firms}: {Does} {Competition} {Matter} for how they {Raise} {Revenue}?},
	shorttitle = {Business {Models} for {Media} {Firms}},
	url = {https://ideas.repec.org/p/ces/ceswps/_2713.html},
	abstract = {The purpose of this article is to analyze how competitive forces may influence the way media firms like TV channels raise revenue. A media firm can either be financed by advertising revenue, by direct payment from the viewers (or the readers, if we consider newspapers), or by both. We show that the scope for raising revenues from consumer payment is constrained by other media firms offering close substitutes. This implies that the less differentiated the media firms’ content, the larger is the fraction of their revenue coming from advertising. A media firm’s scope for raising revenues from ads, on the other hand, is constrained by how many competitors it faces. We should thus expect that direct payment from the media consumers becomes more important the larger the number of competing media products.},
	number = {2713},
	urldate = {2016-08-11},
	institution = {CESifo Group Munich},
	author = {Kind, Hans Jarle and Nilssen, Tore and Sørgard, Lars},
	year = {2009},
	file = {RePEc PDF:/Users/Franzi/Zotero/storage/PZ3BUB7A/Kind et al. - 2009 - Business Models for Media Firms Does Competition .pdf:application/pdf;RePEc Snapshot:/Users/Franzi/Zotero/storage/CZ995T23/_2713.html:text/html}
}

@book{dewenter_einfuhrung_2014,
	title = {Einführung in die neue Ökonomie der {Medienmärkte}: {Eine} wettbewerbsökonomische {Betrachtung} aus {Sicht} der {Theorie} der zweiseitigen {Märkte}},
	isbn = {978-3-658-04736-8},
	shorttitle = {Einführung in die neue Ökonomie der {Medienmärkte}},
	abstract = {Das vorliegende Buch stellt erstmals die Theorie der zweiseitigen Märkte und deren Anwendung auf Medienmärkte intuitiv sowie modelltheoretisch dar. Nach einer Diskussion der ökonomischen Grundlagen werden relevante Modelle zweiseitiger Medienplattformen sowie Anwendungen für die Wettbewerbspolitik besprochen. Anschließend werden die wirtschaftspolitischen Implikationen der Theorie dargestellt. Anhand von realen Wettbewerbsfällen auf Internet-, Zeitungs- und Zeitschriftenmärkten wird diskutiert, ob und inwiefern Medienplattformen einer unterschiedlichen wettbewerbspolitischen und -rechtlichen Behandlung bedürfen. Das Buch dient damit sowohl den Studierenden der Wirtschaftswissenschaften und des Wettbewerbsrechts zum Verständnis der modernen Medienökonomik. Es gibt aber ebenso Hinweise für die wettbewerbspolitische Analyse von Medienmärkten in der Fallpraxis.},
	language = {de},
	publisher = {Springer-Verlag},
	author = {Dewenter, Ralf and Rösch, Jürgen},
	month = oct,
	year = {2014},
	note = {Google-Books-ID: 7uXSBAAAQBAJ},
	keywords = {Business \& Economics / General, Political Science / Public Policy / Economic Policy, Business \& Economics / Industries / Media \& Communications, Business \& Economics / Management}
}

@article{picard_digitization_2011,
	title = {Digitization and {Media} {Business} {Models}},
	url = {https://www.opensocietyfoundations.org/sites/default/files/digitization-media-business-models-20110721.pdf},
	urldate = {2016-08-26},
	journal = {Open Society Foundations},
	author = {Picard, Robert G.},
	month = jul,
	year = {2011},
	file = {Digitization and Media Business Models-final.indd - digitization-media-business-models-20110721.pdf:/Users/Franzi/Zotero/storage/FE378CMX/digitization-media-business-models-20110721.html:text/html}
}

@inproceedings{cabyova_impact_2014,
	title = {{THE} {IMPACT} {OF} {DIGITIZATION} {ON} {ADVERTISING} {IN} {PRINT} {MEDIA}},
	volume = {2},
	isbn = {978-619-7105-23-0},
	url = {http://dx.doi.org/10.5593/sgemsocial2014/B12/S2.124},
	doi = {10.5593/sgemsocial2014/B12/S2.124},
	abstract = {Monitored incomes from the sale of advertising space in print media have continuously declined since 2009, and despite a slight increase in 2013, the volume of sold advertising in newspapers and magazines significantly lags behind the advertising in the electronic media, especially in television. And it is despite the fact that several studies conducted on the effectiveness and efficiency of the so-called print advertising confirm that advertising in newspapers and magazines positively influences buying behavior of consumers. One of the causes of the low volume sales of advertising space in print media can be considered as decreasing marketability of newspapers and magazines and easier access to their online editions, which are for the reader in most cases free, respectively at low price compared to the annual pre-paid printed edition of the periodical. The paper analyzes the way of use of the advertising space in print media and its electronic editions on the market for mobile operators. It examines the attractiveness of advertising space in newspapers and magazines in comparison to their electronic version. The aim is to evaluate the impact of digitization on the use of advertising space in print media and evaluate the potential of advertising space in online editions of these media in terms of advertisers},
	urldate = {2016-08-26},
	booktitle = {{SGEM}2014 {Conference} on {Psychology} and {Psychiatry}, {Sociology} and {Healthcare}, {Education}},
	publisher = {Stef 92 Technology},
	author = {Cabyova, L and Krajcovic, P and Ptacin, J},
	year = {2014},
	keywords = {advertising, 2014, digitization, internet, magazines, newspapers},
	pages = {969--976 pp}
}

@article{granger_investigating_1969,
	title = {Investigating {Causal} {Relations} by {Econometric} {Models} and {Cross}-spectral {Methods}},
	volume = {37},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/1912791},
	doi = {10.2307/1912791},
	abstract = {There occurs on some occasions a difficulty in deciding the direction of causality between two related variables and also whether or not feedback is occurring. Testable definitions of causality and feedback are proposed and illustrated by use of simple two-variable models. The important problem of apparent instantaneous causality is discussed and it is suggested that the problem often arises due to slowness in recording information or because a sufficiently wide class of possible causal variables has not been used. It can be shown that the cross spectrum between two variables can be decomposed into two parts, each relating to a single causal arm of a feedback situation. Measures of causal lag and causal strength can then be constructed. A generalisation of this result with the partial cross spectrum is suggested.},
	number = {3},
	urldate = {2016-08-26},
	journal = {Econometrica},
	author = {Granger, C. W. J.},
	year = {1969},
	pages = {424--438}
}

@article{akaike_new_1974,
	title = {A new look at the statistical model identification},
	volume = {19},
	issn = {0018-9286},
	doi = {10.1109/TAC.1974.1100705},
	abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Akaike, H.},
	month = dec,
	year = {1974},
	keywords = {Parameter identification, Time series, maximum-likelihood (ML) estimation, Art, Estimation theory, History, Linear systems, Maximum likelihood estimation, Roundoff errors, Sampling methods, Stochastic processes, Testing, Time series analysis},
	pages = {716--723},
	file = {IEEE Xplore Abstract Record:/Users/Franzi/Zotero/storage/S42JGXF8/1100705.html:text/html;IEEE Xplore Full Text PDF:/Users/Franzi/Zotero/storage/3EH52Q8T/Akaike - 1974 - A new look at the statistical model identification.pdf:application/pdf}
}

@article{schwarz_estimating_1978,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1176344136},
	doi = {10.1214/aos/1176344136},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	language = {EN},
	number = {2},
	urldate = {2016-08-29},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	month = mar,
	year = {1978},
	mrnumber = {MR468014},
	zmnumber = {0379.62005},
	keywords = {Dimension, Akaike information criterion, asymptotics},
	pages = {461--464},
	file = {Snapshot:/Users/Franzi/Zotero/storage/BPFE84BA/DPubS.html:text/html}
}

@article{stigler_extent_1985,
	title = {The {Extent} of the {Market}},
	volume = {28},
	issn = {0022-486},
	url = {http://chicagounbound.uchicago.edu/jle/vol28/iss3/4},
	number = {3},
	journal = {Journal of Law and Economics},
	author = {Stigler, George and Sherwin, Robert},
	month = oct,
	year = {1985},
	file = {"The Extent of the Market" by George J. Stigler and Robert A. Sherwin:/Users/Franzi/Zotero/storage/SQSCWVZB/4.html:text/html}
}

@book{dewenter_essays_2004,
	title = {Essays on {Interrelated} {Media} {Markets}},
	isbn = {978-3-8329-0697-9},
	language = {en},
	publisher = {Nomos},
	author = {Dewenter, Ralf},
	month = jan,
	year = {2004},
	note = {Google-Books-ID: O0FiAAAAMAAJ}
}

@article{tiao_modeling_1981,
	title = {Modeling {Multiple} {Times} {Series} with {Applications}},
	volume = {76},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2287575},
	doi = {10.2307/2287575},
	abstract = {An approach to the modeling and analysis of multiple time series is proposed. Properties of a class of vector autoregressive moving average models are discussed. Modeling procedures consisting of tentative specification, estimation, and diagnostic checking are outlined and illustrated by three real examples.},
	number = {376},
	urldate = {2016-08-29},
	journal = {Journal of the American Statistical Association},
	author = {Tiao, G. C. and Box, G. E. P.},
	year = {1981},
	pages = {802--816}
}

@article{leonello_horizontal_2010,
	title = {Horizontal {Mergers} in {Two}-{Sided} {Markets}},
	url = {https://www.researchgate.net/publication/242492919_Horizontal_Mergers_in_Two-Sided_Markets},
	abstract = {The paper analyzes the eects of mergers in two-sided markets. In these markets two distinct groups of agents interact through one or more plat- forms. I show that, in two-sided markets, the...},
	urldate = {2016-11-03},
	journal = {ResearchGate},
	author = {Leonello, Agnese},
	month = jan,
	year = {2010},
	file = {Snapshot:/Users/Franzi/Zotero/storage/MPE6UNXA/242492919_Horizontal_Mergers_in_Two-Sided_Markets.html:text/html}
}

@article{evans_antitrust_2003,
	title = {The {Antitrust} {Economics} of {Multi}-{Sided} {Platform} {Markets}},
	volume = {20},
	issn = {0741-9457},
	url = {http://digitalcommons.law.yale.edu/yjreg/vol20/iss2/4},
	number = {2},
	journal = {Yale Journal on Regulation},
	author = {Evans, David S.},
	month = jan,
	year = {2003},
	file = {"The Antitrust Economics of Multi-Sided Platform Markets" by David S. Evans:/Users/Franzi/Zotero/storage/Z8SPAGTV/4.html:text/html}
}

@article{white_market_2006,
	title = {Market {Definition} and {Market} {Power} in {Payment} {Card} {Networks}: {Some} {Comments} and {Considerations}},
	volume = {5},
	issn = {1446-9022, 2194-5993},
	shorttitle = {Market {Definition} and {Market} {Power} in {Payment} {Card} {Networks}},
	url = {http://www.degruyter.com/view/j/rne.2006.5.issue-1/rne.2006.5.1.1089/rne.2006.5.1.1089.xml},
	doi = {10.2202/1446-9022.1089},
	number = {1},
	urldate = {2016-11-03},
	journal = {Review of Network Economics},
	author = {White, Lawrence J.},
	month = jan,
	year = {2006}
}

@article{emch_market_2006,
	title = {Market {Definition} and {Market} {Power} in {Payment} {Card} {Networks}},
	volume = {5},
	issn = {1446-9022},
	url = {https://www.degruyter.com/view/j/rne.2006.5.issue-1/rne.2006.5.1.1088/rne.2006.5.1.1088.xml},
	doi = {10.2202/1446-9022.1088},
	abstract = {We discuss competition among payment card networks, and in particular how antitrust practitioners might approach questions of market definition and market power in these markets. Application of the hypothetical monopolist test to define markets, and the use of traditional metrics to measure market power, may be less straightforward for card networks than for many markets. The "two-sidedness" of the market does not, however, overturn the basic logic of the hypothetical monopolist test or traditional measurements of market power. We demonstrate some practical ways to apply these antitrust principles to competition among payment card networks.},
	number = {1},
	urldate = {2016-11-03},
	journal = {Review of Network Economics},
	author = {Emch, Eric and Thompson, T. Scott},
	year = {2006}
}

@article{alexandrov_antitrust_2011,
	title = {Antitrust and {Competition} in {Two}-{Sided} {Markets}},
	volume = {7},
	issn = {1744-6414, 1744-6422},
	url = {http://jcle.oxfordjournals.org/content/7/4/775},
	doi = {10.1093/joclec/nhr012},
	abstract = {This article extends antitrust analysis to two-sided markets in which a virtual monopolist competes with local bricks-and-mortar dealers. The discussion examines the market power of an Internet market maker as well as an Internet matchmaker. The analysis shows that equilibrium in a two-sided market can be characterized as a one-sided market in which transaction demand depends on the bid-ask spread of the central market maker. This allows for a straightforward extension of critical demand elasticity and critical loss analysis from one-sided markets to two-sided markets, with antitrust tests based on the hypothetical monopolist's bid-ask spread. Antitrust analysis of a one-sided market also carries over to a two-sided market with a matchmaker where antitrust tests are based on the sum of participation fees.},
	language = {en},
	number = {4},
	urldate = {2016-11-03},
	journal = {Journal of Competition Law and Economics},
	author = {Alexandrov, Alexei and Deltas, George and Spulber, Daniel F.},
	month = jan,
	year = {2011},
	pages = {775--812},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/9WVSWFQE/Alexandrov et al. - 2011 - Antitrust and Competition in Two-Sided Markets.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/NPSKPTWR/775.html:text/html}
}

@article{evans_economic_2002,
	title = {Some {Economic} {Aspects} of {Antitrust} {Analysis} in {Dynamically} {Competitive} {Industries}},
	url = {http://www.nber.org/chapters/c10784},
	urldate = {2016-11-15},
	journal = {NBER},
	author = {Evans, David S. and Schmalensee, Richard},
	month = jan,
	year = {2002},
	pages = {1--50},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/4WDJV747/Evans und Schmalensee - 2002 - Some Economic Aspects of Antitrust Analysis in Dyn.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/PBHM4RAH/c10784.html:text/html}
}

@techreport{gual_market_2003,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Market {Definition} in the {Telecoms} {Industry}},
	url = {https://papers.ssrn.com/abstract=462280},
	abstract = {Market definition for antitrust purposes is by now firmly rooted in economic analysis both in the US and the EU, even if the approaches are slightly different. This paper examines the theoretical basis for the legal definitions and assesses whether the general principles need to be adapted when dealing with the telecommunications services industry. The paper finds that the conventional antitrust methodology for market definition can be, to a large extent, readily applied to the telecoms industry but points out some key adjustments that have to be made to this methodology to ensure that the antitrust and regulatory authorities end up defining markets which capture adequately the nature of the competitive interaction in this industry. The definition of markets should be based on a detailed analysis of demand (both complementarities and substitutabilities) and the consideration of all companies which have the assets and capabilities to satisfy these consumer needs. Such an exercise should be done first, and distinguished from the subsequent analysis of the competitive conditions in the markets defined as relevant.},
	number = {ID 462280},
	urldate = {2016-11-15},
	institution = {Social Science Research Network},
	author = {Gual, Jordi},
	month = sep,
	year = {2003},
	keywords = {market definition, Telecommunications, regulation, antitrus},
	file = {Snapshot:/Users/Franzi/Zotero/storage/VUFBFZI8/papers.html:text/html}
}

@article{schaerr_cellophane_1985,
	title = {The {Cellophane} {Fallacy} and the {Justice} {Department}'s {Guidelines} for {Horizontal} {Mergers}},
	volume = {94},
	issn = {0044-0094},
	url = {http://www.jstor.org/stable/796239},
	doi = {10.2307/796239},
	number = {3},
	urldate = {2016-11-15},
	journal = {The Yale Law Journal},
	author = {Schaerr, Gene C.},
	year = {1985},
	pages = {670--693}
}

@incollection{harrington_detecting_2008,
	address = {Cambridge MA},
	title = {Detecting {Cartels}},
	booktitle = {Handbook of {Antitrust} {Economics}. {Edited} by {P}. {Buccirossi}.},
	publisher = {The MIT Press},
	author = {Harrington, Joseph E.},
	year = {2008},
	file = {PhilPapers - Snapshot:/Users/Franzi/Zotero/storage/6JNG8853/undefined.html:text/html}
}

@misc{noauthor_bfm3a978-3-322-81373-22f1.pdf_nodate,
	title = {bfm\%3A978-3-322-81373-2\%2F1.pdf},
	url = {http://download.springer.com/static/pdf/590/bfm%253A978-3-322-81373-2%252F1.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fbook%2Fbfm%3A978-3-322-81373-2%2F1&token2=exp=1480077329~acl=%2Fstatic%2Fpdf%2F590%2Fbfm%25253A978-3-322-81373-2%25252F1.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Fbook%252Fbfm%253A978-3-322-81373-2%252F1*~hmac=296d205ff788e2400e43ef9f81a42c7a92af6e7ea7e7f9cb7a99e7301cd51fd6},
	urldate = {2016-11-25},
	file = {bfm%3A978-3-322-81373-2%2F1.pdf:/Users/Franzi/Zotero/storage/I7MJHIB3/bfm%3A978-3-322-81373-2%2F1.html:text/html}
}

@article{harvey_modeling_1997,
	title = {The {Modeling} and {Seasonal} {Adjustment} of {Weekly} {Observations}},
	volume = {15},
	issn = {0735-0015},
	url = {http://www.jstor.org/stable/1392339},
	doi = {10.2307/1392339},
	abstract = {Several important economic time series are recorded on a particular day every week. Seasonal adjustment of such series is difficult because the number of weeks varies between 52 and 53 and the position of the recording day changes from year to year. In addition certain festivals, most notably Easter, take place at different times according to the year. This article presents a solution to problems of this kind by setting up a structural time series model that allows the seasonal pattern to evolve over time and enables trend extraction and seasonal adjustment to be carried out by means of state-space filtering and smoothing algorithms. The method is illustrated with a Bank of England series on the money supply.},
	number = {3},
	urldate = {2016-11-25},
	journal = {Journal of Business \& Economic Statistics},
	author = {Harvey, Andrew and Koopman, Siem Jan and Riani, Marco},
	year = {1997},
	pages = {354--368}
}

@book{kirchgassner_introduction_2012,
	address = {Heidelberg},
	edition = {2nd ed. 2013},
	title = {Introduction to {Modern} {Time} {Series} {Analysis}},
	isbn = {978-3-642-33435-1},
	abstract = {This book presents modern developments in time series econometrics that are applied to macroeconomic and financial time series, bridging the gap between methods and realistic applications. It presents the most important approaches to the analysis of time series, which may be stationary or nonstationary. Modelling and forecasting univariate time series is the starting point. For multiple stationary time series, Granger causality tests and vector autogressive models are presented. As the modelling of nonstationary uni- or multivariate time series is most important for real applied work, unit root and cointegration analysis as well as vector error correction models are a central topic. Tools for analysing nonstationary data are then transferred to the panel framework. Modelling the (multivariate) volatility of financial time series with autogressive conditional heteroskedastic models is also treated.},
	language = {Englisch},
	publisher = {Springer},
	author = {Kirchgässner, Gebhard and Wolters, Jürgen and Hassler, Uwe},
	month = oct,
	year = {2012}
}

@article{evans_market_2012,
	title = {Market {Definition} and {Merger} {Analysis} for {Multi}-{Sided} {Platforms}},
	journal = {Competition Policy International, Inc.},
	author = {Evans, David S. and Schmalensee, Richard},
	month = nov,
	year = {2012}
}

@book{kaltenhaeuser_abstimmung_2005,
	title = {Abstimmung am {Kiosk}: {Der} {Einfluss} der {Titelseitengestaltung} politischer {Publikumszeitschriften} auf die {Einzelverkaufsauflage}},
	isbn = {978-3-8244-4617-9},
	publisher = {Deutscher Universitats-Verlag},
	author = {Kaltenhaeuser, Bettina},
	month = jan,
	year = {2005}
}

@book{vogel_populaere_1998,
	title = {Die populaere {Presse} in {Deutschland}: ihre {Grundlagen}, {Strukturen} und {Strategien}},
	publisher = {Fischer},
	author = {Vogel, Andreas},
	year = {1998}
}

@misc{noauthor_adaptive_1994,
	title = {Adaptive {User} {Support}: {Ergonomic} {Design} of {Manually} and {Automatically} {Adaptable} {Software}},
	shorttitle = {Adaptive {User} {Support}},
	url = {https://www.crcpress.com/Adaptive-User-Support-Ergonomic-Design-of-Manually-and-Automatically-Adaptable/Oppermann/p/book/9780805816556},
	abstract = {The potential of software applications to solve an array of office and administrative problems is increasing faster than the ability of users to exploit it. We need to make systems easier to learn and more comfortable to use. This book reports a major advance in the effort to accomplish both goals.},
	urldate = {2017-09-29},
	journal = {CRC Press},
	month = aug,
	year = {1994},
	file = {Snapshot:/Users/Franzi/Zotero/storage/BMVMZ5QD/9780805816556.html:text/html}
}

@book{oppermann_adaptive_1994,
	title = {Adaptive {User} {Support}: {Ergonomic} {Design} of {Manually} and {Automatically} {Adaptable} {Software}},
	shorttitle = {Adaptive {User} {Support}},
	url = {https://www.crcpress.com/Adaptive-User-Support-Ergonomic-Design-of-Manually-and-Automatically-Adaptable/Oppermann/p/book/9780805816556},
	abstract = {The potential of software applications to solve an array of office and administrative problems is increasing faster than the ability of users to exploit it. We need to make systems easier to learn and more comfortable to use. This book reports a major advance in the effort to accomplish both goals.},
	urldate = {2017-09-29},
	author = {Oppermann, Reinhard},
	month = aug,
	year = {1994},
	file = {Snapshot:/Users/Franzi/Zotero/storage/93V3R86R/9780805816556.html:text/html}
}

@article{adomavicius_toward_2005,
	title = {Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions},
	volume = {17},
	issn = {1041-4347},
	shorttitle = {Toward the next generation of recommender systems},
	doi = {10.1109/TKDE.2005.99},
	abstract = {This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Adomavicius, G. and Tuzhilin, A.},
	month = jun,
	year = {2005},
	keywords = {content-based retrieval, information filtering, collaborative filtering, content-based approach, contextual information, multicriteria rating estimation methods, recommender systems, Books, Business, Cognitive science, Collaboration, Collaborative work, Context modeling, Filtering, Hybrid power systems, Motion pictures, Index Terms- Recommender systems, extensions to recommender systems., rating estimation methods},
	pages = {734--749},
	file = {IEEE Xplore Abstract Record:/Users/Franzi/Zotero/storage/MEVCSH8T/1423975.html:text/html}
}

@inproceedings{nguyen_improving_2007,
	address = {New York, NY, USA},
	series = {{RecSys} '07},
	title = {Improving {New} {User} {Recommendations} with {Rule}-based {Induction} on {Cold} {User} {Data}},
	isbn = {978-1-59593-730-8},
	url = {http://doi.acm.org/10.1145/1297231.1297251},
	doi = {10.1145/1297231.1297251},
	abstract = {With recommender systems, users receive items recommended on the basis of their profile. New users experience the cold start problem: as their profile is very poor, the system performs very poorly. In this paper, classical new user cold start techniques are improved by exploiting the cold user data, i.e. the user data that is readily available (e.g. age, occupation, location, etc.), in order to automatically associate the new user with a better first profile. Relying on the existing α-community spaces model, a rule-based induction process is used and a recommendation process based on the "level of agreement" principle is defined. The experiments show that the quality of recommendations compares to that obtained after a classical new user technique, while the new user effort is smaller as no initial ratings are asked.},
	booktitle = {Proceedings of the 2007 {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Nguyen, An-Te and Denos, Nathalie and Berrut, Catherine},
	year = {2007},
	keywords = {collaborative filtering, recommender systems, cold start problem, cold user data, new-user problem, rule-based induction},
	pages = {121--128}
}

@article{koren_matrix_2009,
	title = {Matrix {Factorization} {Techniques} for {Recommender} {Systems}},
	volume = {42},
	issn = {0018-9162},
	doi = {10.1109/MC.2009.263},
	abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
	number = {8},
	journal = {Computer},
	author = {Koren, Y. and Bell, R. and Volinsky, C.},
	month = aug,
	year = {2009},
	keywords = {information filtering, recommender systems, Collaboration, Filtering, Motion pictures, matrix decomposition, retail data processing, Netflix Prize competition, matrix factorization technique, nearest neighbor technique, product recommendation system, recommender system, Bioinformatics, Genomics, Nearest neighbor searches, Predictive models, Sea measurements, Computational intelligence, Matrix factorization, Netflix Prize},
	pages = {30--37},
	file = {IEEE Xplore Abstract Record:/Users/Franzi/Zotero/storage/I5KIB5GX/5197422.html:text/html}
}

@inproceedings{korencic_getting_2015,
	address = {New York, NY, USA},
	series = {{TM} '15},
	title = {Getting the {Agenda} {Right}: {Measuring} {Media} {Agenda} {Using} {Topic} {Models}},
	isbn = {978-1-4503-3784-7},
	shorttitle = {Getting the {Agenda} {Right}},
	url = {http://doi.acm.org/10.1145/2809936.2809942},
	doi = {10.1145/2809936.2809942},
	abstract = {Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies.},
	booktitle = {Proceedings of the 2015 {Workshop} on {Topic} {Models}: {Post}-{Processing} and {Applications}},
	publisher = {ACM},
	author = {Korenčić, Damir and Ristov, Strahil and Šnajder, Jan},
	year = {2015},
	keywords = {topic modeling, agenda measuring, agenda setting, document tagging, multilabel classification, news media analysis},
	pages = {61--66}
}

@article{tetlock_giving_2007,
	title = {Giving {Content} to {Investor} {Sentiment}: {The} {Role} of {Media} in the {Stock} {Market}},
	volume = {62},
	issn = {1540-6261},
	shorttitle = {Giving {Content} to {Investor} {Sentiment}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	language = {en},
	number = {3},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	month = jun,
	year = {2007},
	pages = {1139--1168},
	file = {Snapshot:/Users/Franzi/Zotero/storage/R4FBIJ83/abstract.html:text/html}
}

@article{baker_measuring_2016,
	title = {Measuring {Economic} {Policy} {Uncertainty}},
	url = {http://www.nber.org/papers/w21633},
	abstract = {We develop a new index of economic policy uncertainty (EPU) based on newspaper coverage frequency. Several types of evidence – including human readings of 12,000 newspaper articles – indicate that our index proxies for movements in policy-related economic uncertainty. Our US index spikes near tight presidential elections, Gulf Wars I and II, the 9/11 attacks, the failure of Lehman Brothers, the 2011 debt-ceiling dispute and other major battles over fiscal policy. Using firm-level data, we find that policy uncertainty raises stock price volatility and reduces investment and employment in policy-sensitive sectors like defense, healthcare, and infrastructure construction. At the macro level, policy uncertainty innovations foreshadow declines in investment, output, and employment in the United States and, in a panel VAR setting, for 12 major economies. Extending our US index back to 1900, EPU rose dramatically in the 1930s (from late 1931) and has drifted upwards since the 1960s.},
	number = {4},
	journal = {Quarterly Journal of Economics},
	author = {Baker, Scott R. and Bloom, Nicholas and Davis, Steven J.},
	year = {2016},
	note = {DOI: 10.3386/w21633},
	pages = {1593--1636}
}

@techreport{gentzkow_text_2017,
	type = {Working {Paper}},
	title = {Text as {Data}},
	url = {http://www.nber.org/papers/w23276},
	abstract = {An ever increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.},
	number = {23276},
	institution = {National Bureau of Economic Research},
	author = {Gentzkow, Matthew and Kelly, Bryan T. and Taddy, Matt},
	month = mar,
	year = {2017},
	note = {DOI: 10.3386/w23276}
}

@article{blei_probabilistic_2012,
	title = {Probabilistic {Topic} {Models}},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	number = {4},
	journal = {Commun. ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/GWUDPWQT/Blei - 2012 - Probabilistic Topic Models.pdf:application/pdf}
}

@article{blei_latent_2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	journal = {Journal of machine Learning research},
	author = {Blei, David M. and Ng, Andrew Y and Jordan, Michael I},
	month = jan,
	year = {2003},
	pages = {993--1022}
}

@incollection{mcauliffe_supervised_2008,
	title = {Supervised {Topic} {Models}},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {121--128},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/AKKVXDQ9/Mcauliffe und Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/KQV8JHW8/3328-supervised-topic-models.html:text/html}
}

@article{grimmer_bayesian_2010,
	title = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}: {Measuring} {Expressed} {Agendas} in {Senate} {Press} {Releases}},
	volume = {18},
	shorttitle = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}},
	url = {https://papers.ssrn.com/abstract=1541022},
	abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
	number = {1},
	urldate = {2017-10-07},
	journal = {Political Analysis},
	author = {Grimmer, Justin},
	year = {2010},
	keywords = {SSRN, A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases, Justin  Grimmer},
	pages = {1--35},
	file = {Snapshot:/Users/Franzi/Zotero/storage/3QRF5PHE/papers.html:text/html}
}

@article{pritchard_inference_2000,
	title = {Inference of population structure using multilocus genotype data},
	volume = {155},
	issn = {0016-6731},
	abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
	language = {eng},
	number = {2},
	journal = {Genetics},
	author = {Pritchard, J. K. and Stephens, M. and Donnelly, P.},
	month = jun,
	year = {2000},
	pmid = {10835412},
	pmcid = {PMC1461096},
	keywords = {Algorithms, Cluster Analysis, Genetics, Population, Genotype, Humans, Models, Genetic},
	pages = {945--959}
}

@inproceedings{taddy_estimation_2012,
	title = {On estimation and selection for topic models},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Taddy, Matt},
	year = {2012}
}

@article{airoldi_reconceptualizing_2010,
	title = {Reconceptualizing the classification of {PNAS} articles},
	volume = {107},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/107/49/20899},
	doi = {10.1073/pnas.1013452107},
	abstract = {PNAS article classification is rooted in long-standing disciplinary divisions that do not necessarily reflect the structure of modern scientific research. We reevaluate that structure using latent pattern models from statistical machine learning, also known as mixed-membership models, that identify semantic structure in co-occurrence of words in the abstracts and references. Our findings suggest that the latent dimensionality of patterns underlying PNAS research articles in the Biological Sciences is only slightly larger than the number of categories currently in use, but it differs substantially in the content of the categories. Further, the number of articles that are listed under multiple categories is only a small fraction of what it should be. These findings together with the sensitivity analyses suggest ways to reconceptualize the organization of papers published in PNAS.},
	language = {en},
	number = {49},
	urldate = {2017-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Joutard, Cyrille and Love, Tanzy and Shringarpure, Suyash},
	month = jul,
	year = {2010},
	pmid = {21078953},
	keywords = {text analysis, hierarchical modeling, Monte Carlo Markov chain, variational inference, Dirichlet process},
	pages = {20899--20904},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/3HPSSBQ2/Airoldi et al. - 2010 - Reconceptualizing the classification of PNAS artic.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/3SR8QVPZ/20899.html:text/html}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	number = {476},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	month = dec,
	year = {2006},
	pages = {1566--1581},
	file = {Snapshot:/Users/Franzi/Zotero/storage/67GVBDZ8/016214506000000302.html:text/html}
}

@inproceedings{blei_dynamic_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Dynamic {Topic} {Models}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Blei, David M. and Lafferty, John D.},
	year = {2006},
	pages = {113--120}
}

@article{airoldi_improving_2016,
	title = {Improving and {Evaluating} {Topic} {Models} and {Other} {Models} of {Text}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2015.1051182},
	doi = {10.1080/01621459.2015.1051182},
	abstract = {An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parameterizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. Here, we show that words that are both frequent and exclusive to a theme are more effective at characterizing topical content, and we propose a regularization scheme that leads to better estimates of these quantities. We consider a supervised setting where professional editors have annotated documents to topic categories, organized into a tree, in which leaf-nodes correspond to more specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze these annotated documents. A parallelized Hamiltonian Monte Carlo sampler allows the inference to scale to millions of documents. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. In this supervised setting, we validate the efficacy of word frequency and exclusivity at characterizing topical content on two very large collections of documents, from Reuters and the New York Times. In an unsupervised setting, we then consider a simplified version of the model that shares the same regularization scheme with the previous model. We carry out a large randomized experiment on Amazon Mechanical Turk to demonstrate that topic summaries based on frequency and exclusivity, estimated using the proposed regularization scheme, are more interpretable than currently established frequency-based summaries, and that the proposed model produces more efficient estimates of exclusivity than the currently established models.},
	number = {516},
	journal = {Journal of the American Statistical Association},
	author = {Airoldi, Edoardo M. and Bischof, Jonathan M.},
	month = oct,
	year = {2016},
	keywords = {text analysis, Categorical data, Hamiltonian Monte Carlo, High-dimensional data, Parallel inference},
	pages = {1381--1403},
	file = {Snapshot:/Users/Franzi/Zotero/storage/HWNC8RRH/01621459.2015.html:text/html}
}

@article{grimmer_text_2013,
	title = {Text as {Data}: {The} {Promise} and {Pitfalls} of {Automatic} {Content} {Analysis} {Methods} for {Political} {Texts}},
	volume = {21},
	shorttitle = {Text as {Data}},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Stewart, Brandon},
	year = {2013},
	pages = {267--297},
	file = {Text as Data\: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts | Brandon Stewart:/Users/Franzi/Zotero/storage/FF9QRDJ4/text-data-promise-and-pitfalls-automatic-content-analysis-methods-political.html:text/html}
}

@inproceedings{roberts_structural_2013,
	title = {The {Structural} {Topic} {Model} and {Applied} {Social} {Science}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} {Workshop} on {Topic} {Models}: {Computation}, {Application}, and {Evaluation}.},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin and Airoldi, Edoardo},
	year = {2013},
	file = {The Structural Topic Model and Applied Social Science | Brandon Stewart:/Users/Franzi/Zotero/storage/69G2KU2F/structural-topic-model-and-applied-social-science.html:text/html}
}

@incollection{socher_bayesian_2009,
	title = {A {Bayesian} {Analysis} of {Dynamics} in {Free} {Recall}},
	url = {http://papers.nips.cc/paper/3720-a-bayesian-analysis-of-dynamics-in-free-recall.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Socher, Richard and Gershman, Samuel and Sederberg, Per and Norman, Kenneth and Perotte, Adler J. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1714--1722},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/JQWKTPGH/Socher et al. - 2009 - A Bayesian Analysis of Dynamics in Free Recall.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/ZFW3QA7C/3720-a-bayesian-analysis-of-dynamics-in-free-recall.html:text/html}
}

@article{quinn_how_2010,
	title = {How to {Analyze} {Political} {Attention} with {Minimal} {Assumptions} and {Costs}},
	volume = {54},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2009.00427.x/abstract},
	doi = {10.1111/j.1540-5907.2009.00427.x},
	abstract = {Previous methods of analyzing the substance of political attention have had to make several restrictive assumptions or been prohibitively costly when applied to large-scale political texts. Here, we describe a topic model for legislative speech, a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Our method estimates, rather than assumes, the substance of topics, the keywords that identify topics, and the hierarchical nesting of topics. We use the topic model to examine the agenda in the U.S. Senate from 1997 to 2004. Using a new database of over 118,000 speeches (70,000,000 words) from the Congressional Record, our model reveals speech topic categories that are both distinctive and meaningfully interrelated and a richer view of democratic agenda dynamics than had previously been possible.},
	language = {en},
	number = {1},
	journal = {American Journal of Political Science},
	author = {Quinn, Kevin M. and Monroe, Burt L. and Colaresi, Michael and Crespin, Michael H. and Radev, Dragomir R.},
	month = jan,
	year = {2010},
	pages = {209--228},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/59JK6JEX/Quinn et al. - 2010 - How to Analyze Political Attention with Minimal As.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/QSVKCVD5/abstract.html:text/html}
}

@article{roberts_model_2016,
	title = {A {Model} of {Text} for {Experimentation} in the {Social} {Sciences}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	number = {515},
	journal = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	month = jul,
	year = {2016},
	keywords = {text analysis, Causal inference, Experimentation, High-dimensional inference, Social sciences, Variational approximation},
	pages = {988--1003},
	file = {Snapshot:/Users/Franzi/Zotero/storage/ZMEDWA9E/01621459.2016.html:text/html}
}

@book{airoldi_handbook_2014,
	title = {Handbook of mixed membership models and their applications},
	isbn = {978-1-4665-0408-0},
	url = {http://cds.cern.ch/record/1974849},
	abstract = {In response to scientific needs for more diverse and structured explanations of statistical data, researchers have discovered how to model individual data points as belonging to multiple groups. Handbook of Mixed Membership Models and Their Applications shows you how to use these flexible modeling tools to uncover hidden patterns in modern high-dimensional multivariate data. It explores the use of the models in various application settings, including survey data, population genetics, text analysis, image processing and annotation, and molecular biology.Through examples using real data sets, yo},
	urldate = {2017-10-12},
	publisher = {Taylor and Francis},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Blei, David},
	year = {2014},
	file = {Snapshot:/Users/Franzi/Zotero/storage/P7XE3MF4/1974849.html:text/html}
}

@inproceedings{mishler_using_2015,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Using {Structural} {Topic} {Modeling} to {Detect} {Events} and {Cluster} {Twitter} {Users} in the {Ukrainian} {Crisis}},
	isbn = {978-3-319-21379-8 978-3-319-21380-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108},
	doi = {10.1007/978-3-319-21380-4_108},
	abstract = {Structural topic modeling (STM) is a recently introduced technique to model how the content of a collection of documents changes as a function of variables such as author identity or time of writing. We present two proof-of-concept applications of STM using Russian social media data. In our first study, we model how topics change over time, showing that STM can be used to detect significant events such as the downing of Malaysia Air Flight 17. In our second study, we model how topical content varies across a set of authors, showing that STM can be used to cluster Twitter users who are sympathetic to Ukraine versus Russia as well as to cluster accounts that are suspected to belong to the same individual (so-called “sockpuppets”). Structural topic modeling shows promise as a tool for analyzing social media data, a domain that has been largely ignored in the topic modeling literature.},
	language = {en},
	urldate = {2017-10-12},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer, Cham},
	author = {Mishler, Alan and Crabb, Erin Smith and Paletz, Susannah and Hefright, Brook and Golonka, Ewa},
	month = aug,
	year = {2015},
	pages = {639--644},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/ACUCPA8U/Mishler et al. - 2015 - Using Structural Topic Modeling to Detect Events a.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/NG9CQ4T9/978-3-319-21380-4_108.html:text/html}
}

@article{griffiths_finding_2004,
	title = {Finding scientific topics},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5228},
	doi = {10.1073/pnas.0307752101},
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	language = {en},
	number = {suppl 1},
	urldate = {2017-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	month = jun,
	year = {2004},
	pmid = {14872004},
	pages = {5228--5235},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/9P87NGIJ/Griffiths und Steyvers - 2004 - Finding scientific topics.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/TX4TP3NE/5228.html:text/html}
}

@article{erosheva_mixed-membership_2004,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	language = {en},
	number = {suppl 1},
	urldate = {2017-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	month = jun,
	year = {2004},
	pmid = {15020766},
	pages = {5220--5227},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/QX4H27E9/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/NVF8AVJ7/5220.html:text/html}
}

@article{genkin_large-scale_2007,
	title = {Large-{Scale} {Bayesian} {Logistic} {Regression} for {Text} {Categorization}},
	volume = {49},
	issn = {0040-1706},
	url = {http://dx.doi.org/10.1198/004017007000000245},
	doi = {10.1198/004017007000000245},
	abstract = {Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results.},
	number = {3},
	journal = {Technometrics},
	author = {Genkin, Alexander and Lewis, David D. and Madigan, David},
	month = aug,
	year = {2007},
	keywords = {Information retrieval, Lasso, Penalization, Ridge regression, Support vector classifier, Variable selection},
	pages = {291--304},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/9JI658VW/Genkin et al. - 2007 - Large-Scale Bayesian Logistic Regression for Text .pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/9XV4KG9V/004017007000000245.html:text/html}
}

@phdthesis{ponweiser_latent_2012,
	type = {Theses / {Institute} for {Statistics} and {Mathematics}},
	title = {Latent {Dirichlet} {Allocation} in {R}},
	url = {http://epub.wu.ac.at/3558/},
	abstract = {Topic models are a new research field within the computer sciences information retrieval and text mining. They are generative probabilistic models of text corpora inferred by machine learning and they can be used for retrieval and text mining tasks. The most prominent topic model is latent Dirichlet allocation (LDA), which was introduced in 2003 by Blei et al. and has since then sparked off the development of other topic models for domain-specific purposes.
This thesis focuses on LDA's practical application. Its main goal is the replication of the data analyses from the 2004 LDA paper ``Finding scientific topics'' by Thomas Griffiths and Mark Steyvers within the framework of the R statistical programming language and the R{\textasciitilde}package topicmodels by Bettina Grün and Kurt Hornik. The complete process, including extraction of a text corpus from the PNAS journal's website, data preprocessing, transformation into a document-term matrix, model selection, model estimation, as well as presentation of the results, is fully documented and commented. The outcome closely matches the analyses of the original paper, therefore the research by Griffiths/Steyvers can be reproduced. Furthermore, this thesis proves the suitability of the R environment for text mining with LDA. (author's abstract)},
	language = {en},
	urldate = {2017-10-13},
	school = {WU Vienna University of Economics and Business},
	author = {Ponweiser, Martin},
	month = may,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/AKASJPTP/Ponweiser - 2012 - Latent Dirichlet Allocation in R.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/E5J47GZP/3558.html:text/html}
}

@article{silge_tidytext:_2016,
	title = {tidytext: {Text} {Mining} and {Analysis} {Using} {Tidy} {Data} {Principles} in {R}},
	shorttitle = {tidytext},
	doi = {10.21105/joss.00037},
	journal = {The Journal of Open Source Software},
	author = {Silge, Julia and Robinson, David},
	month = jul,
	year = {2016},
	file = {Snapshot:/Users/Franzi/Zotero/storage/KBIXHDAH/305219559_tidytext_Text_Mining_and_Analysis_Using_Tidy_Data_Principles_in_R.pdf:application/pdf}
}

@inproceedings{phan_learning_2008,
	address = {Beijing, China},
	title = {Learning to {Classify} {Short} and {Sparse} {Text} \& {Web} with {Hidden} {Topics} from {Large}-{Scale} {Data} {Collections}},
	doi = {10.1145/1367497.1367510},
	abstract = {This paper presents a general framework for building classi- fiers that deal with short and sparse text \& Web segments by making the most of hidden topics discovered from large- scale data collections. The main motivation of this work is that many classification tasks working with short segments of text \& Web, such as search snippets, forum \& chat mes- sages, blog \& news feeds, product reviews, and book \& movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data bet- ter. The underlying idea of the framework is that for each classification task, we collect a large-scale external data col- lection called "universal dataset", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and MEDLINE (18M words) with two tasks: "Web search domain disambiguation" and "disease categorization for medical text", and achieved significant quality enhancement.},
	booktitle = {Proceedings of the 17th {International} {World} {Wide} {Web} {Conference} ({WWW} 2008)},
	author = {Phan, Xuan-Hieu and Nguyen, Le and Horiguchi, Susumu},
	month = jan,
	year = {2008},
	pages = {91--100},
	file = {Snapshot:/Users/Franzi/Zotero/storage/SP33XWPF/221023426_Learning_to_Classify_Short_and_Sparse_Text_Web_with_Hidden_Topics_from_Large-Scale_Dat.pdf:application/pdf}
}

@inproceedings{rabinovich_inverse_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {The {Inverse} {Regression} {Topic} {Model}},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044829},
	abstract = {Taddy (2013) proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Rabinovich, Maxim and Blei, David M.},
	year = {2014},
	pages = {I--199--I--207}
}

@article{schiller_development_2016,
	title = {Development of the {Social} {Network} {Usage} in {Germany} since 2012},
	journal = {Working Paper TU Darmstadt},
	author = {Schiller, Benjamin and Heimbach, Irina and Strufe, Thorsten and Hinz, Oliver},
	year = {2016}
}

@article{bholat_text_2015,
	title = {Text {Mining} for {Central} {Banks}},
	issn = {1556-5068},
	url = {http://www.academia.edu/13430482/Text_mining_for_central_banks},
	abstract = {Although often applied in other social sciences, text mining has been less frequently used in economics and in policy circles, particularly inside central banks. This Handbook is a brief introduction to the field, discussing how text mining is useful},
	urldate = {2017-11-06},
	journal = {SSRN Electronic Journal},
	author = {Bholat, David M. and Hansen, Stephen and Santos, Pedro M. and Schonhardt-Bailey, Cheryl},
	month = jun,
	year = {2015},
	file = {Snapshot:/Users/Franzi/Zotero/storage/J462FFQU/Text_mining_for_central_banks.html:text/html}
}

@inproceedings{minka_expectation-propagation_2002,
	address = {San Francisco, CA, USA},
	series = {{UAI}'02},
	title = {Expectation-propagation for the {Generative} {Aspect} {Model}},
	isbn = {978-1-55860-897-9},
	url = {http://dl.acm.org/citation.cfm?id=2073876.2073918},
	abstract = {The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets.},
	booktitle = {Proceedings of the {Eighteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Minka, Thomas and Lafferty, John},
	year = {2002},
	pages = {352--359}
}

@article{roberts_stm:_2016,
	title = {stm: {R} {Package} for {Structural} {Topic} {Models}},
	volume = {forthcoming},
	shorttitle = {stm},
	journal = {Journal of Statistical Software},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	month = jan,
	year = {2016},
	file = {stm\: R Package for Structural Topic Models | Dustin Tingley:/Users/Franzi/Zotero/storage/FAWQJE6X/stm-r-package-structural-topic-models.html:text/html}
}

@incollection{roberts_navigating_2016,
	address = {New York},
	title = {Navigating the {Local} {Modes} of {Big} {Data}: {The} {Case} of {Topic} {Models}.},
	booktitle = {Computational {Social} {Science}: {Discovery} and {Prediction}},
	publisher = {Cambridge University Press},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	year = {2016},
	file = {Navigating the Local Modes of Big Data\: The Case of Topic Models | Dustin Tingley:/Users/Franzi/Zotero/storage/TW4DVST8/navigating-local-modes-big-data-case-topic-models.html:text/html}
}

@techreport{grajzl_structural_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {Structural} {Topic} {Model} of the {Features} and the {Cultural} {Origins} of {Bacon}'s {Ideas}},
	url = {https://papers.ssrn.com/abstract=2944816},
	abstract = {We use machine-learning methods to study the features and origins of the thought of Francis Bacon, a key figure in the development of a cultural paradigm that provided intellectual roots for modern economic development. We estimate a structural topic model, a state-of-the-art methodology for analysis of text corpora. The estimates uncover sixteen topics prominent in Bacon's opus. Two are central in the ideas usually associated with Bacon: inductive epistemology and fact-seeking. While Bacon's epistemology is strongly connected with his jurisprudence, fact-seeking is more isolated from Bacon's other intellectual pursuits. The utilitarian promise of science and the central organization of the scientific quest, embraced by Bacon's followers, were not emphasized by him, a finding suggesting that these aspects of the 'Baconian' culture owed little to Bacon's own contributions. Bacon's use of different topics varies notably with intended audience and chosen medium.},
	number = {ID 2944816},
	urldate = {2017-11-07},
	institution = {Social Science Research Network},
	author = {Grajzl, Peter and Murrell, Peter},
	month = oct,
	year = {2017},
	keywords = {Francis Bacon, culture, law, knowledge, natural philosophy, politics, religion},
	file = {Snapshot:/Users/Franzi/Zotero/storage/B862TXNA/papers.html:text/html}
}

@article{baturo_what_2017,
	title = {What {Drives} the {International} {Development} {Agenda}? {An} {NLP} {Analysis} of the {United} {Nations} {General} {Debate} 1970-2016},
	shorttitle = {What {Drives} the {International} {Development} {Agenda}?},
	url = {http://arxiv.org/abs/1708.05873},
	abstract = {There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.},
	journal = {arXiv:1708.05873 [cs]},
	author = {Baturo, Alexander and Dasandi, Niheer and Mikhaylov, Slava J.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05873},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv\:1708.05873 PDF:/Users/Franzi/Zotero/storage/FSRGQVHA/Baturo et al. - 2017 - What Drives the International Development Agenda .pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/H2JRUWQF/1708.html:text/html}
}

@incollection{hagen_data_2018,
	series = {Public {Administration} and {Information} {Technology}},
	title = {Data {Analytics} for {Policy} {Informatics}: {The} {Case} of {E}-{Petitioning}},
	isbn = {978-3-319-61761-9 978-3-319-61762-6},
	shorttitle = {Data {Analytics} for {Policy} {Informatics}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-61762-6_9},
	abstract = {To contribute to the development of policy informatics, we discuss the benefits of analyzing electronic petitions (e-petitions), a form of citizen-government discourse with deep historic roots that has recently transitioned into a technologically-enabled and novel form of political communication. We begin by presenting a rationale for the analysis of e-petitions as a type of e-participation that can contribute to the development of public policy, provided that it is possible to analyze the large volumes of data produced in petitioning processes. From there we consider two data analytic strategies that offer promising approaches to the analysis of e-petitions and that lend themselves to the future creation of policy informatics tools. We discuss the application of topic modeling to the analysis of e-petition textual data to identify emergent topics of substantial concern to the public. We further propose the application of social network analysis to data related to the dynamics of petitioning processes, such as the social connections between petition initiators and signers, and tweets that solicit petition signatures in petitioning campaigns; both may be useful in revealing patterns of collective action. The paper concludes by reflecting on issues that should be brought to bear on the construction of policy informatics tools that make use of e-petitioning data.},
	language = {en},
	urldate = {2017-11-09},
	booktitle = {Policy {Analytics}, {Modelling}, and {Informatics}},
	publisher = {Springer, Cham},
	author = {Hagen, Loni and Harrison, Teresa M. and Dumas, Catherine L.},
	year = {2018},
	note = {DOI: 10.1007/978-3-319-61762-6\_9},
	pages = {205--224},
	file = {Snapshot:/Users/Franzi/Zotero/storage/QBNNBEIJ/978-3-319-61762-6_9.html:text/html}
}

@article{haixia_extracting_2016,
	title = {Extracting {Topics} of {Computer} {Science} {Literature} with {LDA} {Model}, {Extracting} {Topics} of {Computer} {Science} {Literature} with {LDA} {Model}},
	volume = {32},
	issn = {2096-3467},
	url = {http://manu44.magtech.com.cn/Jwk_infotech_wk3/EN/abstract/abstract4288.shtml},
	doi = {10.11925/infotech.1003-3513.2016.11.03},
	language = {cn},
	number = {11},
	urldate = {2017-11-09},
	journal = {Data Analysis and Knowledge Discovery},
	author = {Haixia, Yang and Baojun, Gao and Hanlin, Sun and Haixia, Yang and Baojun, Gao and Hanlin, Sun},
	month = dec,
	year = {2016},
	pages = {20--26},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/PR58B4CK/Haixia et al. - 2016 - Extracting Topics of Computer Science Literature w.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/DDFB9U5P/abstract4288.html:text/html}
}

@article{das_trends_2017,
	title = {Trends in {Transportation} {Research}},
	volume = {2614},
	issn = {0361-1981},
	url = {http://trrjournalonline.trb.org/doi/abs/10.3141/2614-04},
	doi = {10.3141/2614-04},
	abstract = {Proceedings of journal and conference papers are good sources of big textual data to examine research trends in various branches of science. The contents, usually unstructured in nature, require fast machine-learning algorithms to be deciphered. Exploratory analysis through text mining usually provides the descriptive nature of the contents but lacks quantification of the topics and their correlations. Topic models are algorithms designed to discover the main theme or trend in massive collections of unstructured documents. Through the use of a structural topic model, an extension of latent Dirichlet allocation, this study introduced distinct topic models on the basis of the relative frequencies of the words used in the abstracts of 15,357 TRB compendium papers. With data from 7 years (2008 through 2014) of TRB annual meeting compendium papers, the 20 most dominant topics emerged from a bag of 4 million words. The findings of this study contributed to the understanding of topical trends in the complex and evolving field of transportation engineering research.},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Das, Subasish and Dixon, Karen and Sun, Xiaoduan and Dutta, Anandi and Zupancich, Michelle},
	month = jan,
	year = {2017},
	pages = {27--38},
	file = {Snapshot:/Users/Franzi/Zotero/storage/FTII7NXR/2614-04.html:text/html}
}

@article{roberts_structural_2014,
	title = {Structural {Topic} {Models} for {Open}-{Ended} {Survey} {Responses}},
	volume = {58},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12103/abstract},
	doi = {10.1111/ajps.12103},
	abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
	language = {en},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	month = oct,
	year = {2014},
	pages = {1064--1082},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/FD9RK878/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Resp.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/6GGMT3JM/abstract.html:text/html}
}

@article{lucas_computer_2015,
	title = {Computer assisted text analysis for comparative politics},
	volume = {23},
	number = {2},
	journal = {Political Analysis},
	author = {Lucas, Christopher and Nielsen, Richard and Roberts, Margaret and Stewart, Brandon and Storer, Alex and Tingley, Dustin},
	year = {2015},
	pages = {254--277},
	file = {Computer assisted text analysis for comparative politics | Dustin Tingley:/Users/Franzi/Zotero/storage/N5EV4K52/computer-assisted-text-analysis-comparative-politics.html:text/html}
}

@techreport{mueller_reading_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Reading between the {Lines}: {Prediction} of {Political} {Violence} {Using} {Newspaper} {Text}},
	shorttitle = {Reading between the {Lines}},
	url = {https://papers.ssrn.com/abstract=2843535},
	abstract = {This article provides a new methodology to predict armed conflict by using newspaper text. Through machine learning, vast quantities of newspaper text are reduced to interpretable topics. We propose the use of the within-country variation of these topics to predict the timing of conflict. This allows us to avoid the tendency of predicting conflict only in countries where it occurred before. We show that the within-country variation of topics is an extremely robust predictor of conflict and becomes particularly useful when new conflict risks arise. Two aspects seem to be responsible for these features. Topics provide depth because they consist of changing, long lists of terms which makes them able to capture the changing context of conflict. At the same time topics provide width because they summarize all text, including coverage of stabilizing factors.},
	number = {ID 2843535},
	urldate = {2017-11-09},
	institution = {Social Science Research Network},
	author = {Mueller, Hannes Felix and Rauh, Christopher},
	month = sep,
	year = {2016},
	keywords = {Civil War, conflict, Forecasting, Latent Dirichlet Allocation., Machine Learning, panel data, Topic Models},
	file = {Snapshot:/Users/Franzi/Zotero/storage/R7FGX7V9/papers.html:text/html}
}

@techreport{law_constitutional_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Constitutional {Archetypes}},
	url = {https://papers.ssrn.com/abstract=2732519},
	abstract = {It is a core function of constitutions to justify the existence and organization of the state. The ideological narratives embedded in constitutions are not fundamentally unique, however, but instead derive from a limited number of competing models. Each model is defined by a particular type of justification for the existence and organization of the state, and by a symbiotic relationship with a particular legal tradition. These models are so ubiquitous and elemental that they amount to constitutional archetypes.This Article contends as an empirical matter that constitutional narratives of the state boil down to a combination of three basic archetypes–namely, a liberal archetype, a statist archetype, and a universalist archetype. The liberal archetype is closely identified with the common law tradition and views the state as a potentially oppressive concentration of authority in need of regulation and restraint. In keeping with this conception of the state, liberal constitutions emphasize the imposition of limits upon government in the form of negative and procedural rights, as well as a strong and independent judiciary to make these limits effective. The legitimacy of the state is contingent upon adherence to constitutional limits. Constitutions in this vein are largely agnostic as to what goals, if any, society as a whole should pursue through the mechanism of the state.The statist archetype, in contrast, is associated with the civil law tradition and hails the state as the embodiment of a distinctive community and the vehicle for the achievement of the community’s goals. The legitimacy of the state rests upon the strength of the state’s claim to represent the will of a community. Consequently, constitutions in this vein are attentive to the identity, membership, and symbols of the state. Other characteristics of a statist constitution include an emphasis on the articulation of collective goals and positive rights that contemplate an active role for the state, and an obligation on the part of citizens to cooperate with the state in the pursuit of shared goals.The universalist archetype, the newest and most prevalent of the three, is symbiotically intertwined with a post-World War II, post-Westphalian paradigm of international law that rests the legitimacy of the state upon the normative force of a global legal order that encompasses both constitutional law and international law. Characteristics of this archetype include explicit commitment to supranational institutions and supranational law and reliance on generic terms and concepts that can be found not only in a variety of national constitutions, but also in international legal instruments.Empirical evidence of the prevalence and content of these three basic archetypes can be found in the unlikeliest of places – namely, constitutional preambles. Preambles enjoy a reputation for expressing uniquely national values, identities, and narratives. If there is any part of a constitution that ought not to be reducible to a handful of recurring patterns, it is surely the preamble. Yet analysis of the world’s constitutional preambles using methods from computational linguistics suggests that they consist of a combination of the three archetypes. Estimation of a structural topic model yields a quantitative measure of the extent to which each preamble draws upon each archetype.The empirical analysis also highlights the growing commingling and interdependence of constitutional law and international law. The semantic patterns that characterize universalist preambles mirrors those found in leading international human rights instruments. The adoption of the same conceptual and normative vocabulary by both universalist constitutions and key international legal instruments signals the emergence of a globalized ideological dialect common to both domestic constitutional law and public international law. The rising use of this common language by constitutional drafters since World War II is a quantitative indicator of the growing extent to which constitutional law and public international law influence each other.},
	number = {ID 2732519},
	urldate = {2017-11-09},
	institution = {Social Science Research Network},
	author = {Law, David S.},
	month = dec,
	year = {2016},
	keywords = {text analysis, constitution, archetype, ideology, international law, constitutional law, preamble, statism, liberalism, universalism, automated, content analysis, topic model, legal traditions, civil law, common law, empirical, constitutional drafting, constitution-making, constitution-writing},
	file = {Snapshot:/Users/Franzi/Zotero/storage/IMHWX425/papers.html:text/html}
}

@article{farrell_corporate_2016,
	title = {Corporate funding and ideological polarization about climate change},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/1/92},
	doi = {10.1073/pnas.1509433112},
	abstract = {Drawing on large-scale computational data and methods, this research demonstrates how polarization efforts are influenced by a patterned network of political and financial actors. These dynamics, which have been notoriously difficult to quantify, are illustrated here with a computational analysis of climate change politics in the United States. The comprehensive data include all individual and organizational actors in the climate change countermovement (164 organizations), as well as all written and verbal texts produced by this network between 1993–2013 (40,785 texts, more than 39 million words). Two main findings emerge. First, that organizations with corporate funding were more likely to have written and disseminated texts meant to polarize the climate change issue. Second, and more importantly, that corporate funding influences the actual thematic content of these polarization efforts, and the discursive prevalence of that thematic content over time. These findings provide new, and comprehensive, confirmation of dynamics long thought to be at the root of climate change politics and discourse. Beyond the specifics of climate change, this paper has important implications for understanding ideological polarization more generally, and the increasing role of private funding in determining why certain polarizing themes are created and amplified. Lastly, the paper suggests that future studies build on the novel approach taken here that integrates large-scale textual analysis with social networks.},
	language = {en},
	number = {1},
	urldate = {2017-11-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Farrell, Justin},
	month = may,
	year = {2016},
	pmid = {26598653},
	keywords = {politics, funding, polarization, computational social science, climate change},
	pages = {92--97},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/8M3IS9E4/Farrell - 2016 - Corporate funding and ideological polarization abo.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/9R5TQ8PC/92.html:text/html}
}

@article{reich_computer-assisted_2014,
	title = {Computer-{Assisted} {Reading} and {Discovery} for {Student} {Generated} {Text} in {Massive} {Open} {Online} {Courses}},
	volume = {2},
	copyright = {Copyright (c)},
	issn = {1929-7750},
	url = {http://www.learning-analytics.info/journals/index.php/JLA/article/view/4138},
	doi = {10.18608/jla.2015.21.8},
	abstract = {Dealing with the vast quantities of text that students generate in Massive Open Online Courses (MOOCs) and other large-scale online learning environments is a daunting challenge. Computational tools are needed to help instructional teams uncover themes and patterns as students write in forums, assignments, and surveys. This paper introduces to the learning analytics community the Structural Topic Model, an approach to language processing that can 1) ﬁnd syntactic patterns with semantic meaning in unstructured text, 2) identify variation in those patterns across covariates, and 3) uncover archetypal texts that exemplify the documents within a topical pattern. We show examples of computationally aided discovery and reading in three MOOC settings: mapping students’ self-reported motivations, identifying themes in discussion forums, and uncovering patterns of feedback in course evaluations.},
	language = {en},
	number = {1},
	urldate = {2017-11-09},
	journal = {Journal of Learning Analytics},
	author = {Reich, Justin and Tingley, Dustin and Leder-Luis, Jetson and Roberts, Margaret E. and Stewart, Brandon},
	month = nov,
	year = {2014},
	keywords = {text analysis, Massive Open Online Courses, topic modelling, computer‐assisted reading},
	pages = {156--184},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/BPWP7FIR/Reich et al. - 2014 - Computer-Assisted Reading and Discovery for Studen.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/52JEF8DU/4138.html:text/html}
}

@incollection{gabszewicz_media_2015,
	series = {Handbook on the economics of the media. - {Cheltenham}, {UK} : {Edward} {Elgar} {Publishing}, {ISBN} 978-0-85793-888-6. - 2015, p. 3-35},
	title = {Media as multi-sided platforms},
	language = {eng},
	booktitle = {Handbook on the economics of the media},
	author = {Gabszewicz, Jean Jaskold and Resende, Joana and Sonnac, Nathalie},
	year = {2015},
	file = {Media as multi-sided platforms - EconBiz:/Users/Franzi/Zotero/storage/MS8D937F/10011339834.html:text/html}
}

@inproceedings{mimno_optimizing_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Optimizing {Semantic} {Coherence} in {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	year = {2011},
	pages = {262--272},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/758M9D8V/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{wei_lda-based_2006,
	address = {New York, NY, USA},
	series = {{SIGIR} '06},
	title = {{LDA}-based {Document} {Models} for {Ad}-hoc {Retrieval}},
	isbn = {978-1-59593-369-0},
	url = {http://doi.acm.org/10.1145/1148170.1148204},
	doi = {10.1145/1148170.1148204},
	abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
	booktitle = {Proceedings of the 29th {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wei, Xing and Croft, W. Bruce},
	year = {2006},
	keywords = {Information retrieval, topic model, document model, language model, latent dirichlet allocation (LDA)},
	pages = {178--185}
}

@inproceedings{wallach_evaluation_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Evaluation {Methods} for {Topic} {Models}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553515},
	doi = {10.1145/1553374.1553515},
	abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
	year = {2009},
	pages = {1105--1112}
}

@incollection{chang_reading_2009,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	shorttitle = {Reading {Tea} {Leaves}},
	url = {http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan L. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {288--296},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/UUPBMREK/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/68XT476P/3700-reading-tea-leaves-how-humans-interpret-topic-models.html:text/html}
}

@inproceedings{blei_latent_2001,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'01},
	title = {Latent {Dirichlet} {Allocation}},
	url = {http://dl.acm.org/citation.cfm?id=2980539.2980618},
	abstract = {We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hof-mann's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Neural} {Information} {Processing} {Systems}: {Natural} and {Synthetic}},
	publisher = {MIT Press},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2001},
	pages = {601--608}
}

@incollection{griffiths_hierarchical_2004,
	title = {Hierarchical {Topic} {Models} and the {Nested} {Chinese} {Restaurant} {Process}},
	url = {http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Griffiths, Thomas L. and Jordan, Michael I. and Tenenbaum, Joshua B. and Blei, David M.},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
	year = {2004},
	pages = {17--24},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/RC2JTMS3/Griffiths et al. - 2004 - Hierarchical Topic Models and the Nested Chinese R.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/H6Z7JS5C/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.html:text/html}
}

@article{blei_nested_2007,
	title = {The nested {Chinese} restaurant process and {Bayesian} nonparametric inference of topic hierarchies},
	url = {http://arxiv.org/abs/0710.0845},
	abstract = {We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
	journal = {arXiv:0710.0845 [stat]},
	author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
	month = oct,
	year = {2007},
	note = {arXiv: 0710.0845},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:0710.0845 PDF:/Users/Franzi/Zotero/storage/QF33HDD7/Blei et al. - 2007 - The nested Chinese restaurant process and Bayesian.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/GC56ZTK4/0710.html:text/html}
}

@article{griffiths_probabilistic_2002,
	title = {A probabilistic approach to semantic representation},
	volume = {24},
	url = {https://escholarship.org/uc/item/44x9v7m7},
	number = {24},
	urldate = {2017-11-16},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	month = jan,
	year = {2002},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/6EQ8VAJR/Griffiths und Steyvers - 2002 - A probabilistic approach to semantic representatio.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/76HFDXBQ/44x9v7m7.pdf:application/pdf}
}

@techreport{heinrich_parameter_2004,
	title = {Parameter estimation for text analysis},
	abstract = {Abstract. Presents parameter estimation methods common with discrete probability distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation (LDA) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, including a discussion of Dirichlet hyperparameter estimation. Finally, analysis methods of LDA models are discussed.},
	author = {Heinrich, Gregor},
	year = {2004},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/USMHCPT6/Heinrich - 2004 - Parameter estimation for text analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/W4STBVS3/summary.html:text/html}
}

@techreport{heinrich_parameter_2004-1,
	title = {Parameter estimation for text analysis},
	abstract = {Abstract. Presents parameter estimation methods common with discrete probability distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation (LDA) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, including a discussion of Dirichlet hyperparameter estimation. Finally, analysis methods of LDA models are discussed.},
	author = {Heinrich, Gregor},
	year = {2004},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/PTHWFSPM/Heinrich - 2004 - Parameter estimation for text analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/JZDEF9EX/summary.html:text/html}
}

@incollection{steyvers_probabilistic_2006,
	title = {Probabilistic {Topic} {Models}},
	booktitle = {Latent {Semantic} {Analysis}: {A} {Road} to {Meaning}.},
	publisher = {Laurence Erlbaum},
	author = {Steyvers, Mark and Griffiths, Thomas L.},
	editor = {Landauer, L. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
	year = {2006}
}

@incollection{hoffman_online_2010,
	title = {Online {Learning} for {Latent} {Dirichlet} {Allocation}},
	url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {856--864},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/E3N5REJ7/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/NE5ZUD83/3902-online-learning-for-latent-dirichlet-allocation.html:text/html}
}

@inproceedings{buntine_estimating_2009,
	address = {Berlin, Heidelberg},
	series = {{ACML} '09},
	title = {Estimating {Likelihoods} for {Topic} {Models}},
	isbn = {978-3-642-05223-1},
	url = {http://dx.doi.org/10.1007/978-3-642-05224-8_6},
	doi = {10.1007/978-3-642-05224-8_6},
	abstract = {Topic models are a discrete analogue to principle component analysis and independent component analysis that model {\textless}em{\textgreater}topic{\textless}/em{\textgreater} at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.},
	booktitle = {Proceedings of the 1st {Asian} {Conference} on {Machine} {Learning}: {Advances} in {Machine} {Learning}},
	publisher = {Springer-Verlag},
	author = {Buntine, Wray},
	year = {2009},
	pages = {51--64}
}

@inproceedings{alsumait_topic_2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Topic {Significance} {Ranking} of {LDA} {Generative} {Models}},
	isbn = {978-3-642-04179-2 978-3-642-04180-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-04180-8_22},
	doi = {10.1007/978-3-642-04180-8_22},
	abstract = {Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distribution” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.},
	language = {en},
	urldate = {2017-11-16},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {AlSumait, Loulwah and Barbará, Daniel and Gentle, James and Domeniconi, Carlotta},
	month = sep,
	year = {2009},
	pages = {67--82},
	file = {Snapshot:/Users/Franzi/Zotero/storage/WCPH8XR4/10.html:text/html}
}

@inproceedings{mimno_optimizing_2011-1,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Optimizing {Semantic} {Coherence} in {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	year = {2011},
	pages = {262--272},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/8ATB9444/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{mimno_bayesian_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Bayesian {Checking} for {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
	abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Blei, David},
	year = {2011},
	pages = {227--237},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/MFUGSEJM/Mimno und Blei - 2011 - Bayesian Checking for Topic Models.pdf:application/pdf}
}

@inproceedings{newman_automatic_2010,
	address = {Stroudsburg, PA, USA},
	series = {{HLT} '10},
	title = {Automatic {Evaluation} of {Topic} {Coherence}},
	isbn = {978-1-932432-65-7},
	url = {http://dl.acm.org/citation.cfm?id=1857999.1858011},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
	booktitle = {Human {Language} {Technologies}: {The} 2010 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
	year = {2010},
	pages = {100--108},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/QJIP655P/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf:application/pdf}
}

@incollection{wallach_rethinking_2009,
	title = {Rethinking {LDA}: {Why} {Priors} {Matter}},
	shorttitle = {Rethinking {LDA}},
	url = {http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Wallach, Hanna M. and Mimno, David M. and McCallum, Andrew},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1973--1981},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/TPHKVI25/Wallach et al. - 2009 - Rethinking LDA Why Priors Matter.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/6FVP9RXD/3854-rethinking-lda-why-priors-matter.html:text/html}
}

@article{asuncion_smoothing_2012,
	title = {On {Smoothing} and {Inference} for {Topic} {Models}},
	url = {http://arxiv.org/abs/1205.2662},
	abstract = {Latent Dirichlet analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. Various learning algorithms have been developed in recent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.},
	journal = {arXiv:1205.2662 [cs, stat]},
	author = {Asuncion, Arthur and Welling, Max and Smyth, Padhraic and Teh, Yee Whye},
	month = may,
	year = {2012},
	note = {arXiv: 1205.2662},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1205.2662 PDF:/Users/Franzi/Zotero/storage/MRRAS29S/Asuncion et al. - 2012 - On Smoothing and Inference for Topic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/UHH4ICFA/1205.html:text/html}
}

@inproceedings{hofmann_probabilistic_1999,
	address = {New York, NY, USA},
	series = {{SIGIR} '99},
	title = {Probabilistic {Latent} {Semantic} {Indexing}},
	isbn = {978-1-58113-096-6},
	url = {http://doi.acm.org/10.1145/312624.312649},
	doi = {10.1145/312624.312649},
	booktitle = {Proceedings of the 22Nd {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Hofmann, Thomas},
	year = {1999},
	pages = {50--57}
}

@article{deerwester_indexing_1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.},
	number = {6},
	journal = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	year = {1990},
	pages = {391--407},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/EVGTHKF3/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/5KT8CZXC/summary.html:text/html}
}

@article{corden_maximisation_1952,
	title = {The {Maximisation} of {Profit} by a {Newspaper}},
	volume = {20},
	issn = {0034-6527},
	url = {http://www.jstor.org/stable/2295888},
	doi = {10.2307/2295888},
	number = {3},
	journal = {The Review of Economic Studies},
	author = {Corden, W. M.},
	year = {1952},
	pages = {181--190}
}

@article{gustafsson_circulation_1978,
	title = {The circulation spiral and the principle of household coverage},
	volume = {26},
	issn = {0358-5522},
	url = {https://doi.org/10.1080/03585522.1978.10407893},
	doi = {10.1080/03585522.1978.10407893},
	abstract = {The growth of oligopoly within the newspaper industry is a widespread phenomenon which has been examined by both researchers into the mass media and public enquiries into the press in a number of countries. Politicians recognise the development and want to modify the process of concentration, prevent newspaper closures, and even promote new ventures. Many western countries have taken measures to try to control forces bearing toward concentration in the newspaper industry. Such efforts, however, require a thorough knowledge of the market and its mechanism.},
	number = {1},
	journal = {Scandinavian Economic History Review},
	author = {Gustafsson, Karl Erik},
	month = jan,
	year = {1978},
	pages = {1--14},
	file = {Snapshot:/Users/Franzi/Zotero/storage/D8TJWJWK/03585522.1978.html:text/html}
}

@article{blair_pricing_1993,
	title = {Pricing {Decisions} of the {Newspaper} {Monopolist}},
	volume = {59},
	issn = {0038-4038},
	url = {http://www.jstor.org/stable/1059734},
	doi = {10.2307/1059734},
	number = {4},
	journal = {Southern Economic Journal},
	author = {Blair, Roger D. and Romano, Richard E.},
	year = {1993},
	pages = {721--732}
}

@article{evans_empirical_2003,
	title = {Some {Empirical} {Aspects} of {Multi}-sided {Platform} {Industries}},
	volume = {2},
	issn = {1446-9022},
	url = {https://www.degruyter.com/view/j/rne.2003.2.issue-3/rne.2003.2.3.1026/rne.2003.2.3.1026.xml},
	doi = {10.2202/1446-9022.1026},
	abstract = {Multi-sided platform markets have two or more different groups of customers that businesses have to get and keep on board to succeed. These industries range from dating clubs (men and women), to video game consoles (game developers and users), to payment cards (cardholders and merchants), to operating system software (application developers and users). They include some of the most important industries in the economy. A survey of businesses in these industries shows that multi-sided platform businesses devise entry strategies to get multiple sides of the market on board and devise pricing, product, and other competitive strategies to keep multiple customer groups on a common platform that internalizes externalities across members of these groups.},
	number = {3},
	journal = {Review of Network Economics},
	author = {Evans, David S.},
	year = {2003},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/JXJD8ZCR/Evans - 2003 - Some Empirical Aspects of Multi-sided Platform Ind.pdf:application/pdf}
}

@article{ellman_what_2009,
	title = {What do the {Papers} {Sell}? {A} {Model} of {Advertising} and {Media} {Bias}*},
	volume = {119},
	issn = {1468-0297},
	shorttitle = {What do the {Papers} {Sell}?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02218.x/abstract},
	doi = {10.1111/j.1468-0297.2009.02218.x},
	abstract = {We model the market for news as a two-sided market where newspapers sell news to readers who value accuracy and sell space to advertisers who value advert-receptive readers. In this setting, monopolistic newspapers under-report or bias news that sufficiently reduces advertiser profits. Paradoxically, increasing the size of advertising eventually leads competing newspapers to reduce advertiser bias. Nonetheless, advertisers can counter this effect if able to commit to news-sensitive cut-off strategies, potentially inducing as much bias as in the monopoly case. We use these results to explain contrasting historical and recent evidence on commercial bias and influence in the media.},
	language = {en},
	number = {537},
	journal = {The Economic Journal},
	author = {Ellman, Matthew and Germano, Fabrizio},
	month = apr,
	year = {2009},
	pages = {680--704},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/ESUGMQCA/Ellman und Germano - 2009 - What do the Papers Sell A Model of Advertising an.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/E87BHVVH/abstract.html:text/html}
}

@book{wiedmann_text_2016,
	address = {Wiesbaden},
	edition = {1},
	title = {Text {Mining} for {Qualitative} {Data} {Analysis} in the {Social} {Sciences}},
	url = {//www.springer.com/de/book/9783658153083},
	urldate = {2017-11-26},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Wiedmann, Gregor},
	year = {2016},
	file = {Snapshot:/Users/Franzi/Zotero/storage/7P4K4CSX/9783658153083.html:text/html}
}

@incollection{boogaart_linear_2013,
	series = {Use {R}!},
	title = {Linear {Models} for {Compositions}},
	isbn = {978-3-642-36808-0 978-3-642-36809-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-36809-7_5},
	abstract = {Compositions can play the role of dependent and independent variables in linear models. In both cases, the parameters of the linear models are again compositions of the same simplex as the data. Most methods for classical linear models have a close analog in these compositional linear models. This chapter addresses several questions on this subject. What are compositional linear models? How to visualize the dependence of compositions, already multivariable, with further external covariables? How to model and check such dependence with compositional linear models? What are the underlying assumptions? How can we check these assumptions? What is the compositional interpretation of the results? How to use linear models to provide statistical evidence with tests, confidence intervals, and predictive regions? How to visualize model results and model parameters? How to compare compositional linear models and how to find the most appropriate one?},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {Analyzing {Compositional} {Data} with {R}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Boogaart, K. Gerald van den and Tolosana-Delgado, Raimon},
	year = {2013},
	note = {DOI: 10.1007/978-3-642-36809-7\_5},
	pages = {95--175},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/XXRU4BZK/Boogaart und Tolosana-Delgado - 2013 - Linear Models for Compositions.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/66BBBZV2/978-3-642-36809-7_5.html:text/html}
}

@techreport{evans_industrial_2005-1,
	type = {Working {Paper}},
	title = {The {Industrial} {Organization} of {Markets} with {Two}-{Sided} {Platforms}},
	url = {http://www.nber.org/papers/w11603},
	abstract = {Two-sided platforms (2SPs) cater to two or more distinct groups of customers, facilitating value-creating interactions between them. The village market and the village matchmaker were 2SPs; eBay and Match.com are more recent examples. Other examples include payment card systems, magazines, shopping malls, and personal computer operating systems. Building on the seminal work of Rochet and Tirole (2003), a rapidly growing literature has illuminated the economic principles that apply to 2SPs generally. One key result is that 2SPs may find it profit-maximizing to charge prices for one customer group that are below marginal cost or even negative, and such skewed pricing pattern is prevalent, although not universal, in industries that appear to be based on 2SPs. Over the years, courts have also recognized that certain industries, notably payment card systems and newspapers, now understood to be based on 2SPs, are governed by unusual economic relationships. This chapter provides an introduction to the economics of 2SPs and its application to several competition policy issues.},
	number = {11603},
	institution = {National Bureau of Economic Research},
	author = {Evans, David S. and Schmalensee, Richard},
	month = sep,
	year = {2005},
	note = {DOI: 10.3386/w11603},
	file = {NBER Full Text PDF:/Users/Franzi/Zotero/storage/EWJINGI8/Evans und Schmalensee - 2005 - The Industrial Organization of Markets with Two-Si.pdf:application/pdf}
}

@article{arora_practical_2012,
	title = {A {Practical} {Algorithm} for {Topic} {Modeling} with {Provable} {Guarantees}},
	url = {https://arxiv.org/abs/1212.4777},
	urldate = {2017-12-07},
	author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
	month = dec,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/KR8KRPIX/Arora et al. - 2012 - A Practical Algorithm for Topic Modeling with Prov.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/P63RKV2V/1212.html:text/html}
}

@book{roberts_computational_2016,
	address = {New York},
	title = {Computational {Social} {Science}: {Discovery} and {Prediction}},
	shorttitle = {Computational {Social} {Science}},
	publisher = {Cambridge University Press},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	year = {2016},
	file = {Navigating the Local Modes of Big Data\: The Case of Topic Models | Dustin Tingley:/Users/Franzi/Zotero/storage/G5DFC62T/navigating-local-modes-big-data-case-topic-models.html:text/html}
}

@article{steiner_program_1952,
	title = {Program {Patterns} and {Preferences}, and the {Workability} of {Competition} in {Radio} {Broadcasting}},
	volume = {66},
	issn = {0033-5533},
	url = {http://www.jstor.org/stable/1882942},
	doi = {10.2307/1882942},
	abstract = {I. Criteria for the appraisal of workability, 195.--II. The one period model, 197.--III. The model over time, 207.--IV. Relevance of the model to the market structure of the industry, 217.--V. Some suggestions for further analysis, 222.},
	number = {2},
	journal = {The Quarterly Journal of Economics},
	author = {Steiner, Peter O.},
	year = {1952},
	pages = {194--223}
}

@article{gabszewicz_press_2001,
	series = {15th {Annual} {Congress} of the {European} {Economic} {Association}},
	title = {Press advertising and the ascent of the ‘{Pensée} {Unique}’},
	volume = {45},
	issn = {0014-2921},
	url = {http://www.sciencedirect.com/science/article/pii/S0014292101001398},
	doi = {10.1016/S0014-2921(01)00139-8},
	abstract = {The press industry depends in a crucial way on the possibility of financing an important fraction of its activities by advertising receipts. We show that this induces the editors of newspapers to moderate, in several cases, the political message they display to their readers, compared with the political opinions they would have expressed otherwise. To this end, we consider a three-stage game in which editors select sequentially their political image, the price of their newspaper and the advertising tariff they oppose to the advertisers. The intuition of the result lies in the fact that editors have to sell tasteless political messages to their readers in order to sell a larger audience to the advertisers.},
	number = {4},
	journal = {European Economic Review},
	author = {Gabszewicz, Jean J. and Laussel, Dider and Sonnac, Nathalie},
	month = may,
	year = {2001},
	keywords = {advertising, TV-broadcasting, Channels-program diversity},
	pages = {641--651},
	file = {ScienceDirect Full Text PDF:/Users/Franzi/Zotero/storage/8RHUZD5J/Gabszewicz et al. - 2001 - Press advertising and the ascent of the ‘Pensée Un.pdf:application/pdf;ScienceDirect Snapshot:/Users/Franzi/Zotero/storage/JH6EJC28/S0014292101001398.html:text/html}
}

@article{samuelson_aspects_1958,
	title = {Aspects of {Public} {Expenditure} {Theories}},
	volume = {40},
	issn = {0034-6535},
	url = {http://www.jstor.org/stable/1926336},
	doi = {10.2307/1926336},
	number = {4},
	journal = {The Review of Economics and Statistics},
	author = {Samuelson, Paul A.},
	year = {1958},
	pages = {332--338}
}

@article{loughran_when_2011,
	title = {When {Is} a {Liability} {Not} a {Liability}? {Textual} {Analysis}, {Dictionaries}, and 10-{Ks}},
	volume = {66},
	issn = {1540-6261},
	shorttitle = {When {Is} a {Liability} {Not} a {Liability}?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x/abstract},
	doi = {10.1111/j.1540-6261.2010.01625.x},
	abstract = {Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in financial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identified as negative by the widely used Harvard Dictionary are words typically not considered negative in financial contexts. We develop an alternative negative word list, along with five other word lists, that better reflect tone in financial text. We link the word lists to 10-K filing returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.},
	language = {en},
	number = {1},
	urldate = {2018-01-11},
	journal = {The Journal of Finance},
	author = {Loughran, Tim and Mcdonald, Bill},
	month = feb,
	year = {2011},
	pages = {35--65},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/ZFIUBAP9/Loughran und Mcdonald - 2011 - When Is a Liability Not a Liability Textual Analy.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/TE73EQLZ/abstract.html:text/html}
}

@article{shapiro_measuring_2017,
	title = {Measuring {News} {Sentiment}},
	url = {https://econpapers.repec.org/paper/fipfedfwp/2017-01.htm},
	abstract = {We develop and assess new time series measures of economic sentiment based on computational text analysis of economic and financial newspaper articles from January 1980 to April 2015. The text analysis is based on predictive models estimated using machine learning techniques from Kanjoya. We analyze four alternative news sentiment indexes. We find that the news sentiment indexes correlate strongly with contemporaneous business cycle indicators. We also find that innovations to news sentiment predict future economic activity. Furthermore, in most cases, the news sentiment measures outperform the University of Michigan and Conference board measures in predicting the federal funds rate, consumption, employment, inflation, industrial production, and the S\&P500. For some of these economic outcomes, there is evidence that the news sentiment measures have significant predictive power even after conditioning on these survey-based measures.},
	number = {2017-01},
	urldate = {2018-01-11},
	journal = {Federal Reserve Bank of San Francisco Working Paper},
	author = {Shapiro, Adam and Sudhof, Moritz and Wilson, Daniel},
	month = jan,
	year = {2017},
	file = {RePEc PDF:/Users/Franzi/Zotero/storage/TB95H8YV/Shapiro et al. - 2017 - Measuring News Sentiment.pdf:application/pdf;RePEc Snapshot:/Users/Franzi/Zotero/storage/CJ57433M/2017-01.html:text/html}
}

@article{pritchard_association_2000,
	title = {Association {Mapping} in {Structured} {Populations}},
	volume = {67},
	issn = {0002-9297},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1287075/},
	abstract = {The use, in association studies, of the forthcoming dense genomewide collection of single-nucleotide polymorphisms (SNPs) has been heralded as a potential breakthrough in the study of the genetic basis of common complex disorders. A serious problem with association mapping is that population structure can lead to spurious associations between a candidate marker and a phenotype. One common solution has been to abandon case-control studies in favor of family-based tests of association, such as the transmission/disequilibrium test (TDT), but this comes at a considerable cost in the need to collect DNA from close relatives of affected individuals. In this article we describe a novel, statistically valid, method for case-control association studies in structured populations. Our method uses a set of unlinked genetic markers to infer details of population structure, and to estimate the ancestry of sampled individuals, before using this information to test for associations within subpopulations. It provides power comparable with the TDT in many settings and may substantially outperform it if there are conflicting associations in different subpopulations.},
	number = {1},
	urldate = {2018-01-19},
	journal = {American Journal of Human Genetics},
	author = {Pritchard, Jonathan K. and Stephens, Matthew and Rosenberg, Noah A. and Donnelly, Peter},
	month = jul,
	year = {2000},
	pmid = {10827107},
	pmcid = {PMC1287075},
	pages = {170--181},
	file = {PubMed Central Full Text PDF:/Users/Franzi/Zotero/storage/5C5QX5GU/Pritchard et al. - 2000 - Association Mapping in Structured Populations.pdf:application/pdf}
}

@article{erosheva_mixed-membership_2004-1,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	language = {en},
	number = {suppl 1},
	urldate = {2018-01-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	month = jun,
	year = {2004},
	pmid = {15020766},
	pages = {5220--5227},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/EXNI8WAK/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/CXECBUCC/5220.html:text/html}
}

@article{braun_variational_2010,
	title = {Variational inference for large-scale models of discrete choice},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/0712.2526},
	doi = {10.1198/jasa.2009.tm08030},
	abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
	number = {489},
	urldate = {2018-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Braun, Michael and McAuliffe, Jon},
	month = mar,
	year = {2010},
	note = {arXiv: 0712.2526},
	keywords = {Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology},
	pages = {324--335},
	file = {arXiv\:0712.2526 PDF:/Users/Franzi/Zotero/storage/NWRMRPXH/Braun und McAuliffe - 2010 - Variational inference for large-scale models of di.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/V373IACQ/0712.html:text/html}
}

@inproceedings{bischof_summarizing_2012,
	address = {USA},
	series = {{ICML}'12},
	title = {Summarizing {Topical} {Content} with {Word} {Frequency} and {Exclusivity}},
	isbn = {978-1-4503-1285-1},
	url = {http://dl.acm.org/citation.cfm?id=3042573.3042578},
	abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
	urldate = {2018-01-19},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Bischof, Jonathan M. and Airoldi, Edoardo M.},
	year = {2012},
	pages = {9--16}
}

@article{endres_new_2003,
	title = {A new metric for probability distributions},
	volume = {49},
	issn = {0018-9448},
	doi = {10.1109/TIT.2003.813506},
	abstract = {We introduce a metric for probability distributions, which is bounded, information-theoretically motivated, and has a natural Bayesian interpretation. The square root of the well-known χ2 distance is an asymptotic approximation to it. Moreover, it is a close relative of the capacitory discrimination and Jensen-Shannon divergence.},
	number = {7},
	journal = {IEEE Transactions on Information Theory},
	author = {Endres, D. M. and Schindelin, J. E.},
	month = jul,
	year = {2003},
	keywords = {Adaptive estimation, Algorithm design and analysis, asymptotic approximation, Bayes methods, Bayesian interpretation, Bayesian methods, bounded information-theoretically motivated metric, capacitory discrimination, Convergence, Gaussian noise, information theory, Iterative algorithms, Jensen-Shannon divergence, probability, Probability distribution, probability distributions, square root, Wavelet analysis, White noise, Writing, χ2 distance},
	pages = {1858--1860},
	file = {IEEE Xplore Abstract Record:/Users/Franzi/Zotero/storage/H6L3EIL4/1207388.html:text/html}
}

@inproceedings{he_detecting_2009,
	address = {New York, NY, USA},
	series = {{CIKM} '09},
	title = {Detecting {Topic} {Evolution} in {Scientific} {Literature}: {How} {Can} {Citations} {Help}?},
	isbn = {978-1-60558-512-3},
	shorttitle = {Detecting {Topic} {Evolution} in {Scientific} {Literature}},
	url = {http://doi.acm.org/10.1145/1645953.1646076},
	doi = {10.1145/1645953.1646076},
	abstract = {Understanding how topics in scientific literature evolve is an interesting and important problem. Previous work simply models each paper as a bag of words and also considers the impact of authors. However, the impact of one document on another as captured by citations, one important inherent element in scientific literature, has not been considered. In this paper, we address the problem of understanding topic evolution by leveraging citations, and develop citation-aware approaches. We propose an iterative topic evolution learning framework by adapting the Latent Dirichlet Allocation model to the citation network and develop a novel inheritance topic model. We evaluate the effectiveness and efficiency of our approaches and compare with the state of the art approaches on a large collection of more than 650,000 research papers in the last 16 years and the citation network enabled by CiteSeerX. The results clearly show that citations can help to understand topic evolution better.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {He, Qi and Chen, Bi and Pei, Jian and Qiu, Baojun and Mitra, Prasenjit and Giles, Lee},
	year = {2009},
	keywords = {citations, inheritance topic model, topic evolution},
	pages = {957--966}
}

@article{newman_distributed_2009,
	title = {Distributed {Algorithms} for {Topic} {Models}},
	volume = {10},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1577069.1755845},
	abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
	urldate = {2018-01-23},
	journal = {J. Mach. Learn. Res.},
	author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
	month = dec,
	year = {2009},
	pages = {1801--1828},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/8WPWT44A/Newman et al. - 2009 - Distributed Algorithms for Topic Models.pdf:application/pdf}
}

@inproceedings{kim_topic_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Topic {Chains} for {Understanding} a {News} {Corpus}},
	isbn = {978-3-642-19436-8 978-3-642-19437-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-19437-5_13},
	doi = {10.1007/978-3-642-19437-5_13},
	abstract = {The Web is a great resource and archive of news articles for the world. We present a framework, based on probabilistic topic modeling, for uncovering the meaningful structure and trends of important topics and issues hidden within the news archives on the Web. Central in the framework is a topic chain, a temporal organization of similar topics. We experimented with various topic similarity metrics and present our insights on how best to construct topic chains. We discuss how to interpret the topic chains to understand the news corpus by looking at long-term topics, temporary issues, and shifts of focus in the topic chains. We applied our framework to nine months of Korean Web news corpus and present our findings.},
	language = {en},
	urldate = {2018-01-23},
	booktitle = {Computational {Linguistics} and {Intelligent} {Text} {Processing}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Kim, Dongwoo and Oh, Alice},
	month = feb,
	year = {2011},
	pages = {163--176}
}

@inproceedings{wang_mining_2009,
	address = {New York, NY, USA},
	series = {{WSDM} '09},
	title = {Mining {Common} {Topics} from {Multiple} {Asynchronous} {Text} {Streams}},
	isbn = {978-1-60558-390-7},
	url = {http://doi.acm.org/10.1145/1498759.1498826},
	doi = {10.1145/1498759.1498826},
	abstract = {Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the {Second} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Wang, Xiang and Zhang, Kai and Jin, Xiaoming and Shen, Dou},
	year = {2009},
	keywords = {topic model, asynchronous streams, temporal text mining},
	pages = {192--201}
}

@book{ramage_labeled_2009,
	title = {Labeled {LDA}: {A} supervised topic model for credit attribution in multi-labeled corpora},
	shorttitle = {Labeled {LDA}},
	abstract = {A significant portion of the world’s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. 1},
	author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
	year = {2009},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/WD2TZ58J/Ramage et al. - Labeled LDA A supervised topic model for credit a.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/W5DGAHT5/summary.html:text/html}
}

@inproceedings{chaney_visualizing_2012,
	title = {Visualizing {Topic} {Models}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:    1. Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.    2. The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.    3. The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.    4. Author(s) retain all proprietary rights other than copyright (such as patent rights).    5. Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.    6. Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.    7. Author(s) may make limited distribution of all or portions of their article/paper prior to publication.    8. In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.    9. In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/view/4645},
	abstract = {Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools\&mdash;a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method.},
	language = {en},
	urldate = {2018-01-23},
	booktitle = {Sixth {International} {AAAI} {Conference} on {Weblogs} and {Social} {Media}},
	author = {Chaney, Allison June-Barlow and Blei, David M.},
	month = may,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/968W9M2F/Chaney und Blei - 2012 - Visualizing Topic Models.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/DXRG63IG/4645.html:text/html}
}