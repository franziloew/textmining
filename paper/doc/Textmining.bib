
@article{armstrong_competition_2006,
	title = {Competition in two-sided markets},
	volume = {37},
	issn = {1756-2171},
	url = {http://dx.doi.org/10.1111/j.1756-2171.2006.tb00037.x},
	doi = {10.1111/j.1756-2171.2006.tb00037.x},
	number = {3},
	journal = {The RAND Journal of Economics},
	author = {Armstrong, Mark},
	year = {2006},
	pages = {668--691}
}

@article{rochet_platform_2003,
	title = {Platform {Competition} in {Two}-{Sided} {Markets}},
	volume = {1},
	issn = {1542-4774},
	url = {http://onlinelibrary.wiley.com/doi/10.1162/154247603322493212/abstract},
	doi = {10.1162/154247603322493212},
	abstract = {Many if not most markets with network externalities are two-sided. To succeed, platforms in industries such as software, portals and media, payment systems and the Internet, must “get both sides of the market on board.” Accordingly, platforms devote much attention to their business model, that is, to how they court each side while making money overall. This paper builds a model of platform competition with two-sided markets. It unveils the determinants of price allocation and end-user surplus for different governance structures (profit-maximizing platforms and not-for-profit joint undertakings), and compares the outcomes with those under an integrated monopolist and a Ramsey planner. (JEL: L5, L82, L86, L96)},
	language = {en},
	number = {4},
	urldate = {2016-08-04},
	journal = {Journal of the European Economic Association},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	month = jun,
	year = {2003},
	pages = {990--1029},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/WJI5PS42/abstract.html:text/html}
}

@article{caillaud_chicken_2003,
	title = {Chicken \& {Egg}: {Competition} among {Intermediation} {Service} {Providers}},
	volume = {34},
	issn = {0741-6261},
	shorttitle = {Chicken \& {Egg}},
	url = {http://econpapers.repec.org/article/rjerandje/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.htm},
	abstract = {We analyze a model of imperfect price competition between intermediation service providers. We insist on features that are relevant for informational intermediation via the Internet: the presence of indirect network externalities, the possibility of using the nonexclusive services of several intermediaries, and the widespread practice of price discrimination based on users' identity and on usage. Efficient market structures emerge in equilibrium, as well as some specific form of inefficient structures. Intermediaries have incentives to propose non-exclusive services, as this moderates competition and allows them to exert market power. We analyze in detail the pricing and business strategies followed by intermediation services providers. Copyright 2003 by the RAND Corporation.},
	number = {2},
	urldate = {2016-10-18},
	journal = {RAND Journal of Economics},
	author = {Caillaud, Bernard and Jullien, Bruno},
	year = {2003},
	pages = {309--28},
	file = {RePEc Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/GD5R58G2/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.html:text/html}
}

@article{evans_economics_2008,
	title = {The {Economics} of the {Online} {Advertising} {Industry}},
	volume = {7},
	url = {https://ideas.repec.org/a/bpj/rneart/v7y2008i3n2.html},
	abstract = {Internet-based technologies are revolutionizing the stodgy \$625 billion global advertising industry. There are a number of public policy issues to consider. Will a single ad platform emerge or will several remain viable? What are the consequences of alternative market structures for a web economy that is increasingly based on selling eyeballs to advertisers? This article describes the online advertising industry. The industry is populated by a number of multi-sided platforms that facilitate connecting advertisers to viewers. Search-based advertising platforms, the most developed of these, have interesting economic features that result from the combination of keyword bidding by advertisers and single-homing.},
	number = {3},
	urldate = {2016-08-11},
	journal = {Review of Network Economics},
	author = {Evans, David S.},
	year = {2008},
	pages = {1--33},
	file = {RePEc Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/Z6B7DBRS/v7y2008i3n2.html:text/html}
}

@inproceedings{korencic_getting_2015,
	address = {New York, NY, USA},
	series = {{TM} '15},
	title = {Getting the {Agenda} {Right}: {Measuring} {Media} {Agenda} {Using} {Topic} {Models}},
	isbn = {978-1-4503-3784-7},
	shorttitle = {Getting the {Agenda} {Right}},
	url = {http://doi.acm.org/10.1145/2809936.2809942},
	doi = {10.1145/2809936.2809942},
	abstract = {Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies.},
	booktitle = {Proceedings of the 2015 {Workshop} on {Topic} {Models}: {Post}-{Processing} and {Applications}},
	publisher = {ACM},
	author = {Korenčić, Damir and Ristov, Strahil and Šnajder, Jan},
	year = {2015},
	keywords = {agenda measuring, agenda setting, document tagging, multilabel classification, news media analysis, topic modeling},
	pages = {61--66}
}

@article{tetlock_giving_2007,
	title = {Giving {Content} to {Investor} {Sentiment}: {The} {Role} of {Media} in the {Stock} {Market}},
	volume = {62},
	issn = {1540-6261},
	shorttitle = {Giving {Content} to {Investor} {Sentiment}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	language = {en},
	number = {3},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	month = jun,
	year = {2007},
	pages = {1139--1168},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/R4FBIJ83/abstract.html:text/html}
}

@article{baker_measuring_2016,
	title = {Measuring {Economic} {Policy} {Uncertainty}},
	url = {http://www.nber.org/papers/w21633},
	abstract = {We develop a new index of economic policy uncertainty (EPU) based on newspaper coverage frequency. Several types of evidence – including human readings of 12,000 newspaper articles – indicate that our index proxies for movements in policy-related economic uncertainty. Our US index spikes near tight presidential elections, Gulf Wars I and II, the 9/11 attacks, the failure of Lehman Brothers, the 2011 debt-ceiling dispute and other major battles over fiscal policy. Using firm-level data, we find that policy uncertainty raises stock price volatility and reduces investment and employment in policy-sensitive sectors like defense, healthcare, and infrastructure construction. At the macro level, policy uncertainty innovations foreshadow declines in investment, output, and employment in the United States and, in a panel VAR setting, for 12 major economies. Extending our US index back to 1900, EPU rose dramatically in the 1930s (from late 1931) and has drifted upwards since the 1960s.},
	number = {4},
	journal = {Quarterly Journal of Economics},
	author = {Baker, Scott R. and Bloom, Nicholas and Davis, Steven J.},
	year = {2016},
	note = {DOI: 10.3386/w21633},
	pages = {1593--1636}
}

@techreport{gentzkow_text_2017,
	type = {Working {Paper}},
	title = {Text as {Data}},
	url = {http://www.nber.org/papers/w23276},
	abstract = {An ever increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.},
	number = {23276},
	institution = {National Bureau of Economic Research},
	author = {Gentzkow, Matthew and Kelly, Bryan T. and Taddy, Matt},
	month = mar,
	year = {2017},
	note = {DOI: 10.3386/w23276}
}

@article{blei_probabilistic_2012,
	title = {Probabilistic {Topic} {Models}},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	number = {4},
	journal = {Commun. ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
	file = {ACM Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/GWUDPWQT/Blei - 2012 - Probabilistic Topic Models.pdf:application/pdf}
}

@article{blei_latent_2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	journal = {Journal of machine Learning research},
	author = {Blei, David M. and Ng, Andrew Y and Jordan, Michael I},
	month = jan,
	year = {2003},
	pages = {993--1022}
}

@incollection{mcauliffe_supervised_2008,
	title = {Supervised {Topic} {Models}},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {121--128},
	file = {NIPS Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/AKKVXDQ9/Mcauliffe und Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/KQV8JHW8/3328-supervised-topic-models.html:text/html}
}

@article{grimmer_bayesian_2010,
	title = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}: {Measuring} {Expressed} {Agendas} in {Senate} {Press} {Releases}},
	volume = {18},
	shorttitle = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}},
	url = {https://papers.ssrn.com/abstract=1541022},
	abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
	number = {1},
	urldate = {2017-10-07},
	journal = {Political Analysis},
	author = {Grimmer, Justin},
	year = {2010},
	keywords = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases, Justin  Grimmer, SSRN},
	pages = {1--35},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/3QRF5PHE/papers.html:text/html}
}

@article{pritchard_inference_2000,
	title = {Inference of population structure using multilocus genotype data},
	volume = {155},
	issn = {0016-6731},
	abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
	language = {eng},
	number = {2},
	journal = {Genetics},
	author = {Pritchard, J. K. and Stephens, M. and Donnelly, P.},
	month = jun,
	year = {2000},
	pmid = {10835412},
	pmcid = {PMC1461096},
	keywords = {Algorithms, Cluster Analysis, Genetics, Population, Genotype, Humans, Models, Genetic},
	pages = {945--959}
}

@inproceedings{taddy_estimation_2012,
	title = {On estimation and selection for topic models},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Taddy, Matt},
	year = {2012}
}

@article{airoldi_reconceptualizing_2010,
	title = {Reconceptualizing the classification of {PNAS} articles},
	volume = {107},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/107/49/20899},
	doi = {10.1073/pnas.1013452107},
	abstract = {PNAS article classification is rooted in long-standing disciplinary divisions that do not necessarily reflect the structure of modern scientific research. We reevaluate that structure using latent pattern models from statistical machine learning, also known as mixed-membership models, that identify semantic structure in co-occurrence of words in the abstracts and references. Our findings suggest that the latent dimensionality of patterns underlying PNAS research articles in the Biological Sciences is only slightly larger than the number of categories currently in use, but it differs substantially in the content of the categories. Further, the number of articles that are listed under multiple categories is only a small fraction of what it should be. These findings together with the sensitivity analyses suggest ways to reconceptualize the organization of papers published in PNAS.},
	language = {en},
	number = {49},
	urldate = {2017-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Joutard, Cyrille and Love, Tanzy and Shringarpure, Suyash},
	month = dec,
	year = {2010},
	pmid = {21078953},
	keywords = {Dirichlet process, hierarchical modeling, Monte Carlo Markov chain, text analysis, variational inference},
	pages = {20899--20904},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/3HPSSBQ2/Airoldi et al. - 2010 - Reconceptualizing the classification of PNAS artic.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/3SR8QVPZ/20899.html:text/html}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	number = {476},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	month = dec,
	year = {2006},
	pages = {1566--1581},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/67GVBDZ8/016214506000000302.html:text/html}
}

@inproceedings{blei_dynamic_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Dynamic {Topic} {Models}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Blei, David M. and Lafferty, John D.},
	year = {2006},
	pages = {113--120}
}

@article{airoldi_improving_2016,
	title = {Improving and {Evaluating} {Topic} {Models} and {Other} {Models} of {Text}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2015.1051182},
	doi = {10.1080/01621459.2015.1051182},
	abstract = {An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parameterizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. Here, we show that words that are both frequent and exclusive to a theme are more effective at characterizing topical content, and we propose a regularization scheme that leads to better estimates of these quantities. We consider a supervised setting where professional editors have annotated documents to topic categories, organized into a tree, in which leaf-nodes correspond to more specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze these annotated documents. A parallelized Hamiltonian Monte Carlo sampler allows the inference to scale to millions of documents. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. In this supervised setting, we validate the efficacy of word frequency and exclusivity at characterizing topical content on two very large collections of documents, from Reuters and the New York Times. In an unsupervised setting, we then consider a simplified version of the model that shares the same regularization scheme with the previous model. We carry out a large randomized experiment on Amazon Mechanical Turk to demonstrate that topic summaries based on frequency and exclusivity, estimated using the proposed regularization scheme, are more interpretable than currently established frequency-based summaries, and that the proposed model produces more efficient estimates of exclusivity than the currently established models.},
	number = {516},
	journal = {Journal of the American Statistical Association},
	author = {Airoldi, Edoardo M. and Bischof, Jonathan M.},
	month = oct,
	year = {2016},
	keywords = {Categorical data, Hamiltonian Monte Carlo, High-dimensional data, Parallel inference, text analysis},
	pages = {1381--1403},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/HWNC8RRH/01621459.2015.html:text/html}
}

@article{grimmer_text_2013,
	title = {Text as {Data}: {The} {Promise} and {Pitfalls} of {Automatic} {Content} {Analysis} {Methods} for {Political} {Texts}},
	volume = {21},
	shorttitle = {Text as {Data}},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Stewart, Brandon},
	year = {2013},
	pages = {267--297},
	file = {Text as Data\: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts | Brandon Stewart:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/FF9QRDJ4/text-data-promise-and-pitfalls-automatic-content-analysis-methods-political.html:text/html}
}

@inproceedings{roberts_structural_2013,
	title = {The {Structural} {Topic} {Model} and {Applied} {Social} {Science}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} {Workshop} on {Topic} {Models}: {Computation}, {Application}, and {Evaluation}.},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin and Airoldi, Edoardo},
	year = {2013},
	file = {The Structural Topic Model and Applied Social Science | Brandon Stewart:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/69G2KU2F/structural-topic-model-and-applied-social-science.html:text/html}
}

@incollection{socher_bayesian_2009,
	title = {A {Bayesian} {Analysis} of {Dynamics} in {Free} {Recall}},
	url = {http://papers.nips.cc/paper/3720-a-bayesian-analysis-of-dynamics-in-free-recall.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Socher, Richard and Gershman, Samuel and Sederberg, Per and Norman, Kenneth and Perotte, Adler J. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1714--1722},
	file = {NIPS Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/JQWKTPGH/Socher et al. - 2009 - A Bayesian Analysis of Dynamics in Free Recall.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/ZFW3QA7C/3720-a-bayesian-analysis-of-dynamics-in-free-recall.html:text/html}
}

@article{quinn_how_2010,
	title = {How to {Analyze} {Political} {Attention} with {Minimal} {Assumptions} and {Costs}},
	volume = {54},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2009.00427.x/abstract},
	doi = {10.1111/j.1540-5907.2009.00427.x},
	abstract = {Previous methods of analyzing the substance of political attention have had to make several restrictive assumptions or been prohibitively costly when applied to large-scale political texts. Here, we describe a topic model for legislative speech, a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Our method estimates, rather than assumes, the substance of topics, the keywords that identify topics, and the hierarchical nesting of topics. We use the topic model to examine the agenda in the U.S. Senate from 1997 to 2004. Using a new database of over 118,000 speeches (70,000,000 words) from the Congressional Record, our model reveals speech topic categories that are both distinctive and meaningfully interrelated and a richer view of democratic agenda dynamics than had previously been possible.},
	language = {en},
	number = {1},
	journal = {American Journal of Political Science},
	author = {Quinn, Kevin M. and Monroe, Burt L. and Colaresi, Michael and Crespin, Michael H. and Radev, Dragomir R.},
	month = jan,
	year = {2010},
	pages = {209--228},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/59JK6JEX/Quinn et al. - 2010 - How to Analyze Political Attention with Minimal As.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/QSVKCVD5/abstract.html:text/html}
}

@article{roberts_model_2016,
	title = {A {Model} of {Text} for {Experimentation} in the {Social} {Sciences}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	number = {515},
	journal = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	month = jul,
	year = {2016},
	keywords = {Causal inference, Experimentation, High-dimensional inference, Social sciences, text analysis, Variational approximation},
	pages = {988--1003},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/ZMEDWA9E/01621459.2016.html:text/html}
}

@book{airoldi_handbook_2014,
	title = {Handbook of mixed membership models and their applications},
	isbn = {978-1-4665-0408-0},
	url = {http://cds.cern.ch/record/1974849},
	abstract = {In response to scientific needs for more diverse and structured explanations of statistical data, researchers have discovered how to model individual data points as belonging to multiple groups. Handbook of Mixed Membership Models and Their Applications shows you how to use these flexible modeling tools to uncover hidden patterns in modern high-dimensional multivariate data. It explores the use of the models in various application settings, including survey data, population genetics, text analysis, image processing and annotation, and molecular biology.Through examples using real data sets, yo},
	urldate = {2017-10-12},
	publisher = {Taylor and Francis},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Blei, David},
	year = {2014},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/P7XE3MF4/1974849.html:text/html}
}

@inproceedings{mishler_using_2015,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Using {Structural} {Topic} {Modeling} to {Detect} {Events} and {Cluster} {Twitter} {Users} in the {Ukrainian} {Crisis}},
	isbn = {978-3-319-21379-8 978-3-319-21380-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108},
	doi = {10.1007/978-3-319-21380-4_108},
	abstract = {Structural topic modeling (STM) is a recently introduced technique to model how the content of a collection of documents changes as a function of variables such as author identity or time of writing. We present two proof-of-concept applications of STM using Russian social media data. In our first study, we model how topics change over time, showing that STM can be used to detect significant events such as the downing of Malaysia Air Flight 17. In our second study, we model how topical content varies across a set of authors, showing that STM can be used to cluster Twitter users who are sympathetic to Ukraine versus Russia as well as to cluster accounts that are suspected to belong to the same individual (so-called “sockpuppets”). Structural topic modeling shows promise as a tool for analyzing social media data, a domain that has been largely ignored in the topic modeling literature.},
	language = {en},
	urldate = {2017-10-12},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer, Cham},
	author = {Mishler, Alan and Crabb, Erin Smith and Paletz, Susannah and Hefright, Brook and Golonka, Ewa},
	month = aug,
	year = {2015},
	pages = {639--644},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/ACUCPA8U/Mishler et al. - 2015 - Using Structural Topic Modeling to Detect Events a.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/NG9CQ4T9/978-3-319-21380-4_108.html:text/html}
}

@article{griffiths_finding_2004,
	title = {Finding scientific topics},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5228},
	doi = {10.1073/pnas.0307752101},
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	language = {en},
	number = {suppl 1},
	urldate = {2017-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	month = apr,
	year = {2004},
	pmid = {14872004},
	pages = {5228--5235},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/9P87NGIJ/Griffiths und Steyvers - 2004 - Finding scientific topics.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/TX4TP3NE/5228.html:text/html}
}

@article{erosheva_mixed-membership_2004,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	language = {en},
	number = {suppl 1},
	urldate = {2017-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	month = apr,
	year = {2004},
	pmid = {15020766},
	pages = {5220--5227},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/QX4H27E9/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/NVF8AVJ7/5220.html:text/html}
}

@article{genkin_large-scale_2007,
	title = {Large-{Scale} {Bayesian} {Logistic} {Regression} for {Text} {Categorization}},
	volume = {49},
	issn = {0040-1706},
	url = {http://dx.doi.org/10.1198/004017007000000245},
	doi = {10.1198/004017007000000245},
	abstract = {Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results.},
	number = {3},
	journal = {Technometrics},
	author = {Genkin, Alexander and Lewis, David D. and Madigan, David},
	month = aug,
	year = {2007},
	keywords = {Information retrieval, Lasso, Penalization, Ridge regression, Support vector classifier, Variable selection},
	pages = {291--304},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/9JI658VW/Genkin et al. - 2007 - Large-Scale Bayesian Logistic Regression for Text .pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/9XV4KG9V/004017007000000245.html:text/html}
}

@phdthesis{ponweiser_latent_2012,
	type = {Theses / {Institute} for {Statistics} and {Mathematics}},
	title = {Latent {Dirichlet} {Allocation} in {R}},
	url = {http://epub.wu.ac.at/3558/},
	abstract = {Topic models are a new research field within the computer sciences information retrieval and text mining. They are generative probabilistic models of text corpora inferred by machine learning and they can be used for retrieval and text mining tasks. The most prominent topic model is latent Dirichlet allocation (LDA), which was introduced in 2003 by Blei et al. and has since then sparked off the development of other topic models for domain-specific purposes.
This thesis focuses on LDA's practical application. Its main goal is the replication of the data analyses from the 2004 LDA paper ``Finding scientific topics'' by Thomas Griffiths and Mark Steyvers within the framework of the R statistical programming language and the R{\textasciitilde}package topicmodels by Bettina Grün and Kurt Hornik. The complete process, including extraction of a text corpus from the PNAS journal's website, data preprocessing, transformation into a document-term matrix, model selection, model estimation, as well as presentation of the results, is fully documented and commented. The outcome closely matches the analyses of the original paper, therefore the research by Griffiths/Steyvers can be reproduced. Furthermore, this thesis proves the suitability of the R environment for text mining with LDA. (author's abstract)},
	language = {en},
	urldate = {2017-10-13},
	school = {WU Vienna University of Economics and Business},
	author = {Ponweiser, Martin},
	month = may,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/AKASJPTP/Ponweiser - 2012 - Latent Dirichlet Allocation in R.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/E5J47GZP/3558.html:text/html}
}

@article{silge_tidytext:_2016,
	title = {tidytext: {Text} {Mining} and {Analysis} {Using} {Tidy} {Data} {Principles} in {R}},
	shorttitle = {tidytext},
	doi = {10.21105/joss.00037},
	journal = {The Journal of Open Source Software},
	author = {Silge, Julia and Robinson, David},
	month = jul,
	year = {2016},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/KBIXHDAH/305219559_tidytext_Text_Mining_and_Analysis_Using_Tidy_Data_Principles_in_R.pdf:application/pdf}
}

@inproceedings{phan_learning_2008,
	address = {Beijing, China},
	title = {Learning to {Classify} {Short} and {Sparse} {Text} \& {Web} with {Hidden} {Topics} from {Large}-{Scale} {Data} {Collections}},
	doi = {10.1145/1367497.1367510},
	abstract = {This paper presents a general framework for building classi- fiers that deal with short and sparse text \& Web segments by making the most of hidden topics discovered from large- scale data collections. The main motivation of this work is that many classification tasks working with short segments of text \& Web, such as search snippets, forum \& chat mes- sages, blog \& news feeds, product reviews, and book \& movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data bet- ter. The underlying idea of the framework is that for each classification task, we collect a large-scale external data col- lection called "universal dataset", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and MEDLINE (18M words) with two tasks: "Web search domain disambiguation" and "disease categorization for medical text", and achieved significant quality enhancement.},
	booktitle = {Proceedings of the 17th {International} {World} {Wide} {Web} {Conference} ({WWW} 2008)},
	author = {Phan, Xuan-Hieu and Nguyen, Le and Horiguchi, Susumu},
	month = jan,
	year = {2008},
	pages = {91--100},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/SP33XWPF/221023426_Learning_to_Classify_Short_and_Sparse_Text_Web_with_Hidden_Topics_from_Large-Scale_Dat.pdf:application/pdf}
}

@inproceedings{rabinovich_inverse_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {The {Inverse} {Regression} {Topic} {Model}},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044829},
	abstract = {Taddy (2013) proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Rabinovich, Maxim and Blei, David M.},
	year = {2014},
	pages = {I--199--I--207}
}

@article{schiller_development_2016,
	title = {Development of the {Social} {Network} {Usage} in {Germany} since 2012},
	journal = {Working Paper TU Darmstadt},
	author = {Schiller, Benjamin and Heimbach, Irina and Strufe, Thorsten and Hinz, Oliver},
	year = {2016}
}

@article{bholat_text_2015,
	title = {Text {Mining} for {Central} {Banks}},
	issn = {1556-5068},
	url = {http://www.academia.edu/13430482/Text_mining_for_central_banks},
	abstract = {Although often applied in other social sciences, text mining has been less frequently used in economics and in policy circles, particularly inside central banks. This Handbook is a brief introduction to the field, discussing how text mining is useful},
	urldate = {2017-11-06},
	journal = {SSRN Electronic Journal},
	author = {Bholat, David M. and Hansen, Stephen and Santos, Pedro M. and Schonhardt-Bailey, Cheryl},
	month = jun,
	year = {2015},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/J462FFQU/Text_mining_for_central_banks.html:text/html}
}

@inproceedings{minka_expectation-propagation_2002,
	address = {San Francisco, CA, USA},
	series = {{UAI}'02},
	title = {Expectation-propagation for the {Generative} {Aspect} {Model}},
	isbn = {978-1-55860-897-9},
	url = {http://dl.acm.org/citation.cfm?id=2073876.2073918},
	abstract = {The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets.},
	booktitle = {Proceedings of the {Eighteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Minka, Thomas and Lafferty, John},
	year = {2002},
	pages = {352--359}
}

@article{roberts_stm:_2016,
	title = {stm: {R} {Package} for {Structural} {Topic} {Models}},
	volume = {forthcoming},
	shorttitle = {stm},
	journal = {Journal of Statistical Software},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	month = dec,
	year = {2016},
	file = {stm\: R Package for Structural Topic Models | Dustin Tingley:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/FAWQJE6X/stm-r-package-structural-topic-models.html:text/html}
}

@incollection{roberts_navigating_2016,
	address = {New York},
	title = {Navigating the {Local} {Modes} of {Big} {Data}: {The} {Case} of {Topic} {Models}.},
	booktitle = {Computational {Social} {Science}: {Discovery} and {Prediction}},
	publisher = {Cambridge University Press},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	year = {2016},
	file = {Navigating the Local Modes of Big Data\: The Case of Topic Models | Dustin Tingley:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/TW4DVST8/navigating-local-modes-big-data-case-topic-models.html:text/html}
}

@techreport{grajzl_structural_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {Structural} {Topic} {Model} of the {Features} and the {Cultural} {Origins} of {Bacon}'s {Ideas}},
	url = {https://papers.ssrn.com/abstract=2944816},
	abstract = {We use machine-learning methods to study the features and origins of the thought of Francis Bacon, a key figure in the development of a cultural paradigm that provided intellectual roots for modern economic development. We estimate a structural topic model, a state-of-the-art methodology for analysis of text corpora. The estimates uncover sixteen topics prominent in Bacon's opus. Two are central in the ideas usually associated with Bacon: inductive epistemology and fact-seeking. While Bacon's epistemology is strongly connected with his jurisprudence, fact-seeking is more isolated from Bacon's other intellectual pursuits. The utilitarian promise of science and the central organization of the scientific quest, embraced by Bacon's followers, were not emphasized by him, a finding suggesting that these aspects of the 'Baconian' culture owed little to Bacon's own contributions. Bacon's use of different topics varies notably with intended audience and chosen medium.},
	number = {ID 2944816},
	urldate = {2017-11-07},
	institution = {Social Science Research Network},
	author = {Grajzl, Peter and Murrell, Peter},
	month = oct,
	year = {2017},
	keywords = {culture, Francis Bacon, knowledge, law, natural philosophy, politics, religion},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/B862TXNA/papers.html:text/html}
}

@article{baturo_what_2017,
	title = {What {Drives} the {International} {Development} {Agenda}? {An} {NLP} {Analysis} of the {United} {Nations} {General} {Debate} 1970-2016},
	shorttitle = {What {Drives} the {International} {Development} {Agenda}?},
	url = {http://arxiv.org/abs/1708.05873},
	abstract = {There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.},
	journal = {arXiv:1708.05873 [cs]},
	author = {Baturo, Alexander and Dasandi, Niheer and Mikhaylov, Slava J.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05873},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv\:1708.05873 PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/FSRGQVHA/Baturo et al. - 2017 - What Drives the International Development Agenda .pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/H2JRUWQF/1708.html:text/html}
}

@incollection{hagen_data_2018,
	series = {Public {Administration} and {Information} {Technology}},
	title = {Data {Analytics} for {Policy} {Informatics}: {The} {Case} of {E}-{Petitioning}},
	isbn = {978-3-319-61761-9 978-3-319-61762-6},
	shorttitle = {Data {Analytics} for {Policy} {Informatics}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-61762-6_9},
	abstract = {To contribute to the development of policy informatics, we discuss the benefits of analyzing electronic petitions (e-petitions), a form of citizen-government discourse with deep historic roots that has recently transitioned into a technologically-enabled and novel form of political communication. We begin by presenting a rationale for the analysis of e-petitions as a type of e-participation that can contribute to the development of public policy, provided that it is possible to analyze the large volumes of data produced in petitioning processes. From there we consider two data analytic strategies that offer promising approaches to the analysis of e-petitions and that lend themselves to the future creation of policy informatics tools. We discuss the application of topic modeling to the analysis of e-petition textual data to identify emergent topics of substantial concern to the public. We further propose the application of social network analysis to data related to the dynamics of petitioning processes, such as the social connections between petition initiators and signers, and tweets that solicit petition signatures in petitioning campaigns; both may be useful in revealing patterns of collective action. The paper concludes by reflecting on issues that should be brought to bear on the construction of policy informatics tools that make use of e-petitioning data.},
	language = {en},
	urldate = {2017-11-09},
	booktitle = {Policy {Analytics}, {Modelling}, and {Informatics}},
	publisher = {Springer, Cham},
	author = {Hagen, Loni and Harrison, Teresa M. and Dumas, Catherine L.},
	year = {2018},
	note = {DOI: 10.1007/978-3-319-61762-6\_9},
	pages = {205--224},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/QBNNBEIJ/978-3-319-61762-6_9.html:text/html}
}

@article{haixia_extracting_2016,
	title = {Extracting {Topics} of {Computer} {Science} {Literature} with {LDA} {Model}, {Extracting} {Topics} of {Computer} {Science} {Literature} with {LDA} {Model}},
	volume = {32},
	issn = {2096-3467},
	url = {http://manu44.magtech.com.cn/Jwk_infotech_wk3/EN/abstract/abstract4288.shtml},
	doi = {10.11925/infotech.1003-3513.2016.11.03},
	language = {cn},
	number = {11},
	urldate = {2017-11-09},
	journal = {Data Analysis and Knowledge Discovery},
	author = {Haixia, Yang and Baojun, Gao and Hanlin, Sun and Haixia, Yang and Baojun, Gao and Hanlin, Sun},
	month = dec,
	year = {2016},
	pages = {20--26},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/PR58B4CK/Haixia et al. - 2016 - Extracting Topics of Computer Science Literature w.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/DDFB9U5P/abstract4288.html:text/html}
}

@article{das_trends_2017,
	title = {Trends in {Transportation} {Research}},
	volume = {2614},
	issn = {0361-1981},
	url = {http://trrjournalonline.trb.org/doi/abs/10.3141/2614-04},
	doi = {10.3141/2614-04},
	abstract = {Proceedings of journal and conference papers are good sources of big textual data to examine research trends in various branches of science. The contents, usually unstructured in nature, require fast machine-learning algorithms to be deciphered. Exploratory analysis through text mining usually provides the descriptive nature of the contents but lacks quantification of the topics and their correlations. Topic models are algorithms designed to discover the main theme or trend in massive collections of unstructured documents. Through the use of a structural topic model, an extension of latent Dirichlet allocation, this study introduced distinct topic models on the basis of the relative frequencies of the words used in the abstracts of 15,357 TRB compendium papers. With data from 7 years (2008 through 2014) of TRB annual meeting compendium papers, the 20 most dominant topics emerged from a bag of 4 million words. The findings of this study contributed to the understanding of topical trends in the complex and evolving field of transportation engineering research.},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Das, Subasish and Dixon, Karen and Sun, Xiaoduan and Dutta, Anandi and Zupancich, Michelle},
	month = jan,
	year = {2017},
	pages = {27--38},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/FTII7NXR/2614-04.html:text/html}
}

@article{roberts_structural_2014,
	title = {Structural {Topic} {Models} for {Open}-{Ended} {Survey} {Responses}},
	volume = {58},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12103/abstract},
	doi = {10.1111/ajps.12103},
	abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
	language = {en},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	month = oct,
	year = {2014},
	pages = {1064--1082},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/FD9RK878/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Resp.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/6GGMT3JM/abstract.html:text/html}
}

@article{lucas_computer_2015,
	title = {Computer assisted text analysis for comparative politics},
	volume = {23},
	number = {2},
	journal = {Political Analysis},
	author = {Lucas, Christopher and Nielsen, Richard and Roberts, Margaret and Stewart, Brandon and Storer, Alex and Tingley, Dustin},
	year = {2015},
	pages = {254--277},
	file = {Computer assisted text analysis for comparative politics | Dustin Tingley:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/N5EV4K52/computer-assisted-text-analysis-comparative-politics.html:text/html}
}

@techreport{mueller_reading_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Reading between the {Lines}: {Prediction} of {Political} {Violence} {Using} {Newspaper} {Text}},
	shorttitle = {Reading between the {Lines}},
	url = {https://papers.ssrn.com/abstract=2843535},
	abstract = {This article provides a new methodology to predict armed conflict by using newspaper text. Through machine learning, vast quantities of newspaper text are reduced to interpretable topics. We propose the use of the within-country variation of these topics to predict the timing of conflict. This allows us to avoid the tendency of predicting conflict only in countries where it occurred before. We show that the within-country variation of topics is an extremely robust predictor of conflict and becomes particularly useful when new conflict risks arise. Two aspects seem to be responsible for these features. Topics provide depth because they consist of changing, long lists of terms which makes them able to capture the changing context of conflict. At the same time topics provide width because they summarize all text, including coverage of stabilizing factors.},
	number = {ID 2843535},
	urldate = {2017-11-09},
	institution = {Social Science Research Network},
	author = {Mueller, Hannes Felix and Rauh, Christopher},
	month = sep,
	year = {2016},
	keywords = {Civil War, conflict, Forecasting, Latent Dirichlet Allocation., Machine Learning, panel data, Topic Models},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/R7FGX7V9/papers.html:text/html}
}

@techreport{law_constitutional_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Constitutional {Archetypes}},
	url = {https://papers.ssrn.com/abstract=2732519},
	abstract = {It is a core function of constitutions to justify the existence and organization of the state. The ideological narratives embedded in constitutions are not fundamentally unique, however, but instead derive from a limited number of competing models. Each model is defined by a particular type of justification for the existence and organization of the state, and by a symbiotic relationship with a particular legal tradition. These models are so ubiquitous and elemental that they amount to constitutional archetypes.This Article contends as an empirical matter that constitutional narratives of the state boil down to a combination of three basic archetypes–namely, a liberal archetype, a statist archetype, and a universalist archetype. The liberal archetype is closely identified with the common law tradition and views the state as a potentially oppressive concentration of authority in need of regulation and restraint. In keeping with this conception of the state, liberal constitutions emphasize the imposition of limits upon government in the form of negative and procedural rights, as well as a strong and independent judiciary to make these limits effective. The legitimacy of the state is contingent upon adherence to constitutional limits. Constitutions in this vein are largely agnostic as to what goals, if any, society as a whole should pursue through the mechanism of the state.The statist archetype, in contrast, is associated with the civil law tradition and hails the state as the embodiment of a distinctive community and the vehicle for the achievement of the community’s goals. The legitimacy of the state rests upon the strength of the state’s claim to represent the will of a community. Consequently, constitutions in this vein are attentive to the identity, membership, and symbols of the state. Other characteristics of a statist constitution include an emphasis on the articulation of collective goals and positive rights that contemplate an active role for the state, and an obligation on the part of citizens to cooperate with the state in the pursuit of shared goals.The universalist archetype, the newest and most prevalent of the three, is symbiotically intertwined with a post-World War II, post-Westphalian paradigm of international law that rests the legitimacy of the state upon the normative force of a global legal order that encompasses both constitutional law and international law. Characteristics of this archetype include explicit commitment to supranational institutions and supranational law and reliance on generic terms and concepts that can be found not only in a variety of national constitutions, but also in international legal instruments.Empirical evidence of the prevalence and content of these three basic archetypes can be found in the unlikeliest of places – namely, constitutional preambles. Preambles enjoy a reputation for expressing uniquely national values, identities, and narratives. If there is any part of a constitution that ought not to be reducible to a handful of recurring patterns, it is surely the preamble. Yet analysis of the world’s constitutional preambles using methods from computational linguistics suggests that they consist of a combination of the three archetypes. Estimation of a structural topic model yields a quantitative measure of the extent to which each preamble draws upon each archetype.The empirical analysis also highlights the growing commingling and interdependence of constitutional law and international law. The semantic patterns that characterize universalist preambles mirrors those found in leading international human rights instruments. The adoption of the same conceptual and normative vocabulary by both universalist constitutions and key international legal instruments signals the emergence of a globalized ideological dialect common to both domestic constitutional law and public international law. The rising use of this common language by constitutional drafters since World War II is a quantitative indicator of the growing extent to which constitutional law and public international law influence each other.},
	number = {ID 2732519},
	urldate = {2017-11-09},
	institution = {Social Science Research Network},
	author = {Law, David S.},
	month = dec,
	year = {2016},
	keywords = {archetype, automated, civil law, common law, constitution, constitutional drafting, constitutional law, constitution-making, constitution-writing, content analysis, empirical, ideology, international law, legal traditions, liberalism, preamble, statism, text analysis, topic model, universalism},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/IMHWX425/papers.html:text/html}
}

@article{farrell_corporate_2016,
	title = {Corporate funding and ideological polarization about climate change},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/1/92},
	doi = {10.1073/pnas.1509433112},
	abstract = {Drawing on large-scale computational data and methods, this research demonstrates how polarization efforts are influenced by a patterned network of political and financial actors. These dynamics, which have been notoriously difficult to quantify, are illustrated here with a computational analysis of climate change politics in the United States. The comprehensive data include all individual and organizational actors in the climate change countermovement (164 organizations), as well as all written and verbal texts produced by this network between 1993–2013 (40,785 texts, more than 39 million words). Two main findings emerge. First, that organizations with corporate funding were more likely to have written and disseminated texts meant to polarize the climate change issue. Second, and more importantly, that corporate funding influences the actual thematic content of these polarization efforts, and the discursive prevalence of that thematic content over time. These findings provide new, and comprehensive, confirmation of dynamics long thought to be at the root of climate change politics and discourse. Beyond the specifics of climate change, this paper has important implications for understanding ideological polarization more generally, and the increasing role of private funding in determining why certain polarizing themes are created and amplified. Lastly, the paper suggests that future studies build on the novel approach taken here that integrates large-scale textual analysis with social networks.},
	language = {en},
	number = {1},
	urldate = {2017-11-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Farrell, Justin},
	month = jan,
	year = {2016},
	pmid = {26598653},
	keywords = {climate change, computational social science, funding, polarization, politics},
	pages = {92--97},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/8M3IS9E4/Farrell - 2016 - Corporate funding and ideological polarization abo.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/9R5TQ8PC/92.html:text/html}
}

@article{reich_computer-assisted_2014,
	title = {Computer-{Assisted} {Reading} and {Discovery} for {Student} {Generated} {Text} in {Massive} {Open} {Online} {Courses}},
	volume = {2},
	copyright = {Copyright (c)},
	issn = {1929-7750},
	url = {http://www.learning-analytics.info/journals/index.php/JLA/article/view/4138},
	doi = {10.18608/jla.2015.21.8},
	abstract = {Dealing with the vast quantities of text that students generate in Massive Open Online Courses (MOOCs) and other large-scale online learning environments is a daunting challenge. Computational tools are needed to help instructional teams uncover themes and patterns as students write in forums, assignments, and surveys. This paper introduces to the learning analytics community the Structural Topic Model, an approach to language processing that can 1) ﬁnd syntactic patterns with semantic meaning in unstructured text, 2) identify variation in those patterns across covariates, and 3) uncover archetypal texts that exemplify the documents within a topical pattern. We show examples of computationally aided discovery and reading in three MOOC settings: mapping students’ self-reported motivations, identifying themes in discussion forums, and uncovering patterns of feedback in course evaluations.},
	language = {en},
	number = {1},
	urldate = {2017-11-09},
	journal = {Journal of Learning Analytics},
	author = {Reich, Justin and Tingley, Dustin and Leder-Luis, Jetson and Roberts, Margaret E. and Stewart, Brandon},
	month = nov,
	year = {2014},
	keywords = {computer‐assisted reading, Massive Open Online Courses, text analysis, topic modelling},
	pages = {156--184},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/BPWP7FIR/Reich et al. - 2014 - Computer-Assisted Reading and Discovery for Studen.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/52JEF8DU/4138.html:text/html}
}

@incollection{gabszewicz_media_2015,
	series = {Handbook on the economics of the media. - {Cheltenham}, {UK} : {Edward} {Elgar} {Publishing}, {ISBN} 978-0-85793-888-6. - 2015, p. 3-35},
	title = {Media as multi-sided platforms},
	language = {eng},
	booktitle = {Handbook on the economics of the media},
	author = {Gabszewicz, Jean Jaskold and Resende, Joana and Sonnac, Nathalie},
	year = {2015},
	file = {Media as multi-sided platforms - EconBiz:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/MS8D937F/10011339834.html:text/html}
}

@inproceedings{mimno_optimizing_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Optimizing {Semantic} {Coherence} in {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	year = {2011},
	pages = {262--272},
	file = {ACM Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/758M9D8V/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{wei_lda-based_2006,
	address = {New York, NY, USA},
	series = {{SIGIR} '06},
	title = {{LDA}-based {Document} {Models} for {Ad}-hoc {Retrieval}},
	isbn = {978-1-59593-369-0},
	url = {http://doi.acm.org/10.1145/1148170.1148204},
	doi = {10.1145/1148170.1148204},
	abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
	booktitle = {Proceedings of the 29th {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wei, Xing and Croft, W. Bruce},
	year = {2006},
	keywords = {document model, Information retrieval, language model, latent dirichlet allocation (LDA), topic model},
	pages = {178--185}
}

@inproceedings{wallach_evaluation_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Evaluation {Methods} for {Topic} {Models}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553515},
	doi = {10.1145/1553374.1553515},
	abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
	year = {2009},
	pages = {1105--1112}
}

@incollection{chang_reading_2009,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	shorttitle = {Reading {Tea} {Leaves}},
	url = {http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan L. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {288--296},
	file = {NIPS Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/UUPBMREK/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/68XT476P/3700-reading-tea-leaves-how-humans-interpret-topic-models.html:text/html}
}

@inproceedings{blei_latent_2001,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'01},
	title = {Latent {Dirichlet} {Allocation}},
	url = {http://dl.acm.org/citation.cfm?id=2980539.2980618},
	abstract = {We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hof-mann's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Neural} {Information} {Processing} {Systems}: {Natural} and {Synthetic}},
	publisher = {MIT Press},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2001},
	pages = {601--608}
}

@incollection{griffiths_hierarchical_2004,
	title = {Hierarchical {Topic} {Models} and the {Nested} {Chinese} {Restaurant} {Process}},
	url = {http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Griffiths, Thomas L. and Jordan, Michael I. and Tenenbaum, Joshua B. and Blei, David M.},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
	year = {2004},
	pages = {17--24},
	file = {NIPS Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/RC2JTMS3/Griffiths et al. - 2004 - Hierarchical Topic Models and the Nested Chinese R.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/H6Z7JS5C/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.html:text/html}
}

@article{blei_nested_2007,
	title = {The nested {Chinese} restaurant process and {Bayesian} nonparametric inference of topic hierarchies},
	url = {http://arxiv.org/abs/0710.0845},
	abstract = {We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
	journal = {arXiv:0710.0845 [stat]},
	author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
	month = oct,
	year = {2007},
	note = {arXiv: 0710.0845},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:0710.0845 PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/QF33HDD7/Blei et al. - 2007 - The nested Chinese restaurant process and Bayesian.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/GC56ZTK4/0710.html:text/html}
}

@article{griffiths_probabilistic_2002,
	title = {A probabilistic approach to semantic representation},
	volume = {24},
	url = {https://escholarship.org/uc/item/44x9v7m7},
	number = {24},
	urldate = {2017-11-16},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	month = jan,
	year = {2002},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/6EQ8VAJR/Griffiths und Steyvers - 2002 - A probabilistic approach to semantic representatio.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/76HFDXBQ/44x9v7m7.pdf:application/pdf}
}

@techreport{heinrich_parameter_2004,
	title = {Parameter estimation for text analysis},
	abstract = {Abstract. Presents parameter estimation methods common with discrete probability distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation (LDA) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, including a discussion of Dirichlet hyperparameter estimation. Finally, analysis methods of LDA models are discussed.},
	author = {Heinrich, Gregor},
	year = {2004},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/PTHWFSPM/Heinrich - 2004 - Parameter estimation for text analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/JZDEF9EX/summary.html:text/html}
}

@incollection{steyvers_probabilistic_2006,
	title = {Probabilistic {Topic} {Models}},
	booktitle = {Latent {Semantic} {Analysis}: {A} {Road} to {Meaning}.},
	publisher = {Laurence Erlbaum},
	author = {Steyvers, Mark and Griffiths, Thomas L.},
	editor = {Landauer, L. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
	year = {2006}
}

@incollection{hoffman_online_2010,
	title = {Online {Learning} for {Latent} {Dirichlet} {Allocation}},
	url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {856--864},
	file = {NIPS Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/E3N5REJ7/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/NE5ZUD83/3902-online-learning-for-latent-dirichlet-allocation.html:text/html}
}

@inproceedings{buntine_estimating_2009,
	address = {Berlin, Heidelberg},
	series = {{ACML} '09},
	title = {Estimating {Likelihoods} for {Topic} {Models}},
	isbn = {978-3-642-05223-1},
	url = {http://dx.doi.org/10.1007/978-3-642-05224-8_6},
	doi = {10.1007/978-3-642-05224-8_6},
	abstract = {Topic models are a discrete analogue to principle component analysis and independent component analysis that model {\textless}em{\textgreater}topic{\textless}/em{\textgreater} at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.},
	booktitle = {Proceedings of the 1st {Asian} {Conference} on {Machine} {Learning}: {Advances} in {Machine} {Learning}},
	publisher = {Springer-Verlag},
	author = {Buntine, Wray},
	year = {2009},
	pages = {51--64}
}

@inproceedings{alsumait_topic_2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Topic {Significance} {Ranking} of {LDA} {Generative} {Models}},
	isbn = {978-3-642-04179-2 978-3-642-04180-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-04180-8_22},
	doi = {10.1007/978-3-642-04180-8_22},
	abstract = {Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distribution” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.},
	language = {en},
	urldate = {2017-11-16},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {AlSumait, Loulwah and Barbará, Daniel and Gentle, James and Domeniconi, Carlotta},
	month = sep,
	year = {2009},
	pages = {67--82},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/WCPH8XR4/10.html:text/html}
}

@inproceedings{mimno_optimizing_2011-1,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Optimizing {Semantic} {Coherence} in {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	year = {2011},
	pages = {262--272},
	file = {ACM Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/8ATB9444/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{mimno_bayesian_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Bayesian {Checking} for {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
	abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Blei, David},
	year = {2011},
	pages = {227--237},
	file = {ACM Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/MFUGSEJM/Mimno und Blei - 2011 - Bayesian Checking for Topic Models.pdf:application/pdf}
}

@inproceedings{newman_automatic_2010,
	address = {Stroudsburg, PA, USA},
	series = {{HLT} '10},
	title = {Automatic {Evaluation} of {Topic} {Coherence}},
	isbn = {978-1-932432-65-7},
	url = {http://dl.acm.org/citation.cfm?id=1857999.1858011},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
	booktitle = {Human {Language} {Technologies}: {The} 2010 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
	year = {2010},
	pages = {100--108},
	file = {ACM Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/QJIP655P/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf:application/pdf}
}

@incollection{wallach_rethinking_2009,
	title = {Rethinking {LDA}: {Why} {Priors} {Matter}},
	shorttitle = {Rethinking {LDA}},
	url = {http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Wallach, Hanna M. and Mimno, David M. and McCallum, Andrew},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1973--1981},
	file = {NIPS Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/TPHKVI25/Wallach et al. - 2009 - Rethinking LDA Why Priors Matter.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/6FVP9RXD/3854-rethinking-lda-why-priors-matter.html:text/html}
}

@article{asuncion_smoothing_2012,
	title = {On {Smoothing} and {Inference} for {Topic} {Models}},
	url = {http://arxiv.org/abs/1205.2662},
	abstract = {Latent Dirichlet analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. Various learning algorithms have been developed in recent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.},
	journal = {arXiv:1205.2662 [cs, stat]},
	author = {Asuncion, Arthur and Welling, Max and Smyth, Padhraic and Teh, Yee Whye},
	month = may,
	year = {2012},
	note = {arXiv: 1205.2662},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1205.2662 PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/MRRAS29S/Asuncion et al. - 2012 - On Smoothing and Inference for Topic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/UHH4ICFA/1205.html:text/html}
}

@inproceedings{hofmann_probabilistic_1999,
	address = {New York, NY, USA},
	series = {{SIGIR} '99},
	title = {Probabilistic {Latent} {Semantic} {Indexing}},
	isbn = {978-1-58113-096-6},
	url = {http://doi.acm.org/10.1145/312624.312649},
	doi = {10.1145/312624.312649},
	booktitle = {Proceedings of the 22Nd {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Hofmann, Thomas},
	year = {1999},
	pages = {50--57}
}

@article{deerwester_indexing_1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.},
	number = {6},
	journal = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	year = {1990},
	pages = {391--407},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/EVGTHKF3/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/5KT8CZXC/summary.html:text/html}
}

@article{corden_maximisation_1952,
	title = {The {Maximisation} of {Profit} by a {Newspaper}},
	volume = {20},
	issn = {0034-6527},
	url = {http://www.jstor.org/stable/2295888},
	doi = {10.2307/2295888},
	number = {3},
	journal = {The Review of Economic Studies},
	author = {Corden, W. M.},
	year = {1952},
	pages = {181--190}
}

@article{gustafsson_circulation_1978,
	title = {The circulation spiral and the principle of household coverage},
	volume = {26},
	issn = {0358-5522},
	url = {https://doi.org/10.1080/03585522.1978.10407893},
	doi = {10.1080/03585522.1978.10407893},
	abstract = {The growth of oligopoly within the newspaper industry is a widespread phenomenon which has been examined by both researchers into the mass media and public enquiries into the press in a number of countries. Politicians recognise the development and want to modify the process of concentration, prevent newspaper closures, and even promote new ventures. Many western countries have taken measures to try to control forces bearing toward concentration in the newspaper industry. Such efforts, however, require a thorough knowledge of the market and its mechanism.},
	number = {1},
	journal = {Scandinavian Economic History Review},
	author = {Gustafsson, Karl Erik},
	month = jan,
	year = {1978},
	pages = {1--14},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/D8TJWJWK/03585522.1978.html:text/html}
}

@article{blair_pricing_1993,
	title = {Pricing {Decisions} of the {Newspaper} {Monopolist}},
	volume = {59},
	issn = {0038-4038},
	url = {http://www.jstor.org/stable/1059734},
	doi = {10.2307/1059734},
	number = {4},
	journal = {Southern Economic Journal},
	author = {Blair, Roger D. and Romano, Richard E.},
	year = {1993},
	pages = {721--732}
}

@article{evans_empirical_2003,
	title = {Some {Empirical} {Aspects} of {Multi}-sided {Platform} {Industries}},
	volume = {2},
	issn = {1446-9022},
	url = {https://www.degruyter.com/view/j/rne.2003.2.issue-3/rne.2003.2.3.1026/rne.2003.2.3.1026.xml},
	doi = {10.2202/1446-9022.1026},
	abstract = {Multi-sided platform markets have two or more different groups of customers that businesses have to get and keep on board to succeed. These industries range from dating clubs (men and women), to video game consoles (game developers and users), to payment cards (cardholders and merchants), to operating system software (application developers and users). They include some of the most important industries in the economy. A survey of businesses in these industries shows that multi-sided platform businesses devise entry strategies to get multiple sides of the market on board and devise pricing, product, and other competitive strategies to keep multiple customer groups on a common platform that internalizes externalities across members of these groups.},
	number = {3},
	journal = {Review of Network Economics},
	author = {Evans, David S.},
	year = {2003},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/JXJD8ZCR/Evans - 2003 - Some Empirical Aspects of Multi-sided Platform Ind.pdf:application/pdf}
}

@article{ellman_what_2009,
	title = {What do the {Papers} {Sell}? {A} {Model} of {Advertising} and {Media} {Bias}*},
	volume = {119},
	issn = {1468-0297},
	shorttitle = {What do the {Papers} {Sell}?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02218.x/abstract},
	doi = {10.1111/j.1468-0297.2009.02218.x},
	abstract = {We model the market for news as a two-sided market where newspapers sell news to readers who value accuracy and sell space to advertisers who value advert-receptive readers. In this setting, monopolistic newspapers under-report or bias news that sufficiently reduces advertiser profits. Paradoxically, increasing the size of advertising eventually leads competing newspapers to reduce advertiser bias. Nonetheless, advertisers can counter this effect if able to commit to news-sensitive cut-off strategies, potentially inducing as much bias as in the monopoly case. We use these results to explain contrasting historical and recent evidence on commercial bias and influence in the media.},
	language = {en},
	number = {537},
	journal = {The Economic Journal},
	author = {Ellman, Matthew and Germano, Fabrizio},
	month = apr,
	year = {2009},
	pages = {680--704},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/ESUGMQCA/Ellman und Germano - 2009 - What do the Papers Sell A Model of Advertising an.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/E87BHVVH/abstract.html:text/html}
}

@book{wiedmann_text_2016,
	address = {Wiesbaden},
	edition = {1},
	title = {Text {Mining} for {Qualitative} {Data} {Analysis} in the {Social} {Sciences}},
	url = {//www.springer.com/de/book/9783658153083},
	urldate = {2017-11-26},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Wiedmann, Gregor},
	year = {2016},
	file = {Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/7P4K4CSX/9783658153083.html:text/html}
}

@incollection{boogaart_linear_2013,
	series = {Use {R}!},
	title = {Linear {Models} for {Compositions}},
	isbn = {978-3-642-36808-0 978-3-642-36809-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-36809-7_5},
	abstract = {Compositions can play the role of dependent and independent variables in linear models. In both cases, the parameters of the linear models are again compositions of the same simplex as the data. Most methods for classical linear models have a close analog in these compositional linear models. This chapter addresses several questions on this subject. What are compositional linear models? How to visualize the dependence of compositions, already multivariable, with further external covariables? How to model and check such dependence with compositional linear models? What are the underlying assumptions? How can we check these assumptions? What is the compositional interpretation of the results? How to use linear models to provide statistical evidence with tests, confidence intervals, and predictive regions? How to visualize model results and model parameters? How to compare compositional linear models and how to find the most appropriate one?},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {Analyzing {Compositional} {Data} with {R}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Boogaart, K. Gerald van den and Tolosana-Delgado, Raimon},
	year = {2013},
	note = {DOI: 10.1007/978-3-642-36809-7\_5},
	pages = {95--175},
	file = {Full Text PDF:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/XXRU4BZK/Boogaart und Tolosana-Delgado - 2013 - Linear Models for Compositions.pdf:application/pdf;Snapshot:/Users/Franzi/Library/Application Support/Zotero/Profiles/la73mnmp.default/zotero/storage/66BBBZV2/978-3-642-36809-7_5.html:text/html}
}