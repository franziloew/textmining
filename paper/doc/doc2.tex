\documentclass[12pt,a4paper,notitlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{titling}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage[caption = false]{subfig}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{tikz-3dplot}
\usepackage{subcaption}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{multirow,rotating}
\usepackage[autostyle]{csquotes}
\usepackage[toc,page]{appendix}
\DeclareUnicodeCharacter{20AC}{\euro}
\usepackage[backend=biber,
			style=authoryear-comp,
			isbn=false,
			doi=false,
			bibstyle=authoryear,
			natbib,
			]{biblatex}

\begin{document}

\section{Introduction}

Social networks such as Facebook are becoming increasingly important for online news services: an increasing number of their readers access the news pages via links in the networks. Users of Facebook, for example, can use their profile to share links to external websites - such as news portals - with their online friends. This has led to the development of social media into an important generator of traffic on the internet pages. In Germany, 94\% of online news articles are shared on Facebook in 2015, followed by Twitter with 3.5\% and Google+ with 2.3\% \citep{schiller_development_2016}. The advertising-financed business model of the media houses is based on the premise that users visit their websites in order to achieve high advertising revenues. For this reason, news agencies are particularly interested in finding out which topics are often shared on Facebook. \citet{schiller_development_2016} show, that social media users choose a certain site depending on the researched topic. FOCUS Online is targeted for articles from politics and business, sports news is more likely to be shared from Bild.de. 

While these static resorts give an indication on the content of an article, multiple articles in the same resort probably don't cover the same topics (and are not equally shared). Especially if the articles originate from different news portals. Furthermore, articles can contain more than one topic. To take these factors into account, we use a mixed-membership model \citep{airoldi_handbook_2014} - or topic model as they are often referred in the literature of text analysis \citep{blei_probabilistic_2012} to reveal the underlying topics of a collection of articles (a corpus), and how the articles exhibit them. We then estimate the effect of topic prevalence on the number of Facebook shares.

Within topic models the Latent Dirichlet Allocation (LDA henceforth) is a widely used topic modeling technique, where each document (article) is viewed as a mixture of topics (represented by the document-topic distribution) and each topic is a mixture of unique terms (represented by the topic-term distribution). To "learn" the topic prevalence and the topic-term distribution, collapsed Gibbs sampling\footnote{See Section \ref{section_gibbs} for a non-formal description of the Gibbs sampler} can be used. LDA makes a statistical assumption that all texts in the modeled corpus are generated by the same underlying process \citep{blei_latent_2003}. Thus, it is not ideally suited to examining differences in topical content that are affected by covariates such as author identity or time of writing.

\citet{roberts_model_2016} develop a structural topic model (STM) that allows to incorporate external variables that effect both topical content and topical prevalence. We use this approach to analyze online news about the german federal elections, where we allow prevalence of topics to vary across newswire services. We then estimate the effect of topical prevalence (the posterior document-topic distribution) on the number of Facebook shares. 


\section{Related Literature}

% Quantitative approaches
% -----------------------


% Topic Modeling / LDA
% ---------------------------

Topic modeling is a statistical and computational technique for discerning information about the contents of a large corpus of documents without reading or annotating the original texts. A topic model uncovers patterns of word co-occurrence across the corpus, yielding a set of word clusters, together with associated probabilities of occurrence, which constitute the topics.

See \citep{taddy_estimation_2012} for a review of topic estimation techniques)

Since its introduction into text analysis, topic modeling has become hugely popular.8 (See \citet{blei_probabilistic_2012} for an overview.) The model has been especially useful in political science (e.g., \citep{grimmer_bayesian_2010}), where researchers have been successful in attaching political issues and beliefs to the estimated latent topics.

Topic modeling is alternatively labeled as “latent Dirichlet allocation,” (LDA) which refers to the Bayesian model in \citet{blei_latent_2003} that treats each $\boldsymbol{v}_i$ and $\boldsymbol{\theta}_l$ as generated from a Dirichlet - distributed prior.
The same model was independently introduced in genetics by \citet{pritchard_inference_2000} for factorizing gene expression as a function of latent populations; it has been similarly successful in that field. 

The basic topic model has been generalized and extended in variety of ways. A prominent example is the dynamic topic model of \citet{blei_dynamic_2006}, which considers documents that are indexed by date (e.g., publication date for academic articles) and allows the topics, say $\boldsymbol{\Theta}_t$, to evolve smoothly in time. 

% Structural topic models (STM)
% ----------------------------
A typical application of topic modeling in the social sciences first estimates LDA, then uses estimates of $\theta_d$ as the dependent variable in an regression on covariates to test whether different types of documents have different content. 

This is contradictory because documents are assumed to be generated by a statistical process that we subsequently reject.
The structural topic model (STM) of Roberts et. al. (2016) explicitly introduces covariates into a topic model, and allows one to estimate the impact of document-level covariates on topic content and prevalence as part of the topic model itself.

% ---------------------------------
% Statistical Analysis of Text Data
% ----------------------------------
\section{Statistical Analysis of Text Data}

Consider a collection of documents by $d \in \lbrace 1 ... D \rbrace$, each containing $n \in \lbrace 1 ... N_d \rbrace$ words. Primary observations consist of words $w_{d,n,}$ that are instances of unique terms from a vocabulary of terms, indexed by $v \in \lbrace 1 ... V \rbrace$. 

To use text as data and reduce the dimensionality, a common strategy is to (a) pre-process the text by imposing some preliminary restrictions (stop-word removal, tokenization) based on the nature of the data (twitter text, newspaper articles, speeches, etc.) and (b) to represent a document $d$ as a vector of word counts, $\boldsymbol{n}_d \in \boldsymbol{N}^V$. This representation is often referred to as the bag of words model, since the order in which words are used within a document is completely disregarded. Nowadays, the bag of words model is a common representation for most of statistic literature about text data analysis (\citet{blei_latent_2003}; \citet{erosheva_mixed-membership_2004}; \citet{griffiths_finding_2004}; \citet{genkin_large-scale_2007}).

Term-Document matrices represent frequency distribution of unique terms in the documents. Any one document will contain only a subset of all unique terms, and the rows corresponding to unused terms will all be zero.The key task then becomes how to extract low-dimensional information from documents that are high-dimensional by nature. This is analogous to a situation in which a researcher has a database with thousands of covariates and is attempting to choose which subset of them, or which summary statistics, should be included in regression analysis.
% The description of the data and how it was processed in order to reduce it to a manageable scale without losing significant information can be found in chapter ... 

\begin{enumerate}
	\item Represent raw text $D$ as a numerical array $\boldsymbol{C}$. 
	 
	\item Map $\boldsymbol{C}$ to predict values $\boldsymbol{\hat{V}}$ of unknown outcomes $\boldsymbol{V}$. 
	
	E.g. the variable of interest $\boldsymbol{V}$ is an indicator whether the email is spam. The prediction $\boldsymbol{\hat{V}}$ determines whether or not to send the email to a spam filter. Sometimes the attribute of interest is latent, such as the topics of a newspaper article.
	\item Use $\boldsymbol{\hat{V}}$ in subsequent descriptive or causal analysis.
\end{enumerate}


Regarding 2, the methods to connect counts $\boldsymbol{c}_i$ to attributes $\boldsymbol{v}_i$ can be roughly divided into four categories \citep{gentzkow_text_2017}:
\begin{enumerate}
	\item Dictionary-based methods: 
	No statistical inference. Simply specify $\boldsymbol{\hat{v}_i}=f(\boldsymbol{c}_i)$ for some unknown function $f(\cdot)$. Sometimes based on a specific dictionary of terms (\citet{tetlock_giving_2007}, \citet{baker_measuring_2015}).
	\item Text regression methods:
	Directly estimate the conditional outcome distribution $p(\boldsymbol{v}_i|\boldsymbol{c}_i)$. Intuition: If we want to predict $\boldsymbol{v_i}$ from $\boldsymbol{c}_i$, we would regress the observed values of the former ($\boldsymbol{V}^{train}$) on the corresponding latter ($\boldsymbol{C}^{train}$). High dimensionality of $\boldsymbol{c}_i$ ($p > n^{train}$) requires use of appropriate regression techniques to avoid overfitting (e.g. $L_1$ regularized linear or logistic regression)
	\item Generative model of $p(\boldsymbol{c}_i|\boldsymbol{v}_i)$.
	Intuition: In many cases the underlying causal relationship runs from outcomes to language rather than the other way around. E.g. Google searches about flu do not cause flu cases to occur, rather, people with flu are more likely to produce such searches.
	\begin{enumerate}
		\item Observed attributes (supervised methods):
		Supervised machine learning starts with a researcher classifying observations to ‘train’ an algorithm under human ‘supervision’ – to ‘learn’ the correlation between the researcher’s ascribed classes and words characteristic of documents in those classes (Grimmer and Stewart (2013)). Fitting the model based on the observed training data $\boldsymbol{V}^{train}$, say $f_{\boldsymbol{\theta}}(\boldsymbol{c}_i;\boldsymbol{v}_i)$ for a vector of parameters $\boldsymbol{\theta}$, to this training set. The fitted model $f_{\hat{\boldsymbol{\theta}}}$ can be inverted in order to infer $\boldsymbol{v}_i$ for documents in the test set.
		\item Latent attributes (unsupervised methods): The function relating $\boldsymbol{c}_i$ to $\boldsymbol{v}_i$ is unknown, as we cannot observe the true value of $v_i$. Principal component analysis (PCA), latent Dirichlet allocation (LDA, topic modeling, structural-topic modeling). Unsupervised machine learning involves taking unclassified observations and uncovering hidden patterns that structure them in some meaningful way. The outputs of algorithms for unsupervised machine learning can be used as inputs into econometric models for predicting some variable of interest, but this is a different approach from intentionally choosing the dimensions of content based on their predictive ability.
	\end{enumerate}
	\item Deep learning techniques: neural networks, distributed language models.
\end{enumerate}

The goal of this paper is to find the latent topics within newspaper articles and how different types of media outlets (as well as the date?) influence the topic prevalence as well as the language to describe a topic (the word-topic distribution). We implement generative model (topic model). 
% A description of the generative process can be found in chapter ...


% -----
% Data
% -----

\section{Data}

To explore the effect of topic prevalence of an article on the times this article is shared on Facebook, we analyze a sample of 1768 news articles containing the term "Bundestagswahl" (Federal Election) dated from 01.06.2017 to 30.09.2017\footnote{German federal elections took place on 24th of September 2017.} and originated from f

% ---------------------------------
% Generative Process
% ----------------------------------

\section{Generative Process}

In this unsupervised method, the words in a document are viewed as the realization of some stochastic process. The generative process is defined through a probability model for $p(\boldsymbol{c}_i|\boldsymbol{v}_i)$.

Each observation $\boldsymbol{c}_i$ is a conditionally independent draw from the vocabulary of possible tokens accodring to some document-specific token probability vector $\boldsymbol{q}_i=[q_{i1}...q_{ip}]'$. Conditioning on document length, $m_i=\sum_jc_{ij}$, this implies a multinomial distribution for the counts

\begin{equation}\label{eq_1}
	\boldsymbol{c}_i \sim \boldsymbol{MN}(\boldsymbol{q}_i,m_i). 
\end{equation}

Under the basic model in \ref{eq_1}, a connection between text and attributes is defined through the link function $\boldsymbol{q}_i=q(\boldsymbol{v}_i)$. 

\begin{equation}\label{eq_2}
	\boldsymbol{E} \bigg[\frac{\boldsymbol{c}_i}{m_i}\bigg]=\boldsymbol{q}_i =v_{i1}\boldsymbol{\theta}_1+v_{i2}\boldsymbol{\theta}_2+v_{ik}\boldsymbol{\theta}_k=\boldsymbol{\Theta v}_i
\end{equation}

where attributes $v_{il}$ are referred as topic wights, restricting $v_{il}\geq 0$ and $\sum^k_{l=1}v_{il}=1$, and each topic $\boldsymbol{\theta}_l$ is a probability vector over possible tokens: $\theta_{lj}\geq 0$ and $\sum^p_{j=1}\theta_{il}=1$. Each $\boldsymbol{v}_i$ and $\boldsymbol{\theta}_l$ is generated from a Dirichlet-distributed prior.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. Formally speaking, each document has its own probability distribution over topics. Then, for each word in each document, a topic assignment is made and then, conditional on the assignment, a word from the corresponding topic. 

Estimation of topic models make use of some alternating inference for $\boldsymbol{V|\Theta}$ and $\boldsymbol{\Theta|V}$.

\begin{enumerate}
	\item Expectation-maximization algorithm (EM)
	Either maximize the likelihood implied by \ref{eq_1} and \ref{eq_2} or, after incorporating the usual Dirichlet priors on $\boldsymbol{v}_i$ and $\boldsymbol{\theta}_l$ \citep{taddy_estimation_2012} 
	\item Target full posterior distribution $p(\boldsymbol{\Theta,V|c}_i})$
\end{enumerate}

Choice of number of topics $k$ is often fairly arbitrary. In practice it is very common to simply start with a number of topics on the order of ten, and then adjust the number of topics in whatever direction seems to improve interpretability. Whether this ad hoc procedure is problematic depends on the application. In many applications of topic models to date the goal is to provide an intuitive description of text rather than inference on some underlying “true” parameters; in these cases, the ad hoc selection of the number of topics may be reasonable.

Data-driven approaches: 
\begin{enumerate}
	\item \citet{taddy_estimation_2012} describes a model selection process for $k$ that is based upon Bayes factors.
	\item \citet{airoldi_reconceptualizing_2010} provide a cross-validation (CV) scheme
	\item \citet{teh_hierarchical_2006} use Bayesian nonparametric techniques that view $k$ as an unknown model parameter.
\end{enumerate}


\subsection{Structural Topic Model}

The process for generating individual words is the same as for plain LDA conditional on the $\beta_k$ and $\pi_d$ terms. 

However both objects can depend on potentially different sets of document-level covariates. Each document has:
\begin{enumerate}
	\item Topic Prevalence. Attributes $r_d$ that affect the likelihood of discussing topic $k$. how much of a document is associated with a topic
	\item Topic Content. Attributes $r_d$ that affect the likelihood of discussing term $v$ overall, and of discussing it within topic $k$. the words used within a topic
\end{enumerate}The generation of the $k$ and $d$ terms is via multinomial logistic regression, which breaks local conjugacy.

The standard topic modeling technique, Latent Dirichlet Allocation (LDA), may have limited utility in the realm of social media. LDA makes a statistical assumption that all texts in the modeled corpus are generated by the same underlying process (Blei). Thus, it is not ideally suited to examining differences in topical content that are affected by external variables such as author identity or time of writing.

Structural topic modeling (STM) is a recently introduced variant of LDA that is designed to address precisely this limitation . STM can represent the effect of external variables on both topical content and topical prevalence. The external variables can consist of any metadata that distinguishes one text from another, including variables relating to author identity (gender, age, political affiliation, etc.), textual genre (for example, news stories versus academic articles), and time of production.

stmVigenette: "The goal of the Structural Topic Model is to allow researchers to discover topics and estimate their relationship to document metadata. Outputs of the model can be used to conduct hypothesis testing about these relationships."  

\subsubsection{Estimation of the STM}
In STM, metadata can be entered in the topic model in two ways: topical prevalence and topical content. Metadata covariates for topical prevalence allow the observed metadata to affect the frequency with which a topic is discussed. Covariates in topical content allow the observed metadata to affect the word rate use within a given topic{that is, how a particular topic is discussed.

We use the online magazine-type as a covariate in the topic prevalence portion of the model with the  data described above. Each document is modeled as a mixture of multiple topics. Topical prevalence captures how much each topic contributes to a document. Because different documents come from different sources, it is natural then to want to allow this prevalence to vary with metadata that we have about document sources.

We will simply let prevalence be a function of the magazine variable, which is coded as eitherSpiegel Online or FOCUS Online and the variable day which is an integer measure of days running from 01-01-2017 to 31-07-2017.

\subsection{Gibbs Sampler}\label{section_gibbs}

Strategy for discovering topics  \cite{griffiths_finding_2004}
\begin{itemize}
	\item Considering the posterior distribution over the assignments of words to topics, $P(z|w)$. (and not explicitly representing $\phi$ or $\theta$ as parameters to be estimated)
	\item Examine this posterior distribution to obtain $\phi$ or $\theta$
\end{itemize}


After making assumptions about the parameters (number of topics $K$, prior distributions $\alpha$ and $\beta$), the procedure of Gibbs sampling is as follows:

\begin{enumerate}
	\item Go through each document and randomly assign each word in the document to one of $K$ topics, based on the prior distributions.
	\item For each document $d$, go through each word $w$ and compute:
	\begin{enumerate}
		\item The document-topic distribution $\theta = p(t|d)$
		\item The topic-term distribution $\phi = p(w|t)$
	\end{enumerate}
	\item Reassign word $w$ a new topic $t^*$, where we choose topic $t^*$ with probability $p(t^*|d)*p(w|t^*)$
\end{enumerate}

On repeating the last step a large number of times, the algorithm reaches a steady state where topic assignments are pretty good. These posterior distributions $\theta$ and $\phi$ are then used to determine the topic mixtures of each document.

\subsection{Validate accuracy}

\begin{itemize}
	\item Manual audits: cross checking some subset of the fitted values against the coding a human would produce by hand
	\item Inspection of fitted parameters
	\item Interpretation of fitted topics usually proceeds by ranking the tokens in each topic according to token probability.
\end{itemize}

Caution against the over-interpretation of unsupervised models: posterior distributions informing parameter estimates are highly multimodal, and multiple topic model runs can lead to multiple different interpretations. Add some supervision (\citet{airoldi_improving_2016}, \citet{gentzkow_text_2017})  

\end{document}
