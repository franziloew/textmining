---
title: "Topic Modeling of News"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm",
          "plyr","dplyr","class","knitr","kableExtra","cldr","data.table",
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales")
lapply(libs, library, character.only = TRUE)
```

# Load and prepare Dataframe

```{r, message=FALSE, include=FALSE}
opts_knit$set(root.dir = "/Users/Franzi/CloudStation/textmining")
rm(list=ls())

theme_set(new = theme_bw())
set.seed(95616)

# set color
library(grDevices)
col <- c("#FF61CC","#C77CFF","#00A9FF","#00BFC4","#00BE67","#7CAE00","#CD9600","#F8766D")

# load the dataframe
news <- read_feather('/Users/Franzi/CloudStation/textmining/data/news_sep2')

# Format the time Variable
news$date <- as.POSIXct(news$date, format ="%Y-%m-%d")
```

## Subset Data

```{r}
# keep only german articles
news <- news[which(news$lang == "deu"),]

# Delete Articel "Die Welt"
news %>%
  filter(source != "DIE WELT") -> news

news %>%
  arrange(date) -> news
```

### Number of published Articels by News Source 

```{r}
news %>%
  group_by(source) %>%
  tally(sort = TRUE)%>%
  ggplot(aes(source, n)) +
  geom_col(fill = col, alpha = 0.7)
```

### Number of published Articels by News Source and Date

```{r}
news %>%
  group_by(date, source) %>%
  tally(sort = TRUE) -> news.count

ggplot(news.count, aes(x = date, y = n,  group = source)) + 
  geom_line(aes(color = source)) +
  xlab('Publish Date') +
  ylab("Number of Articels") 

```

# Pre-Process Text

```{r, message=FALSE, warning=FALSE, include=FALSE}
clean.text = function(x)
{
  x = tolower(x)
  x = gsub("[[:punct:]]", " ", x)  # replace punctuation with space
  x = gsub("[[:cntrl:]]", " ", x)  # replace control characters with space
  x = gsub("[[:digit:]]", "", x)  # remove numbers
  x = gsub("ὣ", " ", x)
  x = gsub("\n{1,}Anzeige\n{1,}", " ", x)
  x = gsub("^[[:space:]]+", "", x) # remove whitespace at beginning of documents
  x = gsub("[[:space:]]+$", "", x) # remove whitespace at end of documents 
  x = gsub("[\n{2,}]", "", x)
  return(x)
}

# apply function to the "orig" dataframe
news$text_cleaned <- clean.text(news$text)

# get stopwords from online dict and add costum stopwords
#exceptions   <- c("nicht")
#my_stopwords <- setdiff(stopwords("german"), exceptions)
my_stopwords <- c(stopwords("german"), 'stern','spiegel','focus','n-tv',"spon","zeitmagazin","sz", "seitennavigation","startseite","gibt","immer","uhr","dass","mehr","sagte","sei","wurde","twitterfacebookgoogle","bildplus","whatsappteilen","kartengeschichte","elbvertiefung","econa","dpa","prozent","freepik","colourbox")

# Remove stopwords
news$text_cleaned<- removeWords(news$text_cleaned, my_stopwords)

# save file
write.csv(news, file = "/Users/Franzi/CloudStation/textmining/data/news_cleaned.csv")
```

## Tokenize terms

```{r}
news.token <- news %>%
  group_by(source) %>%
  unnest_tokens(word, text_cleaned) %>%
  count(source, word, sort = TRUE)  %>%
  bind_tf_idf(word, source, n) %>%
  arrange(desc(tf_idf)) 
```

# Topic Modeling
## LDA
```{r}
# Build corpus
btw.lda <- Corpus(VectorSource(btw$text_cleaned))
# Create document-term matrix
dtm <- DocumentTermMatrix(btw.lda)
```

```{r}
# Run LDA using Gibbs sampling
library(topicmodels)

k = 20 # Topics

t1 <- Sys.time()
lda20 <-LDA(dtm, 20, method="Gibbs", control=list(nstart=5, iter = 10000))
t2 <- Sys.time()
t2 - t1
```

## Structural Topic Model

```{r}
btw <- btw[,c("date","source","text_cleaned")]

# Process data
processed <- textProcessor(btw$text_cleaned, metadata = btw,
                           wordLengths = c(2,Inf),
                           lowercase = F,
                           removestopwords = F,
                           removenumbers = F,
                           removepunctuation = F,
                           stem = F)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
```

### Estimation
```{r, echo=FALSE, message=FALSE, warning=FALSE}
t1 <- Sys.time()
stm <- stm(documents = out$documents, vocab = out$vocab, K=20, prevalence = ~source+s(date), data=out$meta, init.type = "Spectral")
t2 <- Sys.time()
t2 - t1

#save(stm, file = "stm_k40.RDa")
```

### Interpret the results

#### Wordclouds by topic
```{r}
cloud(stm, topic =8, scale = c(5,.25))
```

```{r}
# get document - topic distribution 
theta <- as.data.frame(stm$theta)
theta$source <- out$meta$source
theta$date <- out$meta$date
```

#### get the mean distribution of topics  ...
##### ... by date
```{r}
theta %>% 
  select(- source) %>%
  group_by(date) %>% 
  summarise_all(funs(mean)) -> date_mean

# plot 
melt1 = melt(date_mean, id = "date")

ggplot(melt1, aes(x=date, y = variable)) +   
  geom_raster(aes(fill=value)) + 
  scale_fill_gradient(low = "light blue", high = "dark blue") +
  labs(x="Date", y="Topics", title="Average Topic Weights") + 
  theme_bw() 

```

##### ... by source
```{r, message=FALSE, warning=TRUE}
theta %>% 
  select(- date) %>%
  group_by(source) %>% 
  summarise_all(funs(mean)) -> source_mean

# plot 
melt2 = melt(source_mean, id = "source")

ggplot(melt2, aes(x=source, y = variable)) +   
  geom_raster(aes(fill=value)) + 
  scale_fill_gradient(low = "light blue", high = "dark blue") +
  labs(x="Source", y="Topics", title="Average Topic Weights") + 
  theme_bw() 

```

### compute cosine similarity

```{r, message=FALSE}
library(proxy)
cosine_dist <- as.matrix(dist(as.matrix(source_mean[,1:20]), method = "cosine"))
colnames(cosine_dist) <- unique(out$meta$source)
rownames(cosine_dist) <- unique(out$meta$source)
```

### Correlogram
```{r, message=FALSE}
devtools::install_github("sinhrks/ggfortify")
library("ggfortify")
devtools::install_github("kassambara/ggcorrplot")
library(ggcorrplot)
library("ggthemes")

ggcorrplot(cosine_dist, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           #colors = c("tomato2", "white", "springgreen3"), 
           title="", 
           ggtheme=theme_bw)
```

### correlations between topics
```{r}
mod.out.corr <- topicCorr(stm)
plot(mod.out.corr)
```

### stmBrowser
```{r}
library(stmBrowser)
setwd(tempdir())
stmBrowser(stm, data=out$meta, c("source","date"), text="text_cleaned")

```

