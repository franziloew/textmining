---
title: "Topic Modeling of News"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm","RColorBrewer",
          "plyr","dplyr","class","knitr","kableExtra","cldr","data.table",
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales","rjson")
lapply(libs, library, character.only = TRUE)

```

# Load and prepare Dataframes
```{r}
rm(list =ls())

load("../output/news_cleaned.Rda")
df2 <- read_csv("../data/eventbride/eventbride_okt_18.csv")
```

## Generate new variables 
```{r}
## Extract site
df2$site <- str_extract(df2$source, "(?<='uri': ')[A-z][^']*")

# number of Facebook shares
df2$fb_shares <- str_extract(df2$shares, "(?<='facebook': )[0-9]*")
df2$fb_shares <- ifelse(is.na(df2$fb_shares),0, df2$fb_shares)
df2$fb_shares <- as.integer(df2$fb_shares)

# Text Length
names(df2) <- gsub("body", "text", names(df2))
df2$text_length <- sapply(gregexpr("\\S+", df2$text), length)
```

## Reduce Dataframe
```{r}
keeps <- c( "welt.de","spiegel.de","stern.de","focus.de","zeit.de","bild.de" )

df %>%
  filter(site %in% keeps) %>%
  filter(grepl("bundestagswahl", text, ignore.case = TRUE)) -> btw1

df2 %>%
  filter(site %in% keeps) %>%
  filter(grepl("bundestagswahl", text, ignore.case = TRUE)) -> btw2
```

## Combine Dataframes
```{r}
common_cols <- intersect(colnames(btw1), colnames(btw2))

btw <- rbind(
  btw1[, common_cols], 
  btw2[, common_cols]
)
```

```{r}
btw$title.text <- paste(btw$title, btw$text, sep = " ")
```

# Pre-Process Text

## Filtering
```{r}
btw %>%
  # Keep unique observations
  distinct(title, site, .keep_all = TRUE) %>%
  # Remove Articles after 10.10.2017
  filter(date < as.Date("2017-10-10")) -> btw
```

```{r}
pat <- 'Aus unserem Netzwerk[^"]*'
btw %>%
  filter(grepl(pat, text, perl = TRUE)) %>%
  group_by(site) %>%
  tally(sort = TRUE)
```

```{r}
as.data.frame(str_match(btw$text, pat)) ->test
test %>%
  filter(!is.na(test)) %>%
  .[1]
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
clean.text <- function(x)
  {
  # All
  x = gsub("Getty Images", "", x)

  # Bild.de
  x = gsub("Shopübersicht Top Gutscheine", "", x)

  # Handelsblatt.com
  x = gsub("RSS Feed", "", x)
  x = gsub("Deutsche Presse Agentur", "", x)

  # FOCUS.de
  x = gsub("FOCUS Online/Wochit", "", x)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub('Im Interesse unserer User[^"]*', "", x, perl = TRUE)
  x = gsub('Sie haben noch 800[^"]*', "", x, perl = TRUE)
  x = gsub('Erzählen Sie auf FOCUS Online über Ihren Heimatort Teilen Sie Ihren Artikel und Ihr Foto', "", x, perl = TRUE)
  x = gsub("Bericht schreiben", "", x)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub("Hier können Sie selbst Artikel verfassen:","", x)
  x = gsub("Live-Ticker", "", x)
  x = gsub('Aus unserem Netzwerk[^"]*', "", x, perl = TRUE)

  # Spiegel.de
  x = gsub("7 mal 17", "", x)
  x = gsub("Zur Startseite Diesen Artikel... Drucken Feedback Nutzungsrechte", "", x)
  x = gsub('Liebe Leserin, lieber Leser,\num diesen[^"]*', "", x)
  x = gsub('Liebe Leserin, lieber Leser, um diesen[^"]*', "", x)
  x = gsub('ejf[^"]*', "", x)
  x = gsub('tjf[^"]*', "", x)

  # Zeit.de
  x = gsub("Inhalt Seite", "", x)
  
  # Tagesschau
  x = gsub('Hinweis.+?(?=(einfügen))', "", x, perl = TRUE)
  
  return(x)
}

# apply function to dataframe
btw$text_cleaned <- clean.text(btw$title.text)
```

```{r}
btw$text_cleaned <- gsub("[[:punct:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:cntrl:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:digit:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("^[[:space:]]+", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:space:]]+$", " ", btw$text_cleaned)
btw$text_cleaned <- tolower(btw$text_cleaned)
```

## Remove Stopwords
```{r}
btw$text_cleaned<- removeWords(btw$text_cleaned, stopwords("german"))
```

### Remove customized stopwords
```{r}
mystopwords <- c("focus","online","spiegel", "stern", "handelsblatt", "de", "bild","bildplus", "zeit", "ersten", "tagesschau", "wdr", "ndr", "ich","sie","passwort","kommentar","wurde","ihr","der","im","artikel","mehr","ihren","foto","e", "uhr","videolänge","dass","mindestens","das","mail","die","schon","neuer abschnitt", "login", "loggen", "inaktiv", "nwmi", "nwnoa", "polizei","beim","dpa","video","quelle","afp","witters","fotogalerie", "registrierter","als","spiegel","vielen","in","es","bitte","dank","unserer","nutzer","sei","beitrag","user","seit","zeichen", "datenschutzerklärung","premium","nutzungsbedingungen","nutzungsrechte","pflichtfelder","registrierung","anzeige","großbuchstaben","sonderzeichen","html","seitennavigation","fullscreen","statista","club","sagte","borenda","spreepicture","shopübersicht")

mystopwords <- distinct(as.data.frame(mystopwords))

btw$text_cleaned<- removeWords(btw$text_cleaned, mystopwords$mystopwords)
```

## Stemming
```{r}
stem_text<- function(text, language = "porter", mc.cores = 1) {
  # stem each word in a block of text
  stem_string <- function(str, language) {
    str <- strsplit(x = str, split = "\\s")
    str <- wordStem(unlist(str), language = language)
    str <- paste(str, collapse = " ")
    return(str)
  }
   
  # stem each text block in turn
  x <- mclapply(X = text, FUN = stem_string, language, mc.cores = mc.cores)
   
  # return stemed text blocks
  return(unlist(x))
}

btw$text_cleaned <- stem_text(btw$text_cleaned)
```


```{r}
save(btw, file="../output/btw_combined.Rda")
```

