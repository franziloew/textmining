---
title: "Topic Modeling of News"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm","RColorBrewer",
          "plyr","dplyr","class","knitr","data.table","parallel","SnowballC",
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales","rjson")
lapply(libs, library, character.only = TRUE)
```

# Load and prepare Dataframes
```{r message=FALSE}
rm(list =ls())

load("../output/news_cleaned.Rda")
okt <- read_csv("../data/eventbride/eventbride_okt_18.csv")
nov_03 <- read_csv("../data/eventbride/eventbride_nov_03.csv")
nov_22 <- read_csv("../data/eventbride/eventbride_nov_22.csv")
```

## combine dataframes
```{r}
btw1 <- bind_rows(list(okt,nov_03,nov_22)) %>%
  mutate(text = body,
         # Extract site 
         site = str_extract(source, "(?<='uri': ')[A-z][^']*")) %>%
  # Delete duplicates
  #filter(isDuplicate=="False") %>%
  distinct(text, site, .keep_all = TRUE) %>%
  select(date,title,text,site,url) 

btw1 <- bind_rows(df %>%
                   mutate(date = as.Date(date)) %>%
                   select(date,title,text,site,url),
                 btw1)
```

# Pre-Process Text
## select news-outlets
```{r}
keeps <- c("welt.de","spiegel.de","stern.de","focus.de","zeit.de","bild.de","tagesschau.de")

btw1 %>%
  mutate(title_text = paste(title, text, sep=" ")) %>%
  filter(site %in% keeps) -> btw
```

## select politic section
```{r}
pat1 <- "\\w\\.de/politik/deutschland"
pat2 <- "\\w\\.de/politik/inland"
pat3 <- "\\w\\.de/politik/wahl"
pat4 <- "\\w\\.de/politik/bundestagswahl"
pat5 <- "tagesschau\\.de/inland"

btw %>%
  filter(grepl(paste(pat1,pat2,pat3,pat4,pat5,
                     sep="|"), url, perl = TRUE)) -> btw
```

## count number of terms in a string
```{r}
btw$text_length <- sapply(gregexpr("\\S+", btw$text), length)
```

## ... and filter short text & other
```{r}
btw %>%
  filter(text_length > 200) %>%
  # remove articles that contain daily overviews
  filter(!grepl("News des Tages", title_text)) %>%
  filter(!startsWith(title,"News")) %>%
  # remove articles that only contain video 
  filter(!grepl("Video einbetten Nutzungsbedingungen Embedding Tagesschau", title_text)) %>%
  # remove english text
  filter(!grepl("bild-international", url)) %>%
  filter(!grepl("german",url)) %>%
  # remove text that mostly contain user comments
  filter(!grepl("kommentar", title, ignore.case = TRUE)) %>%
  filter(!grepl("ticker", title, ignore.case = TRUE)) %>%
  filter(!grepl("stream", title, ignore.case = TRUE)) %>%
  filter(!startsWith(text,"1.")) -> btw
```

## Clean text

The following two chuncks of code are just to check right regex definitions to use it inthe clean.text function.
```{r}
pat <- "Dieser Artikel wurde ausgedruckt unter der Adresse: www.tagesschau.de/inland/[^.]*"
btw %>%
  filter(grepl(pat, title_text, perl = TRUE, ignore.case = TRUE)) %>%
  #filter(site=="tagesschau.de") %>%
  #select(url, title_text) %>% 
  #.[1,] %>%
  #htmlTable::htmlTable()
  group_by(site) %>%
  tally(sort = TRUE)
```

```{r}
as.data.frame(str_match(btw$title_text, pat)) ->test
test %>%
  filter(!is.na(test)) %>%
  .[1:100,]
```

### define function  
```{r, echo=FALSE, message=FALSE, warning=FALSE}
clean.text <- function(x)
  {
  # All
  x = gsub("Getty Images", "", x)
  x = gsub('Startseite[^\n]*', "", x, ignore.case = TRUE, perl = TRUE)
  x = gsub("deutsche presse agentur","", x, ignore.case = TRUE, perl = TRUE)

  # Bild.de
  x = gsub("Shopübersicht Top Gutscheine", "", x)
  x = gsub('Politik Inland[^\n]*', "", x, perl = TRUE)
  
  # welt.de
  x = gsub('Quelle: N24[^\n]*', "", x, perl = TRUE)
  x = gsub('infocom[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Infografik[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('versuchen Sie es[^.]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Video konnte nicht[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Welt twitter', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('\\w{1,} für Abonnenten', "", x, perl = TRUE, ignore.case = TRUE)

  # FOCUS.de
  x = gsub("FOCUS Online", "", x)
  x = gsub('Wochit[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub('Im Interesse unserer User[^"]*', "", x, perl = TRUE)
  x = gsub('Sie haben noch 800[^"]*', "", x, perl = TRUE)
  x = gsub('Erzählen Sie auf FOCUS Online über Ihren Heimatort Teilen Sie Ihren Artikel und Ihr Foto', "", x, perl = TRUE)
  x = gsub("Bericht schreiben", "", x)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub("Hier können Sie selbst Artikel verfassen:","", x)
  x = gsub("Live-Ticker", "", x)
  x = gsub('Aus unserem Netzwerk[^"]*', "", x, perl = TRUE)

  # Spiegel.de
  x = gsub("7 mal 17", "", x)
  x = gsub("Zur Startseite Diesen Artikel... Drucken Feedback Nutzungsrechte", "", x)
  x = gsub('Liebe Leserin, lieber Leser,\num diesen[^"]*', "", x)
  x = gsub('Liebe Leserin, lieber Leser, um diesen[^"]*', "", x)
  x = gsub('ejf[^"]*', "", x)
  x = gsub('tjf[^"]*', "", x)
  x = gsub('Fotostrecke[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Florian Gathmann[^\n]*', "", x, perl = TRUE)

  # Zeit.de
  x = gsub("Inhalt Seite", "", x)
  x = gsub('Seitennavig[^"]*',"", x, perl=TRUE)
  x = gsub('Kartengeschichte[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  
  # Stern.de
  x = gsub('Fullscreen[^\n]*', "", x, perl = TRUE)
  
  # Tagesschau.de
  x = gsub("Hinweis: Falls die [^\\.]*", "", x, perl=TRUE)
  x = gsub("auswählen", "", x, perl = TRUE)
  x = gsub("Dieser Artikel wurde ausgedruckt unter der Adresse: www.tagesschau.de/inland/[^.]*", "", x, perl = TRUE)
  
  return(x)
}

# apply function to dataframe
btw$text_cleaned <- clean.text(btw$title_text)
```

```{r}
btw$text_cleaned <- gsub("[[:punct:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:cntrl:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:digit:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("^[[:space:]]+", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:space:]]+$", " ", btw$text_cleaned)
btw$text_cleaned <- tolower(btw$text_cleaned)
```

## Remove Stopwords
```{r}
btw$text_cleaned<- removeWords(btw$text_cleaned, stopwords("german"))
```

### Remove customized stopwords
```{r}
mystopwords <- c("focus","online","spiegel", "stern", "de", "bild","bildplus", "zeit", "ersten", "tagesschau", "zeitmagazin","seitenanfang","ja","mal","heute","ich","sie","passwort","kommentar","wurde","ihr","der","im","artikel","mehr","ihren","foto","e","seien","comment","ticker","live","laif","uhr","videolänge","dass","mindestens","das","mail","die","schon","neuer abschnitt", "login", "loggen", "inaktiv","nwmi","wäre","viele","nwnoa","morgenkolumne","beim","dpa","video","quelle","afp","witters","fotogalerie","wurden","worden","wegen","sagt","immer","gibt","geht","spon","registrierter","als","spiegel","vielen","in","es","bitte","dank","unserer","nutzer","sei","beitrag","user","seit","zeichen","tba","datenschutzerklärung","premium","nutzungsbedingungen","nutzungsrechte","pflichtfelder","registrierung","anzeige","großbuchstaben","sonderzeichen","html","seitennavigation","fullscreen","statista","club","sagte","borenda","spreepicture","shopübersicht")

mystopwords <- distinct(as.data.frame(mystopwords))

btw$text_cleaned<- removeWords(btw$text_cleaned, mystopwords$mystopwords)
```

### check words with high tf-idf 
```{r}
token <- btw %>%
  group_by(site) %>%
  unnest_tokens(word, text_cleaned) %>%
  count(site, word, sort = TRUE)  %>%
  bind_tf_idf(word, site, n) %>%
  arrange(desc(tf_idf))

token %>%
  arrange(desc(tf)) %>%
  arrange(site) %>%
  top_n(10)
```

### Bigrams
```{r, include=FALSE}
bigrams <- btw %>%
  unnest_tokens(bigram, text_cleaned, token="ngrams", n=2)

bigrams %>%
  group_by(site) %>%
  count(bigram) %>%
  arrange(desc(n)) %>%
  top_n(10)
```

## Stemming (not applied)
```{r }
stem_text<- function(text, language = "porter", mc.cores = 1) {
  # stem each word in a block of text
  stem_string <- function(str, language) {
    str <- strsplit(x = str, split = "\\s")
    str <- wordStem(unlist(str), language = language)
    str <- paste(str, collapse = " ")
    return(str)
  }
   
  # stem each text block in turn
  x <- mclapply(X = text, FUN = stem_string, language, mc.cores = mc.cores)
   
  # return stemed text blocks
  return(unlist(x))
}

btw$text_cleaned <- stem_text(btw$text_cleaned)
```

# Add new variables
## FB Shares
```{r}
endpoint <- "https://api.sharedcount.com/?url="
api1 <- "&apikey=24abe04472f42e5003938b0314844010d6425d37"
api2 <- "&apikey=68d76247a02b3d39ff592aa5616ce9f73f182934"

pattern <- 'share_count\\":[0-9]+'
shares <- NULL

for (i in (length(shares)+1):nrow(btw)){
  page <- readLines(paste0(endpoint,btw$url[i],api1))
  string <- str_extract(page, pattern)
  shares <- append(shares, as.numeric(str_extract(string, "[0-9]+")))
}
```

```{r}
btw <- cbind(btw, shares)

btw <- btw %>% mutate(fb_shares = shares) %>%
  select(-shares)
```

## month & document number
```{r}
btw %>%
  mutate(month = month(date),
         articleID = as.numeric(rownames(btw))) -> btw
```

```{r}
save(btw, file="../output/btw_combined.Rda")
```
