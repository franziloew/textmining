---
title: "Topic Modeling of News"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm","RColorBrewer",
          "plyr","dplyr","class","knitr","data.table","parallel","SnowballC",
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales","rjson")
lapply(libs, library, character.only = TRUE)

```

# Load and prepare Dataframes
```{r}
rm(list =ls())

load("../output/news_cleaned.Rda")
okt <- read_csv("../data/eventbride/eventbride_okt_18.csv")
nov <- read_csv("../data/eventbride/eventbride_nov_03.csv")
```

## Generate new variables 
```{r}
### Oktober ###
# Extract site
okt$site <- str_extract(okt$source, "(?<='uri': ')[A-z][^']*")

# number of Facebook shares
okt$fb_shares <- str_extract(okt$shares, "(?<='facebook': )[0-9]*")
okt$fb_shares <- ifelse(is.na(okt$fb_shares),0, okt$fb_shares)
okt$fb_shares <- as.integer(okt$fb_shares)
names(okt) <- gsub("body", "text", names(okt))

### November ###
# Extract site
nov$site <- str_extract(nov$source, "(?<='uri': ')[A-z][^']*")

# number of Facebook shares
nov$fb_shares <- str_extract(nov$shares, "(?<='facebook': )[0-9]*")
nov$fb_shares <- ifelse(is.na(nov$fb_shares),0, nov$fb_shares)
nov$fb_shares <- as.integer(nov$fb_shares)
names(nov) <- gsub("body", "text", names(nov))
```

## Reduce Dataframe
```{r}
keeps <- c( "welt.de","spiegel.de","stern.de","focus.de","zeit.de","bild.de" )
keys <- paste(c("bundestagswahl","Koalition"), collapse = "|")

df %>%
  filter(site %in% keeps) %>%
  filter(grepl(keys, text, ignore.case = TRUE)) -> btw1

okt %>%
  filter(isDuplicate == "False") %>%
  filter(site %in% keeps) %>%
  filter(grepl(keys, text, ignore.case = TRUE)) -> btw2

nov %>%
  filter(isDuplicate == "False") %>%
  filter(site %in% keeps) %>%
  filter(grepl(keys, text, ignore.case = TRUE)) -> btw3
```


## Combine Dataframes
```{r}
common_cols <- intersect(colnames(btw1), colnames(btw2))

btw <- rbind(
  btw1[, common_cols], 
  btw2[, common_cols],
  btw3[, common_cols]
)

```

## Count number of terms in a string
```{r}
btw$text_length <- sapply(gregexpr("\\S+", btw$text), length)
```

```{r}
btw$title.text <- paste(btw$title, btw$text, sep = " ")
```

# Pre-Process Text

## Filtering
```{r}  
btw %>%
  group_by(title, site) %>%
  ungroup() %>%
  distinct(title,site, .keep_all = TRUE) -> btw
```

```{r}
btw %>%
  # remove articles that contain daily overviews
  filter(!grepl("Elbvertiefung", title.text)) %>%
  filter(!grepl("News des Tages", title.text)) %>%
  # remove articles that dont contain text
  filter(!grepl('</div>', title.text)) -> btw
```

```{r}
pat <- 'Florian Gathmann[^\n]*'
btw %>%
  filter(grepl(pat, title.text, perl = TRUE, ignore.case = TRUE)) %>%
  #select(text) %>%
  #.[1,] %>%
  #htmlTable::htmlTable()
  group_by(site) %>%
  tally(sort = TRUE)
```

```{r}
as.data.frame(str_match(btw$title.text, pat)) ->test
test %>%
  filter(!is.na(test)) %>%
  .[1:100,]
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
clean.text <- function(x)
  {
  # All
  x = gsub("Getty Images", "", x)
  x = gsub('Startseite[^\n]*', "", x, ignore.case = TRUE, perl = TRUE)

  # Bild.de
  x = gsub("Shopübersicht Top Gutscheine", "", x)
  x = gsub('Politik Inland[^\n]*', "", x, perl = TRUE)
  
  # welt.de
  x = gsub('Quelle: N24[^\n]*', "", x, perl = TRUE)
  x = gsub('infocom[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Infografik[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('versuchen Sie es[^.]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Video konnte nicht[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Welt twitter', "", x, perl = TRUE, ignore.case = TRUE)

  # FOCUS.de
  x = gsub("FOCUS Online", "", x)
  x = gsub('Wochit[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub('Im Interesse unserer User[^"]*', "", x, perl = TRUE)
  x = gsub('Sie haben noch 800[^"]*', "", x, perl = TRUE)
  x = gsub('Erzählen Sie auf FOCUS Online über Ihren Heimatort Teilen Sie Ihren Artikel und Ihr Foto', "", x, perl = TRUE)
  x = gsub("Bericht schreiben", "", x)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub("Hier können Sie selbst Artikel verfassen:","", x)
  x = gsub("Live-Ticker", "", x)
  x = gsub('Aus unserem Netzwerk[^"]*', "", x, perl = TRUE)

  # Spiegel.de
  x = gsub("7 mal 17", "", x)
  x = gsub("Zur Startseite Diesen Artikel... Drucken Feedback Nutzungsrechte", "", x)
  x = gsub('Liebe Leserin, lieber Leser,\num diesen[^"]*', "", x)
  x = gsub('Liebe Leserin, lieber Leser, um diesen[^"]*', "", x)
  x = gsub('ejf[^"]*', "", x)
  x = gsub('tjf[^"]*', "", x)
  x = gsub('Fotostrecke[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Florian Gathmann[^\n]*', "", x, perl = TRUE)

  # Zeit.de
  x = gsub("Inhalt Seite", "", x)
  x = gsub('Seitennavig[^"]*',"", x, perl=TRUE)
  x = gsub('Kartengeschichte[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  
  # Stern.de
  x = gsub('Fullscreen[^\n]*', "", x, perl = TRUE)
  
  return(x)
}

# apply function to dataframe
btw$text_cleaned <- clean.text(btw$title.text)
```

```{r}
btw$text_cleaned <- gsub("[[:punct:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:cntrl:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:digit:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("^[[:space:]]+", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:space:]]+$", " ", btw$text_cleaned)
btw$text_cleaned <- tolower(btw$text_cleaned)
```

## Remove Stopwords
```{r}
btw$text_cleaned<- removeWords(btw$text_cleaned, stopwords("german"))
```

### Remove customized stopwords
```{r}
mystopwords <- c("focus","online","spiegel", "stern", "handelsblatt", "de", "bild","bildplus", "zeit", "ersten", "tagesschau", "wdr", "ndr", "ich","sie","passwort","kommentar","wurde","ihr","der","im","artikel","mehr","ihren","foto","e","seien","comment", "uhr","videolänge","dass","mindestens","das","mail","die","schon","neuer abschnitt", "login", "loggen", "inaktiv", "nwmi", "nwnoa", "polizei","beim","dpa","video","quelle","afp","witters","fotogalerie","wurden","worden","wegen","sagt","immer","gibt","geht","spon", "registrierter","als","spiegel","vielen","in","es","bitte","dank","unserer","nutzer","sei","beitrag","user","seit","zeichen", "datenschutzerklärung","premium","nutzungsbedingungen","nutzungsrechte","pflichtfelder","registrierung","anzeige","großbuchstaben","sonderzeichen","html","seitennavigation","fullscreen","statista","club","sagte","borenda","spreepicture","shopübersicht")

mystopwords <- distinct(as.data.frame(mystopwords))

btw$text_cleaned<- removeWords(btw$text_cleaned, mystopwords$mystopwords)
```

## Stemming (not applied)
```{r }
stem_text<- function(text, language = "porter", mc.cores = 1) {
  # stem each word in a block of text
  stem_string <- function(str, language) {
    str <- strsplit(x = str, split = "\\s")
    str <- wordStem(unlist(str), language = language)
    str <- paste(str, collapse = " ")
    return(str)
  }
   
  # stem each text block in turn
  x <- mclapply(X = text, FUN = stem_string, language, mc.cores = mc.cores)
   
  # return stemed text blocks
  return(unlist(x))
}

btw$text_cleaned <- stem_text(btw$text_cleaned)
```

```{r}
load(file="../output/btw_combined.Rda")
```

# Extract FB Shares
```{r}
library(SocialMediaMineR)


facebook <- get_facebook(btw$url[1:100])

page <- readLines(paste0(check,btw$url[i]))


check <- "https://graph.facebook.com/?id="
pattern <- 'share_count\\":[0-9]+'

shares <- NULL

for (i in (length(shares)+1):nrow(btw)){
  page <- readLines(paste0(check,btw$url[i]))
  string <- str_extract(page, pattern)
  shares <- append(shares, as.numeric(str_extract(string, "[0-9]+")))
  Sys.sleep(10)
}
```


```{r}
save(btw, file="../output/btw_combined.Rda")
```

```{r}
url_slice <- split(btw$url, ceiling(seq_along(btw$url)/1000))
file = "../output/urls"

for (i in 1:length(url_slice)){
  url = url_slice[[i]]
  count = as.character(i)
  write.table(url, file=paste0(file,count,".csv"), sep ="\n", 
              row.names = FALSE, quote = FALSE, col.names = FALSE)
}

write.table(urls1, file="../output/urls1.csv", sep ="\n", row.names = FALSE, quote = FALSE)
write.table(urls2, file="../output/urls2.csv", sep ="\n", row.names = FALSE, quote = FALSE)
```

```{r}
library(Rfacebook)

app_id = 1580533335373831
app_secret = "07ecdc314355649e13447278c57627f6"

shares
```


