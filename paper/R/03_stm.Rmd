---
title: "News Topic Modeling"
subtitle: ""
author: "Franziska Löw"
date: ""
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: show
    self_contained: true
---

```{r Setup}
suppressPackageStartupMessages({
  library(dplyr)       # Data manipulation
  library(stringr)     # String manipulation
  library(lubridate)   # Date and time manipulation
  library(purrr)       # Functional programming
  library(tidyr)       # Reshaping
  library(magrittr)    # Advanced piping
  library(pushoverr)   # Pushover notifications
  library(doMC)        # Parallel Computing
  library(readr)       # Importing data
  library(tibble)      # Better data frames
  
  library(ggplot2)     # Static data visualization
  library(ggrepel)     # Repel text labels
  library(ggiraph)     # GGplot interactive
  library(scales)      # Scales
  library(viridis)     # Viridis color scales
  library(htmlwidgets) # JS visuliaztions
  library(htmltools)   # Arbitrary html
  library(ggjoy)       # Create joyplots
  library(gganimate)   # Animating ggplots
  library(tweenr)      # Tweening charts
  
  library(httr)        # HTTP functions
  library(jsonlite)    # JSON parsing
  
  library(tidytext)    # Tidy text mining
  library(tm)    # Tidy text mining
  library(hunspell)    # Text processing
  library(stringdist)  # String distances
  library(topicmodels) # Topic modelling
  library(stm)         # Sructural Topic Model
  library(proxy)       # Distance measures
  library(SnowballC)   # Stemming
})

# Theming
quartzFonts(
  Roboto = 
    c("Roboto-Light",
      "Roboto-Bold",
      "Roboto-Regular",
      "Roboto-Thin")
)

theme_set(
  theme_bw(base_family = "Roboto", base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", size = 14, 
                                margin = margin(0, 0, 4, 0, "pt")),
      plot.subtitle = element_text(size = 12),
      plot.caption = element_text(size = 6, hjust = 0),
      axis.title = element_text(size = 10),
      panel.border = element_blank()
    )
)

rm(list=ls())
# Functions
source("func/02-topic-modelling-functions.R")

```

```{r caching, echo = FALSE}
#load("../output/models/LDAGibbs 40 .Rda")
load("../output/models/STM 40 .Rda")
#load("../output/btw_combined.Rda")
knitr::opts_chunk$set(eval = TRUE, warning = FALSE)

btw$articleID <- as.integer(rownames(btw))
```

## Introduction

Social networks such as Facebook are becoming more and more important for online news services: an increasing number of their readers access the news pages via links in the networks. Users of Facebook, for example, can use their profile to share links to external websites - such as news portals - with their online friends. This has led to the development of social media into an important generator of traffic on the internet pages. In Germany, 94\% of online shared news articles in 2015 are distributed via Facebook, followed by Twitter with 3.5\% and Google+ with 2.3\% \citep{schiller_development_2016}. The advertising-financed business model of the media houses is based on the premise that users visit their websites in order to achieve high advertising revenues. For this reason, news agencies are particularly interested in finding out which topics are more likely shared on these platforms. \citet{schiller_development_2016} show, that social media users choose a certain site depending on the researched topic. FOCUS Online for example is targeted for articles from politics and business, sports news is more likely to be shared from Bild.de. 

While these static resorts give an indication on the content of an article, multiple articles in the same resort probably don't cover the same topics (and are not equally shared). Especially if the articles originate from different news portals. Furthermore, articles can contain more than one topic. We use a structural topic model to reveal the underlying topics of a collection of articles (a corpus), and how the articles exhibit them. We then estimate the effect of topic prevalence on the number of Facebook shares. 

## Methodology

Mapping raw text to one or more topics, without having prior knowledge on what those topics are, translates to an unsupervised classification problem on natural language. Within topic models the Latent Dirichlet Allocation (LDA) is a widely used technique, where each document (article) is viewed as a mixture of topics (represented by the document-topic distribution) and each topic is a mixture of unique terms (represented by the topic-term distribution).\cite{blei_latent_2003} 

This model views the text generation process as conforming to the following characteristics: 

  * A topic is a mixture of words. A topic is comprised of many words, and each word maps to one or more topics.
  * A document (in this case, a news article), is a mixture of topics. Each document can be thought of as containing a proportion of words from each topic.

To "learn" the topic prevalence and the topic-term distribution, collapsed Gibbs sampling\footnote{See Section \ref{section_gibbs} for a non-formal description of the Gibbs sampler} can be used. One of the important considerations of this model is that the number of topics $k$ must be known a-priori.

![Research Strategy](../figs/research_strategy.png)

## Dataset

To explore the effect of topic prevalence of an article on the times this article is shared on Facebook, we analyze a sample of 6307 news articles containing the term "Bundestagswahl" (Federal Election) dated from 01.06.2017 to 10.10.2017\footnote{German federal elections took place on 24th of September 2017.} and originated from six different german news sources.\footnote{Bild.de, Focus.de, Spiegel.de, Stern.de, Welt.de, Zeit.de} For each document in our sample, we observe the day it was written, the number of shares on Facebook and the newswire service publishing the article. We include the month the document was written and the news agency as covariates on topical prevalence. We also include news agency as a covariate affecting topical content to estimate how topics are discussed in different ways by different news agencies \citep{roberts_model_2016}.

Figure -- shows the number of articles by news agency and the how many of these articles were shared on Facebook. 

```{r echo=FALSE}
btw$shared <- ifelse(btw$fb_shares == 0, "0", NA)
btw$shared <- ifelse(btw$fb_shares > 0 | btw$fb_shares >= 100, "0-100", btw$shared)
btw$shared <- ifelse(btw$fb_shares > 100, "> 100", btw$shared)

btw %>%
  ggplot(aes(site, fill=shared)) +
  geom_bar(alpha = .7) +
  labs(x="", y="Count") 
```

```{r}
btw %>% 
  # Tokenize by word
  unnest_tokens(word, text_cleaned, token = "words") ->
  # Assign to variable
  posts_tokenized.dt

# Remove short words
posts_tokenized.dt %>% 
  filter(str_length(word) >= 3) ->
  posts_tokenized.dt

# Summarise
posts_tokenized.dt %>% 
  group_by(articleID, word) %>%
  dplyr::summarise(term_frequency = n()) %>%
  ungroup() ->
  posts_tokenized.dt
```

## Analysis

### STM Training

```{r eval=FALSE, include=FALSE}
# Build Corpus
btw$month <- month(btw$date)
# Process data
processed <- textProcessor(btw$text_cleaned, metadata = btw[,c("date","month","site","text_cleaned","text")],
                           wordLengths = c(2,Inf),
                           lowercase = F,
                           removestopwords = F,
                           removenumbers = F,
                           removepunctuation = F,
                           stem = F)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
```

We then train the LDA on the full dataset, with $k = 40$.

```{r}
k = 40 # Topics

t1 <- Sys.time()
stmOut <- stm(documents = out$documents, 
           vocab = out$vocab, 
           K=k, 
           prevalence = ~site+s(month), 
           content = ~site, 
           data=out$meta, 
           init.type = "Spectral")
t2 <- Sys.time()
t2 - t1

save(stmOut, btw, out, file=paste("../output/models/STM",k,".Rda"))

```

### STM Exploratiom

#### Topic Summary
```{r}
plot(stmOut, type="summary", xlim = c(0,.14), n=5, text.cex = 0.7, main = "Topics in german online news covering the Federal Elections")
```

Assign labels to the topics based in the interpretation of word-topic probabilities. 

```{r}
Labelprep <- labelTopics(stmOut)

# Topic Labels
topicLabels <- as.data.frame(Labelprep$topics)
topicLabels %>%
  transmute(labels = paste(V1,V2,V3, sep = ", "),
            topic = rownames(.)) -> topicLabels

# Covariate topic labels
rown <- as.data.frame(rep(rownames(Labelprep$topics),6))
rown <- rown %>% 
  transmute(topics = as.numeric(.[,1])) %>% arrange(topics) %>%
  mutate(site = rep(rownames(Labelprepr$covariate), 40))

covLables <- as.data.frame(Labelprepr$interaction)
covLables <- covLables %>%
  transmute(topic = as.numeric(rown$topics),
            site = rown$site,
    labels = paste(V1,V2,V3, sep = ", "),
            topic.site = paste0("Topic", rown$topics, ", ",rown$site))
```


We can also plot the exclusivity and semantic coherence, which are two metrics that measure the “interpretability” of each topic. Higher semantic coherence indicate topics that have more consistent words (more interpretable) while exclusivity measures how exclusive the words are to the topic relative to other topics (e.g. low values mean topics that are vague and share a lot of words with other topics while high values indicate words that are very unique/exclusive to the topic). 

Does not support models with content covariates.

```{r}
# par(mfrow = c(1,1),mar = c(2, 2, 2, 2))
# 
# topicQuality(stmOut,
#              documents = out$documents, 
#              main = "Topic Interpretability: Exclusivity and Semantic Coherence")
```

#### Estimating the Effect of covariates
One of the main contributions of STM is to provide a mechanism to test (with statistical significance) the impact document-level covariates (e.g. party, time of post) have on the topic proportions (prevalence). Method used for plotting. "pointestimate" estimates mean topic proportions for each value of the covariate.

```{r Estimating the Effect of Covariates}
k <- 40

prep <- estimateEffect(1:k ~ site+s(month), stmobj = stmOut, meta = out$meta, uncertainty = "Global")
```

```{r Plot Effect Of Covariates On Topics}
par(mfrow = c(3,3), mar = c(4,4,2,2))
for (i in 1:k){
  plot(prep, "site", method = "pointestimate", topics = i, 
       labeltype = "custom", custom.labels = prep$data$site,
       cex = 0.6, cex.main = 0.7,
       main = topicLabels[which(topicLabels$topic==i),])
}
```

We can also explore the effect of a contiuous covariate like time. In this case, we use month using a b-spline to smooth out our results.

For simplicity, I removed confidence intervals and only plotted the (point) estimates.

```{r}
col = palette(rainbow(6))

par(mfrow = c(2,2),mar = c(4,4,2,2))
for (i in 1:k){
  plot(prep, "month", method = "continuous", topics = i, 
                      main = paste0(topicNames$labels[i],"\n: Topic ", topicNames$topic[i]),
                      cex.main = .8,
                      printlegend = FALSE, ylab = "Exp. Topic Prob", 
                      xlab = "Time (June 2017 to Oct 2016)",
                      ylim = c(-0.07,0.35),
                      cex.axis = .6,
                      cex.label = .6,
                      moderator = "site", moderator.value = prep$data$site[1],  linecol = col[1],
                      ci.level = 0
                      )
  for (j in 2:6){
    plot(prep, "month", method = "continuous", topics = i, 
                      #main = paste0(topicNames$labels[i],": Topic ", topicNames$topic[i]),
                      printlegend = FALSE, ylab = "Exp. Topic Prob", 
                      ylim = c(-0.07,0.35),
                      moderator = "site", moderator.value = prep$data$site[j],  linecol = col[j],
                      ci.level = 0, add="T"
                      )
  }
  
legend(-1, 1.9, prep$data$site, lwd = 2, col = col)
}

prep$parameters
```


#### Topic Labeling

```{r Extract wtp and dtp}
# Word-topic probabilities
stmOut %>% tidy("beta") %>% filter(!is.na(topic)) -> posts.wtp

# Document-topic probablities
stmOut %>% tidy("gamma") -> posts.dtp
```

We can then assess the word-topic probabilities in order to get an idea of the topic.

Since we have over 30,000 unique terms in the corpus, we need to extract the top few words that most uniquely define each topic, so that we can more easily visualize and label them. 

```{r}
### Create Topic Labels
btw.terms <- labelTopics(stmOut,1, n=30)
btw.terms <- as.data.frame(t(btw.terms$topics))

topicTerms <- btw.terms %>% gather(topic)
topicTerms <- cbind(topicTerms, Rank = rep(1:30))
topicTerms <- topicTerms %>% 
  filter(Rank < 5) %>%
  mutate(topic = as.numeric(topic))

topicLabel <- data.frame()
for (i in 1:40){
  z <- dplyr::filter(topicTerms, topic==i)
  l <- as.data.frame(paste(z[1,2], z[2,2], z[3,2], z[4,2], sep = " "), stringAsFactors = FALSE)
  topicLabel <- rbind(topicLabel, l)
}
colnames(topicLabel) <- c("topic_name")
topicLabel$topic <- as.integer(rownames(topicLabel))
```


```{r Compute relevance}
word_relevance.dt <- posts.wtp %>%
  filter(str_length(term) >= 3) %>%
  # Compute lambda and phi_kw
  mutate(lambda = 0.6, phi_kw = beta) %>% 
  # Compute and join the p_w
  left_join(
    posts_tokenized.dt %>%
      group_by(word) %>% 
      summarise(frequency = sum(term_frequency)) %>% 
      ungroup() %>% 
      mutate(p_w = frequency/sum(frequency)) %>% 
      select(-frequency),
    by = c("term" = "word")
  ) %>%
  # Compute the relevance
  mutate(relevance = lambda * log(phi_kw) + (1 - lambda) * log(phi_kw/p_w))
  filter(!is.na(relevance)) 

```

```{r Loading topic mappings}
### Most likely Topics per Article
btw.topics <- make.dt(stmOut)
btw.topics %>% select(-docnum) -> btw.topics

# Create Dataframe
btw.topics$V1 <- colnames(btw.topics)[apply(btw.topics,1,which.max)]

btw.topics %>%
  select(V1) %>% 
  transmute(articleID = as.integer(rownames(.)),
            topic = as.integer(gsub("Topic","",V1))) -> doctopics.df

# Add Topic to origian DF
btw.dt <- btw %>%
  inner_join(.,doctopics.df, by="articleID")

topics_mapping.dt<-btw.dt %>%
  group_by(topic) %>%
  slice(1) %>%
  select(topic, title.text, articleID, fb_shares) 
```

```{r}
## Combine with Topic label
topics_mapping.dt %>%
  left_join(., topicLabel, by="topic") -> topics_mapping.dt
```

### Sample articles

To test, wheather the topic classification make sense, we pull out sample articles classified by the model to see if they match the assigned topic.


Now that we have labelled each topic, we produce sample documents classified to that topic for reference. We produce a random sample of 10 posts from the 300 highest probability fits per topic.

```{r Document classification}

set.seed(9272)

posts_classification.sdt <- posts.dtp %>% 
  group_by(articleID = document) %>% 
  summarise(topic = min(topic[gamma == max(gamma)]), gamma = max(gamma)) %>% 
  ungroup() %>%
  inner_join(btw %>% select(title, title.text, articleID), by = "articleID") %>%
  inner_join(topics_mapping.dt %>% select(topic, topic_name), by = "topic") %>%
  mutate(topic_title = 
           paste0("Topic ", formatC(topic, flag = "0", width = 2), 
                  " - ", topic_name)) %>%
  group_by(topic_name) %>%
  top_n(300, gamma) %>%
  sample_n(5) %>% 
  mutate(row = row_number()) %>% 
  ungroup() 
```

We produce a chart that shows this in a presentable manner.

```{r Document classification chart, echo=TRUE}

ggsave(
  plot = {
    posts_classification.sdt %>% 
      ggplot(aes(x = 0, y = row, color = factor(topic, levels = 1:40))) +
      facet_wrap(~topic_title, ncol = 5) +
      geom_text(
        aes(label = title),
        size = 2, hjust = 0, family = "Roboto"
      ) +
      geom_point(aes(x = -0.025), size = 0.5) +
      scale_x_continuous(limits = c(-0.05, 1), expand = c(0, 0)) +
      scale_color_manual(
        values = rainbow(40) %>% 
          adjustcolor(red.f = 0.6, green.f = 0.6, blue.f = 0.6)
      ) +
      theme(
        axis.title       = element_blank(),
        axis.text        = element_blank(),
        axis.ticks       = element_blank(),
        panel.grid       = element_blank(),
        strip.background = element_blank(),
        panel.border     = element_blank(),
        strip.text       = element_text(face = "bold", hjust = 0, size = 6),
        plot.background  = element_rect(fill = "#FAFAFA"),
        legend.position  = "none"
      ) +
      labs(
        title    = "GERMAN NEWS LANDSCAPE",
        subtitle = "Sample online news articles in the year 2017",
        caption  = 
"DATA SOURCE: Webhose.io, Eventbride.com

CHART NOTES:
  1. Topics were derived through Latent Dirichlet Allocation (LDA) with 40 topics
  2. Documents were classified according to the topic that has the highest probability
"
      )
  },
  device = "png",
  filename = "../figs/02-stm-topic-classification-sample.png",
  dpi = 600, height = 10, width = 16
)

```

![Facebook news samples](../figs/02-stm-topic-classification-sample.png)

