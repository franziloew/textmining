---
title: "Topic Modeling of News"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm","RColorBrewer","sampling",
          "plyr","dplyr","class","knitr","data.table","parallel","SnowballC",
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales","rjson")
lapply(libs, library, character.only = TRUE)
```

# Load and prepare Dataframes
```{r message=FALSE}
rm(list =ls())

load("../output/news_cleaned.Rda")
okt <- read_csv("../data/eventbride/eventbride_okt_18.csv")
nov_03 <- read_csv("../data/eventbride/eventbride_nov_03.csv")
nov_22 <- read_csv("../data/eventbride/eventbride_nov_22.csv")
dec_16 <- read_csv("../data/eventbride/eventbride_dec_16.csv")
jan_1 <- read_csv("../data/eventbride/eventbride_jan_1.csv")
```

## combine dataframes

```{r}
df_full <- bind_rows(list(okt,nov_03,nov_22,dec_16,jan_1)) %>%
  mutate(text = body,
         # Extract site 
         site = str_extract(source, "(?<='uri': ')[A-z][^']*")) %>%
  # Delete duplicates
  #filter(isDuplicate=="False") %>%
  distinct(text, site, .keep_all = TRUE) %>%
  select(date,title,text,site,url)  

df_full <- bind_rows(df %>%
                   mutate(date = as.Date(date)) %>%
                   select(date,title,text,site,url),
                 df_full)
```

# Pre-Process Text
## select news-outlets
```{r}
df_full %>%
  mutate(month=month(date)) %>%
  filter(month > 5) %>%
  ggplot(aes(site, fill=factor(month))) +
  geom_bar() +
  labs(x="", y="") +
  theme(axis.text.x = element_text(angle = 60))
```


```{r}
keeps <- c("spiegel.de", "focus.de", "zeit.de","bild.de",
           "tagesschau.de", "welt.de")

df <- df_full %>%
  mutate(title_text = paste(title, text, sep=" ")) %>%
  filter(site %in% keeps) %>%
  filter(date>=as.Date("2017-06-01")) %>%
  filter(date< as.Date("2018-01-01"))
```

## select politic section
```{r}
pat1 <- "\\w\\.de/politik/deutschland"
pat2 <- "\\w\\.de/politik/inland"
pat3 <- "\\w\\.de/politik/wahl"
pat4 <- "\\w\\.de/politik/bundestagswahl"
pat5 <- "tagesschau.de/inland"

df %>%
  mutate(politics = ifelse(grepl(paste(pat1,pat2,pat3,pat4,pat5,
                      sep="|"), url, perl = TRUE),1,0)) -> df

#........................
# pat <- paste("cdu","csu","union","spd","fdp","afd",
#              "linkspartei","die linke","die grünen","bündnis 90", sep="|")
# 
# df %>%
#   mutate(politics = ifelse(grepl(pat, title_text, ignore.case = TRUE),1,0)) -> btw1
```

```{r}
btw <- df %>% filter(politics==1)
```

```{r}
btw %>%
  group_by(date, site) %>%
  dplyr::summarize(obs = n()) %>%
  ggplot(aes(date, obs, color = factor(site))) +
  geom_line() +
  #geom_vline(aes(xintercept=as.Date("2017-09-24")),
  #           linetype = 2, color="grey40") +
  #geom_vline(aes(xintercept=as.Date("2017-11-20")),
  #               linetype = 2, color="grey40") +
  #scale_color_manual(values = col) +
  labs(x="", y="Count",color="") +
  scale_x_date(breaks = date_breaks("2 weeks"), labels=date_format("%d.%m", tz="CET"))
```

## count number of terms in a string
```{r}
btw$text_length <- sapply(gregexpr("\\S+", btw$text), length)
```

## ... and filter short text & other
```{r}
btw <- btw %>%
  filter(text_length > 120) %>%
  arrange(desc(text_length)) %>%
  distinct(url, .keep_all = TRUE) %>%
  
  # remove articles that contain daily overviews
  filter(!grepl("Nachrichten am Morgen", title)) %>%
  filter(!grepl("elbvertiefung", url)) %>%
  filter(!grepl("zeit-magazin", url)) %>%
  filter(!grepl("s-magazin", url)) %>%
  filter(!grepl("Der Morgen live", title)) %>%
  filter(!grepl("Die Lage am", title)) %>%
  filter(!startsWith(title,"News")) %>%

  # remove articles that only contain video 
  filter(!grepl("Video einbetten Nutzungsbedingungen Embedding Tagesschau", title_text)) %>%
  filter(!grepl("</div>", title_text)) %>%
  
  # remove english text
  filter(!grepl("bild-international", url)) %>%
  filter(!grepl("spiegel.de/international", url)) %>%
  filter(!grepl("german",url)) %>%
  filter(!grepl("arabisch",url)) %>%
  
  # remove text that mostly contain user comments
  filter(!grepl("ticker", title, ignore.case = TRUE)) %>%
  filter(!grepl("stream", title, ignore.case = TRUE)) %>%
  filter(!startsWith(text,"1.")) %>%
  
  # remove articles behind a pay-wall
  filter(!grepl("SPIEGEL-Plus-Artikel", text)) %>%

  # remove articles about the weater
  filter(!grepl("wetter", url))
```

```{r}
save(btw, file="../output/btw_combined.Rda")

rm(list=ls())
load("../output/btw_combined.Rda")
```

```{r}
btw %>%
  ggplot(aes(site)) +
  geom_bar(show.legend = FALSE, alpha=0.8)
```

## Clean text

The following two chuncks of code are just to check right regex definitions to use it inthe clean.text function.
```{r}
pat <- "Wahlprogramm"
btw %>%
  filter(grepl(pat, title, perl = TRUE, ignore.case = TRUE)) %>%
  select(title) 
  #.[2,]
  #group_by(site) %>%
  #tally(sort = TRUE)
```

```{r}
btw_test <- btw 

as.data.frame(str_match(btw_test$title_text , pat)) ->test

test %>%
  filter(!is.na(test)) %>%
  .[2,]
```


### define function  
```{r, echo=FALSE, message=FALSE, warning=FALSE}
clean.text <- function(x)
  {
  # All
  x = gsub("Getty Images", "", x)
  x = gsub('Startseite[^\n]*', "", x, ignore.case = TRUE, perl = TRUE)
  x = gsub("deutsche presse agentur","", x, ignore.case = TRUE, perl = TRUE)
  x = gsub("Eine Kolumne von \\w{1,} \\w{1,}", "", x, ignore.case = T, perl = TRUE)

  # Bild.de
  x = gsub("Shopübersicht Top Gutscheine", "", x)
  x = gsub('Politik Inland[^\n]*', "", x, perl = TRUE)
  
  # welt.de
  x = gsub('Quelle: N24[^\n]*', "", x, perl = TRUE)
  x = gsub('infocom[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Infografik[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('versuchen Sie es[^.]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Video konnte nicht[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Welt twitter', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('\\w{1,} für Abonnenten', "", x, perl = TRUE, ignore.case = TRUE)

  # FOCUS.de
  x = gsub("FOCUS Online", "", x)
  x = gsub('Wochit[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub('Im Interesse unserer User[^"]*', "", x, perl = TRUE)
  x = gsub('Sie haben noch 800[^"]*', "", x, perl = TRUE)
  x = gsub('Erzählen Sie auf FOCUS Online über Ihren Heimatort Teilen Sie Ihren Artikel und Ihr Foto', "", x, perl = TRUE)
  x = gsub("Bericht schreiben", "", x)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub("Hier können Sie selbst Artikel verfassen:","", x)
  x = gsub("Live-Ticker", "", x)
  x = gsub('Aus unserem Netzwerk[^"]*', "", x, perl = TRUE)
  x = gsub("</div>[^*]*", "", x, perl = TRUE)


  # Spiegel.de
  x = gsub("7 mal 17", "", x)
  x = gsub("Zur Startseite Diesen Artikel... Drucken Feedback Nutzungsrechte", "", x)
  x = gsub('Liebe Leserin, lieber Leser,\num diesen[^"]*', "", x)
  x = gsub('Liebe Leserin, lieber Leser, um diesen[^"]*', "", x)
  x = gsub('ejf[^"]*', "", x)
  x = gsub('tjf[^"]*', "", x)
  x = gsub('Fotostrecke[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Florian Gathmann[^\n]*', "", x, perl = TRUE)
  x = gsub('Eine Kolumne von Jan Fleischhauer', "", x, perl = TRUE)
  x = gsub("Wenig Zeit? Am Textende gibt's eine Zusammenfassung", "", x)
  x = gsub("Twitter: @\\w{1,} folgen Mehr Artikel von \\w{1,} \\w{1,}", "", x, perl = TRUE)

  # Zeit.de
  x = gsub("Inhalt Seite", "", x)
  x = gsub("\\w{1,} \\w{1,} zur Autorenseite", "", x, perl = TRUE)
  x = gsub('Seitennavig[^"]*',"", x, perl=TRUE)
  x = gsub('Kartengeschichte[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  
  # Stern.de
  x = gsub('Fullscreen[^\n]*', "", x, perl = TRUE)
  
  # Tagesschau.de
  x = gsub("Hinweis: Falls die [^\\.]*", "", x, perl=TRUE)
  x = gsub("auswählen", "", x, perl = TRUE)
  x = gsub("Dieser Artikel wurde ausgedruckt unter der Adresse: [^\\s]*", "", x, perl = TRUE)
  x = gsub("faktenfinder.tagesschau.de", "", x, perl=TRUE)
  
  # Deutschlandfunk
  # x = gsub("Äußerungen unserer Gesprächspartner[^.]*", "", x, perl = TRUE)
  # x = gsub("im Gespräch mit \\w{1,}", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Heinlein", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Barenberg", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Büüsker", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Dobovisek", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Schmidt-Mattern", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Zagatta", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Armbrüster", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Münchenberg", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Heckmann", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Kaess", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Engels", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Zurheide", "", x, ignore.case = TRUE, perl=TRUE)
  # x = gsub("Marc Müller", "", x, ignore.case = TRUE, perl=TRUE)

  return(x)
}

# apply function to dataframe
btw$text_cleaned <- clean.text(btw$title_text)

btw$text_cleaned <- gsub("[[:punct:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:cntrl:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:digit:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("^[[:space:]]+", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:space:]]+$", " ", btw$text_cleaned)
btw$text_cleaned <- tolower(btw$text_cleaned)

## Remove stopwords
# 1
german_stopwords_full <- read.table("dict/german_stopwords_full.txt", stringsAsFactors = F)
german_stopwords_full <- german_stopwords_full$V1

# 2
mystopwords <- c("focus","online","spiegel", "stern", "de", "bild","bildplus","n-tv.de", "zeit", "ersten","ard", "tagesschau","müssen","sagen","faktenfinder", "zeitmagazin","seitenanfang","ja","mal","heute","ich","sie","passwort","kommentar","wurde","ihr","der","im","artikel","mehr","ihren","foto","e","seien","comment","ticker","live","laif","uhr","videolänge","dass","mindestens","das","mail","die","schon","neuer abschnitt", "login", "loggen", "inaktiv","nwmi","wäre","viele","nwnoa","morgenkolumne","beim","dpa","video","quelle","afp","witters","fotogalerie","wurden","worden","wegen","sagt","immer","gibt","geht","spon","registrierter","als","spiegel","vielen","in","es","bitte","dank","unserer","nutzer","sei","beitrag","user","seit","zeichen","tba","datenschutzerklärung","premium","nutzungsbedingungen","nutzungsrechte","pflichtfelder","registrierung","anzeige","großbuchstaben","sonderzeichen","html","seitennavigation","fullscreen","statista","club","sagte","borenda","spreepicture","shopübersicht","herr","imago","dobovisek","barenberg","heinlein","armbrüster","kaess","münchenberg","büüsker","tsereteli","konietzny","klenkes","hauptstadtstudio","newsletter","premiumbereich","nachrichtenpodcast","karrierespiegel","picture alliance","appnutzer","civey","abo")

stopwords <- c(german_stopwords_full, mystopwords)
stopwords <- unique(stopwords)

# 3
btw$text_cleaned<- removeWords(btw$text_cleaned, stopwords)
```

## month & document number
```{r}
btw %>%
  mutate(month = month(date),
         week = week(date),
         articleID = as.numeric(rownames(btw)),
         site = ifelse(site=="bild.de","Bild.de",site),
         site = ifelse(site=="focus.de","FOCUS ONLINE",site),
         site = ifelse(site=="spiegel.de","SPIEGEL ONLINE",site),
         site = ifelse(site=="zeit.de","ZEIT ONLINE",site),
         site = ifelse(site=="tagesschau.de","Tagesschau.de",site),
         site = ifelse(site=="welt.de","DIE WELT",site)) -> btw
```

```{r}
save(btw, file="../output/btw_combined.Rda")
```

## Stemming
```{r }
stem_text<- function(text, language = "porter", mc.cores = 1) {
  # stem each word in a block of text
  stem_string <- function(str, language) {
    str <- strsplit(x = str, split = "\\s")
    str <- wordStem(unlist(str), language = language)
    str <- paste(str, collapse = " ")
    return(str)
  }
   
  # stem each text block in turn
  x <- mclapply(X = text, FUN = stem_string, language, mc.cores = mc.cores)
   
  # return stemed text blocks
  return(unlist(x))
}

btw$text_cleaned1 <- stem_text(btw$text_cleaned)
```

```{r}
save(btw, file="../output/btw_combined.Rda")
```

### check words with high tf-idf 
```{r}
token <- btw %>%
  group_by(site) %>%
  unnest_tokens(word, text_cleaned1) %>%
  dplyr::count(site, word, sort = TRUE)  %>%
  bind_tf_idf(word, site, n) %>%
  dplyr::arrange(desc(tf_idf))

token %>%
  arrange(desc(tf)) %>%
  arrange(site) %>%
  top_n(10)
```

### Bigrams
```{r, include=FALSE}
bigrams <- btw %>%
  unnest_tokens(bigram, text_cleaned, token="ngrams", n=2)

bigrams %>%
  group_by(site) %>%
  count(bigram) %>%
  arrange(desc(n)) %>%
  top_n(10)
```

# Structural Topic Model

## Preparation
```{r}
btw <- btw %>% 
  mutate(ownership = ifelse(site == "Tagesschau.de", "public", "private"))
```

### Build Corpus
```{r eval=FALSE, include=FALSE}
# Process data

### without stemming
processed1 <- textProcessor(btw$text_cleaned1, metadata = btw[,c("ownership","text_cleaned1","month")],
                           wordLengths = c(2,Inf),
                           lowercase = F,
                           removestopwords = F,
                           removenumbers = F,
                           removepunctuation = F,
                           stem = F)
out_ownership <- prepDocuments(processed1$documents, processed1$vocab, processed1$meta)
out_ownership$meta$ownership <- as.factor(out_ownership$meta$ownership)
###

# processed <- textProcessor(btw$text_cleaned1, metadata = btw[,c("site","text_cleaned1","month")],
#                            wordLengths = c(2,Inf),
#                            lowercase = F,
#                            removestopwords = F,
#                            removenumbers = F,
#                            removepunctuation = F,
#                            stem = F)
# out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
# out$meta$site <- as.factor(out$meta$site)

save(btw, out_ownership, file="../output/btw_comb_ownership.Rda")
```

# FB Shares
```{r}
endpoint <- "https://api.sharedcount.com/?url="
api1 <- "&apikey=24abe04472f42e5003938b0314844010d6425d37"
api2 <- "&apikey=68d76247a02b3d39ff592aa5616ce9f73f182934"

pattern <- 'share_count\\":[0-9]+'
shares <- NULL

for (i in (length(shares)+1):nrow(btw)){
  page <- readLines(paste0(endpoint,btw$url[i],api2))
  string <- str_extract(page, pattern)
  shares <- append(shares, as.numeric(str_extract(string, "[0-9]+")))
}
```

```{r}
btw <- cbind(btw, shares)

btw <- btw %>% mutate(fb_shares = shares) %>%
  select(-shares)

unique(btw$site)
```


