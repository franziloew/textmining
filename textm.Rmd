---
title: "Textmining in Economics"
author: "Franziska Löw"
date: "08. Oktober 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm",
          "plyr","dplyr","class","knitr","kableExtra","cldr",'network', 'sna', 'qdap',
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales")
lapply(libs, library, character.only = TRUE)
```

# Introduction

## Introduction

- In economics, empirical work is largely based on quantitative data (e.g. prices, demand, votes, etc.)
- But a large amount of unstructured text is also generated in economic environments (e.g. company reports, court decisions, media articles, etc.)
- At an abstract level, text is simply a string of characters.
    - Latin alphabet ('a', 'A', 'p' and so on)
    - Punctuation (e.g. !), 
    - Numbers, 
    - Non-alphanumerical characters (e.g. @)

**Key Question:** How can we obtain an informative, quantitative representation of these character strings?

## Concepts

- A single observation in a textual database is called a *document*.

- The set of documents that make up the dataset is called a *corpus*.

- Covariates associated with each document are called *metadata*.

## Example

In the text analysis ["Bundestagswahl 2017"](https://franziloew.github.io/politsentiment/election.html) we use a corpus of Tweets containing one of the parliamentary parties during the german parliamentary elections 2017:

- 151930 Tweets (without retweets) from 12am to 12pm on the 24-10-2017

- A document is a single Tweet 

- Associated Metadata: time, user name, if the user is a news outlet or a "privat" twitter-user, etc.

```{r, include=FALSE}
rm(list = ls())
load(file = "~/GitHub/politsentiment_data/out/tweets_18_24.rda")
# during and after elecction
tweets <- tweets[which(tweets$day == "24"),]
tweets <- with(tweets, tweets[hour(date) >= 12 & hour(date) <= 23, ])
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
# get the tweets that are retweeted 
tweets$isretweet <- ifelse(grepl('^RT', tweets$text),TRUE,FALSE)

# Extract the retweets
tweets %>%
  distinct(text,name, .keep_all = TRUE) %>%
  filter(isretweet == TRUE) -> rt

# pull the original author's name
rt$sender <- substr(rt$text, 5, regexpr(':', rt$text) - 1)

# Extract the retweets
orig <-  tweets[which(tweets$isretweet==FALSE),]
```

***

Text data is inherently high-dimensional, so an analysis of text usually tries to reduce this dimensionality following these steps:

1. Represent the textual database $D$ (**document**) as a numerical array $\boldsymbol{C}$ (**corpus**)
  - *Text-preprocessing, Bag-of-words model*
	 
2. **Statistical Methods** 
  - Map $\boldsymbol{C}$ to predict values $\boldsymbol{\hat{V}}$ of unknown outcomes $\boldsymbol{V}$. 
	- Sometimes the attribute of interest is latent (e.g. topics of a newspaper article)
	
3. Use $\boldsymbol{\hat{V}}$ in subsequent **descriptive or causal analysis.**

# Represent text as Data

## Text Pre-Processing

1. **Tokenization** splitting of a raw character string into individual elements of interest: words, numbers, punctuation.

2. **Stopword Removal** Stopwords (highly frequent terms) are stripped out of token lists as they do not help distinguish one document from another.

3. **Linguistic Roots** For many applications, the relevant information in tokens is their linguistic root, not their grammatical form (e.g. treat "looking", "look", "looks" as equivalent tokens)

4. **Multi-Word Phrases** Sometimes groups of individual tokens like “United Kingdom”  have a specific meaning (Use Ngrams)
  
## Cleaning Data

```{r, echo=FALSE}
htmlTable(orig[1:4,c("text","text_cleaned")], align = "l")
```

## Tokenization

```{r, echo=FALSE, message=FALSE, warning=FALSE}
orig$text_cleaned <- removeWords(orig$text_cleaned, c("cdu","csu","union","spd","fdp","grüne","grünen","linke","afd"))

orig %>%
  #group_by(partei) %>%
  unnest_tokens(word, text_cleaned) %>%
  count(word, sort = TRUE)  %>%
  top_n(15) %>%
  htmlTable()
```

## Tokenization (Bigrams)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
orig %>%
  #group_by(partei) %>%
  unnest_tokens(bigram, text_cleaned, token="ngrams", n=2) %>%
  count(bigram, sort = TRUE)  %>%
  top_n(15) %>%
  htmlTable()
```

## What is the data?

After pre-processing, each document is a finite list of terms.

A basic quantitative representation of a corpus is the **bag-of-words model**

- Index each unique term in the corpus by some $v \in {1,...,V}$ where $V$ is the number of unique terms.
- For each document $d \in {1,...,D}$ compute the count $x_{d,v}$ as the number of occurrences of term $v$ in document $d$.
- The $D x V$ matrix $\boldsymbol{X}$ of all such counts is called the document-term matrix.

## Example

```{r}
Doc1 <- 
  c("text", "mining", "is", "more", "fun", "than", "coal", "mining")
```
has the bag-of-words representation

```{r, comment= ""}
table(Doc1)
```

But this has the same represantation:
```{r}
Doc1 <- 
  c("coal", "mining", "is", "more", "fun", "than", "text", "mining")
```

## bag-of-words model

The bag-of-words model is useful for describing content, but we lose all information about sentence structure.

- Any useful representation of text will throw away some information; that’s the essential purpose of text mining.

- The question is whether we are keeping the relevant information for our needs, and getting rid of the extraneous information.


# Term Frequency 

***
How to reflect the importance of a term? 3 concepts:

- TF: Term-Frenquency $tf_{v,d}$ for term $v$ and document $d$
- IDF: Inverse Document Frequency $idf_v = log(D/df_v)$, where $df_v$ is the number of documents that contain term $v$ and $D$ ist die number of Documents
- TF-IDF: $tf-idf_{d,v}=tf_{d,v} * idf_v$
  - Gives prominence to words that occur many times in few documents.
  
***

```{r, include=FALSE}
news.token <- tweets %>%
  group_by(partei) %>%
  unnest_tokens(word, text_cleaned) %>%
  count(partei, word, sort = TRUE)  %>%
  bind_tf_idf(word, partei, n) %>%
  arrange(desc(tf_idf))
```

```{r, echo=FALSE, comment = ""}
news.token[1:10,]
```
  
# Statistical Methods 

***
Methods to connect counts $\boldsymbol{c}_i$ to attributes $\boldsymbol{v}_i$ can be roughly divided into four categories:

**1. Dictionary-based methods** 

No statistical inference. Simply specify $\boldsymbol{\hat{v}_i}=f(\boldsymbol{c}_i)$ for some unknown function $f(\cdot)$. Sometimes based on a specific dictionary of terms.

**2. Text regression methods**

Directly estimate the conditional outcome distribution $p(\boldsymbol{v}_i|\boldsymbol{c}_i)$. 
	
**3. Generative model** of $p(\boldsymbol{c}_i|\boldsymbol{v}_i)$.

**4. Deep learning techniques** neural networks, distributed language models.

## Dictionary-based methods

- In **Tetlock (2007)** $c_i$ is a bag-of-words representation and the outcome of interest $v_i$ is the latent “sentiment” of *Wall
Street Journal* columns, defined along a number of dimensions such as “positive,” “optimistic,” etc.

- In **Baker et al. (2016)** $c_i$ is the count of articles in a given newspaper-month containing a set of pre-specified terms such as “policy,” “uncertainty,” and “Federal Reserve,” and the outcome of interest $v_i$ is the degree of “policy uncertainty” in the economy.

## Text Regression Methods

**Intuition** if we want to predict $v_i$ from $c_i$: regress the observed values of the former ($V^{train}$) on the corresponding values of the latter ($C^{train}$)

- Any generic regression technique can be applied, depending upon the nature of $v_i$

- high-dimensionality of $c_i$, ($p > n^{train}$), requires use of regression techniques appropriate (e.g. $L1$ regularized linear or logistic regression)

## Generative Model

**Intuition** In many cases the underlying causal relationship runs from outcomes to language rather than the other way around. (E.g. Google searches about flu do not cause flu cases to occur, rather, people with flu are more likely to produce such searches.)

**1. Observed attributes** (supervised methods):
  
  - Fitting the model based on the observed training data $\boldsymbol{V}^{train}$, say $f_{\boldsymbol{\theta}}(\boldsymbol{c}_i;\boldsymbol{v}_i)$ for a vector of parameters $\boldsymbol{\theta}$, to this training set. 
  
  - The fitted model $f_{\hat{\boldsymbol{\theta}}}$ can be inverted in order to infer $\boldsymbol{v}_i$ for documents in the test set.

***
**2. Latent attributes** (unsupervised methods): 
  
  - The function relating $\boldsymbol{c}_i$ to $\boldsymbol{v}_i$ is unknown, as we cannot observe the true value of $v_i$. Unsupervised machine learning involves taking unclassified observations and uncovering hidden patterns that structure them in some meaningful way. 
  
  - The outputs of algorithms for unsupervised machine learning can be used as inputs into econometric models for predicting some variable of interest, but this is a different approach from intentionally choosing the dimensions of content based on their predictive ability.
  
  - E.g.: Principal Component Analysis, Topic Modeling (latent Dirichlet Allocation)
  
# Topic Modeling
## Topic Modeling

method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection.

  - Discovering hidden topical patterns that are present across the collection
  - Annotating documents according to these topics
  - Using these annotations to organize, search and summarize texts

There are many techniques that are used to obtain topic models: Latent Dirichlet allocation

## Latent Dirichlet Allocation (LDA)

consider the following set of documents as the corpus:
```{r}
doc1 <- "I had a peanut butter sandwich for breakfast"
doc2 <- "I like to eat almonds, peanut and walnuts."
doc3 <- "My neighbor got a little dog yesterday."
doc4 <- "Cats and dogs are mortal enemies."
doc5 <- "You must not feed peanuts to your dog."
```

This gives the following Document-Term-Matrix
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)

# Build corpus 
corpus <- rbind(doc1, doc2, doc3, doc4, doc5)

# clean corpus
corpus <- tolower(corpus)
corpus <- removeWords(corpus, stopwords("english"))
corpus <- gsub( "[^[:alnum:]]", " ", corpus)
corpus <- gsub("^\\s+|\\s+$", "", corpus) 

#Create document-term matrix
dtm <- Corpus(VectorSource(corpus))
dtm <- DocumentTermMatrix(dtm)

inspect(dtm)
```

***
Now we set the model parameter

```{r, include=FALSE}
library(lda)

corpusLDA <- lexicalize(corpus)
```

```{r, echo=FALSE}
ldaModel <- lda.collapsed.gibbs.sampler(corpusLDA$documents,K=2,vocab=corpusLDA$vocab,burnin=9999,num.iterations=100000,alpha=0.8,eta=0.1)

top.words <- top.topic.words(ldaModel$topics, 3, by.score=TRUE)
print(top.words)
```




