---
title: "Text as Data"
author: "Franziska Löw"
date: "08. Oktober 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

***
- Most empirical work in economics relies on inherently quantitative data: prices, demand, votes, etc.
- But a large amount of unstructured text is also generated in economic environments: 
    - company reports, 
    - policy committee deliberations, 
    - court decisions, 
    - media articles, 
    - political speeches, etc. 
- One can use such data qualitatively, but increasing interest in treating text quantitatively.


## What is Text?

At an abstract level, text is simply a string of characters.

- Some of these may be from the Latin alphabet ('a', 'A', 'p' and so on) but there may also be: 
    - Punctuation (e.g. !), 
    - Numbers, 
    - Non-alphanumerical characters (e.g. @)

**Key Question:** How can we obtain an informative, quantitative representation of these character strings? This is the goal of text mining.

# Representing Text as Data

## Textual Database

- A single observation in a textual database is called a document $D$.

- The set of documents that make up the dataset is called a corpus.

- We often have covariates associated with each document that are sometimes called metadata.

## Reduce Dimensionality

Textmining analysis take the following steps to reduce dimensionality to a managable size:

1. **Represent Text as Data** Bag-of-words model, n-grams, Text-preprocessing
	 
2. **Statistical Methods** Map $\boldsymbol{C}$ to predict values $\boldsymbol{\hat{V}}$ of unknown outcomes $\boldsymbol{V}$. 
	E.g. the variable of interest $\boldsymbol{V}$ is an indicator whether the email is spam. The prediction $\boldsymbol{\hat{V}}$ determines whether or not to send the email to a spam filter. Sometimes the attribute of interest is latent, such as the topics of a newspaper article.
	
3. Use $\boldsymbol{\hat{V}}$ in subsequent **descriptive or causal analysis.**

# Represent text as Data

## Represent text as Data
Convert raw text $D$ to a numerical array $\boldsymbol{C}$:
A basic quantitative representation of a corpus is the bag-of-words model:

- Index each unique term in the corpus by some $v \in {1,...,V}$ where $V$ is the number of unique terms.
- For each document $d \in {1,...,D}$ compute the count $x_{d,v}$ as the number of occurrences of term $v$ in document $d$.
- The $D x V$ matrix $\boldsymbol{X}$ of all such counts is called the document-term matrix.

## Example

```{r}
Doc1 <- 
  c("text", "mining", "is", "more", "fun", "than", "coal", "mining")
```
has the bag-of-words representation

```{r, comment= ""}
table(Doc1)
```

But this has the same represantation:
```{r}
Doc1 <- 
  c("coal", "mining", "is", "more", "fun", "than", "text", "mining")
```

## Text Mining and Information

The bag-of-words model is useful for describing content, but we lose all information about sentence structure.

- Any useful representation of text will throw away some information; that’s the essential purpose of text mining.

- The question is whether we are keeping the relevant information for our needs, and getting rid of the extraneous information.

## Text Pre-Processing

Pre-process strings to obtain a cleaner representation:

1. **Tokenization** splitting of a raw character string into individual elements of interest.

2. **Stopword Removal** The frequency distribution of words in natural languages is highly skewed.
  
3. **Linguistic Roots** For many applications, the relevant information in tokens is their linguistic root, not their grammatical form. (Stemming or Lemmatizing) 
  
4. **Multi-Word Phrases** Sometimes groups of individual tokens like “donald trump” or “fc bayern” have a specific meaning.

## Example: Twitter

Tweets after Stopword Removal.

```{r, echo=FALSE}
rm(list = ls())
load(file = "~/GitHub/politsentiment_data/out/tweets_18_24.rda")

htmlTable::htmlTable(tweets[1:3,c("text","text_cleaned")], align="l")
```

## Tokenization & Term Frequency 

How to reflect the importance of a term? 3 concepts:

- TF: Term-Frenquency $tf_{v,d}$ for term $v$ and document $d$
- IDF: Inverse Document Frequency $idf_v = log(D/df_v)$, where $df_v$ is the number of documents that contain term $v$ and $D$ ist die number of Documents
- TF-IDF: $tf-idf_{d,v}=tf_{d,v} * idf_v$
  - Gives prominence to words that occur many times in few documents.
  
***
```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr",
          "plyr","dplyr",
          "htmlTable")
lapply(libs, library, character.only = TRUE)
```

```{r, include=FALSE}
rm(list = ls())
load(file = "~/GitHub/politsentiment_data/out/tweets_18_24.rda")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# get the tweets that are retweeted 
tweets$isretweet <- ifelse(grepl('^RT', tweets$text),TRUE,FALSE)

# Extract the retweets
tweets %>%
  distinct(text,name, .keep_all = TRUE) %>%
  filter(isretweet == TRUE) -> rt

# pull the original author's name
rt$sender <- substr(rt$text, 5, regexpr(':', rt$text) - 1)

# Extract the retweets
orig <-  tweets[which(tweets$isretweet==FALSE),]
```

```{r, include=FALSE}
news.token <- tweets %>%
  group_by(partei) %>%
  unnest_tokens(word, text_cleaned) %>%
  count(partei, word, sort = TRUE)  %>%
  bind_tf_idf(word, partei, n) %>%
  arrange(desc(tf_idf))
```

```{r, echo=FALSE, comment = ""}
news.token[1:10,]
```
  
# Statistical Methods 

***
Methods to connect counts $\boldsymbol{c}_i$ to attributes $\boldsymbol{v}_i$ can be roughly divided into four categories:

**1. Dictionary-based methods** 

No statistical inference. Simply specify $\boldsymbol{\hat{v}_i}=f(\boldsymbol{c}_i)$ for some unknown function $f(\cdot)$. Sometimes based on a specific dictionary of terms.

**2. Text regression methods**

Directly estimate the conditional outcome distribution $p(\boldsymbol{v}_i|\boldsymbol{c}_i)$. 
	
**3. Generative model** of $p(\boldsymbol{c}_i|\boldsymbol{v}_i)$.

**4. Deep learning techniques** neural networks, distributed language models.

## Dictionary-based methods

- In **Tetlock (2007)** $c_i$ is a bag-of-words representation and the outcome of interest $v_i$ is the latent “sentiment” of *Wall
Street Journal* columns, defined along a number of dimensions such as “positive,” “optimistic,” etc.

- In **Baker et al. (2016)** $c_i$ is the count of articles in a given newspaper-month containing a set of pre-specified terms such as “policy,” “uncertainty,” and “Federal Reserve,” and the outcome of interest $v_i$ is the degree of “policy uncertainty” in the economy.

## Text Regression Methods

**Intuition** if we want to predict $v_i$ from $c_i$: regress the observed values of the former ($V^{train}$) on the corresponding values of the latter ($C^{train}$)

- Any generic regression technique can be applied, depending upon the nature of $v_i$

- high-dimensionality of $c_i$, ($p > n^{train}), requires use of regression techniques appropriate (e.g. $L1$ regularized linear or logistic regression)

## Generative Model

**Intuition** In many cases the underlying causal relationship runs from outcomes to language rather than the other way around. (E.g. Google searches about flu do not cause flu cases to occur, rather, people with flu are more likely to produce such searches.)

**1. Observed attributes** (supervised methods):
  
  - Fitting the model based on the observed training data $\boldsymbol{V}^{train}$, say $f_{\boldsymbol{\theta}}(\boldsymbol{c}_i;\boldsymbol{v}_i)$ for a vector of parameters $\boldsymbol{\theta}$, to this training set. 
  
  - The fitted model $f_{\hat{\boldsymbol{\theta}}}$ can be inverted in order to infer $\boldsymbol{v}_i$ for documents in the test set.

***
**2. Latent attributes** (unsupervised methods): 
  
  - The function relating $\boldsymbol{c}_i$ to $\boldsymbol{v}_i$ is unknown, as we cannot observe the true value of $v_i$. Unsupervised machine learning involves taking unclassified observations and uncovering hidden patterns that structure them in some meaningful way. 
  
  - The outputs of algorithms for unsupervised machine learning can be used as inputs into econometric models for predicting some variable of interest, but this is a different approach from intentionally choosing the dimensions of content based on their predictive ability.
  
  - E.g.: Principal Component Analysis, latent Dirichlet Allocation





